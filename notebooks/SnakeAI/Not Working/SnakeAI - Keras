{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SnakeAI - Keras","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"N96stO4oCgj0"},"source":["https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c\n","\n","# Benchmark DQN"]},{"cell_type":"code","metadata":{"id":"BfJ114Gbc27q","colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"status":"error","timestamp":1612963375273,"user_tz":180,"elapsed":12367,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}},"outputId":"ab4092b8-60fa-40d5-b3ce-bdc5f9790618"},"source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by AI algorithms.\n","\"\"\"\n","\n","import sys # To close the window when the game is over\n","from os import environ, path # To center the game window the best possible\n","import random # Random numbers used for the food\n","import logging # Logging function for movements and errors\n","from itertools import tee # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4, 'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes:\n","        BLOCK_SIZE: The size in pixels of a block.\n","        HEAD_COLOR: Color of the head.\n","        BODY_COLOR: Color of the body.\n","        FOOD_COLOR: Color of the food.\n","        GAME_SPEED: Speed in ticks of the game. The higher the faster.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Initialize all global variables.\"\"\"\n","        self.BOARD_SIZE = 30\n","        self.BLOCK_SIZE = 20\n","        self.HEAD_COLOR = (0, 0, 0)\n","        self.BODY_COLOR = (0, 200, 0)\n","        self.FOOD_COLOR = (200, 0, 0)\n","        self.GAME_SPEED = 10\n","        self.BENCHMARK = 10\n","\n","        if self.BOARD_SIZE > 50:\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                return pygame.Color(42, 42, 42)\n","            else:\n","                return pygame.Color(152, 152, 152)\n","\n","        return pygame.Color(42, 42, 42)\n","\n","    def get_background(self):\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                return pygame.Color(152, 152, 152)\n","\n","        return None\n","\n","    def set_rect(self):\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes:\n","        head: The head of the snake, located according to the board size.\n","        body: Starts with 3 parts and grows when food is eaten.\n","        orientation: Current orientation where head is pointing.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else (food), return without popping.\"\"\"\n","        if action == actions['IDLE']\\\n","            or (action, self.previous_action) in forbidden_moves:\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            return True\n","        else:\n","            self.body.pop()\n","\n","            return False\n","\n","    def return_body(self):\n","        \"\"\"Return the whole body.\"\"\"\n","        return self.body\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes:\n","        pos: Current position of food.\n","        is_food_on_screen: Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","    def set_food_on_screen(self, bool_value):\n","        \"\"\"Set flag for existence (or not) of food.\"\"\"\n","        self.is_food_on_screen = bool_value\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes:\n","        window: pygame window to show the game.\n","        fps: Define Clock and ticks in which the game will be displayed.\n","        snake: The actual snake who is going to be played.\n","        food_generator: Generator of food which responds to the snake.\n","        food_pos: Position of the food on the board.\n","        game_over: Flag for game_over.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score.\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.fps = pygame.time.Clock()\n","\n","    def menu(self):\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images/snake_logo.png\"))\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE, int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","\n","        self.screen_rect = self.window.get_rect()\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                                  4 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                                  6 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                     8 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","\n","        while True:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                option.draw()\n","\n","                if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                    option.hovered = True\n","\n","                    if option == menu_options[0]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['PLAY']\n","                    elif option == menu_options[1]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['BENCHMARK']\n","                    elif option == menu_options[2]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['LEADERBOARDS']\n","                    elif option == menu_options[3]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['QUIT']\n","                else:\n","                    option.hovered = False\n","\n","            self.window.blit(img, img_rect.bottomleft)\n","            pygame.display.update()\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                                 4 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                                 12 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                self.select_speed()\n","                self.reset_game()\n","                self.start_match()\n","                score = self.single_player()\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                self.select_speed()\n","                score = []\n","\n","                for i in range(var.BENCHMARK):\n","                    self.reset_game()\n","                    self.start_match()\n","                    score.append(self.single_player())\n","\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game.\"\"\"\n","        menu_options = [None] * 5\n","\n","        menu_options[0] = TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                     4 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[1] = TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                     6 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[3] = TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                               10 * self.screen_rect.centery / 10),\n","                                               self.window, (1 / 15), \"menu\")\n","\n","        if isinstance(score, int):\n","            text_score = 'SCORE: ' + str(score)\n","\n","        else:\n","            text_score = 'MEAN SCORE: ' + str(sum(score) / var.BENCHMARK)\n","\n","            menu_options[2] = TextBlock(' ADD TO LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                                  8 * self.screen_rect.centery / 10),\n","                                                                  self.window, (1 / 15), \"menu\")\n","\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","\n","        menu_options[4] = TextBlock(text_score, (self.screen_rect.centerx,\n","                                                 15 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\")\n","\n","        while True:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['PLAY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['MENU']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['ADD_LEADERBOARDS']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    pygame.quit()\n","                                    sys.exit()\n","                    else:\n","                        option.hovered = False\n","\n","            pygame.display.update()\n","\n","    def single_player(self):\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where the snake keeps going each tick. It generate food, check\n","        # collisions and draw.\n","        while True:\n","            action = self.handle_input()\n","\n","            if self.play(action):\n","                return current_size\n","\n","            self.draw(color_list)\n","            current_size = self.snake.length # Update the body size\n","\n","            if current_size > previous_size:\n","                color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                           current_size)\n","\n","                previous_size = current_size\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body and re-\n","        turn.\"\"\"\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","\n","            return True\n","\n","        return False\n","\n","    def is_won(self):\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        return self.food_generator.generate_food(self.snake.body)\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            return actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            return actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            return actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            return actions['DOWN']\n","        else:\n","            return self.snake.previous_action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\"\"\"\n","        body = self.snake.return_body()\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        for part in body:\n","            canvas[part[0], part[1]] = point_type['BODY']\n","\n","        canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","        if self.local_state:\n","            canvas = self.eval_local_safety(canvas, body)\n","\n","        canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.set_food_on_screen(False)\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\"\"\"\n","        if self.game_over:\n","            return -1\n","        elif self.scored:\n","            return self.snake.length\n","\n","        return -0.005\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps.\n","\n","        If component is changed to 4, it does the same to RGBA colors.\"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","        pygame.display.update()\n","        self.fps.tick(var.GAME_SPEED)\n","\n","def resource_path(relative_path):\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","import numpy as np\n","\n","from random import sample, uniform\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 100, per = False, alpha = 0.6,\n","                 epsilon = 0.001, beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.per = per\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","        if self.per:\n","            self.per_epsilon = epsilon\n","            self.per_alpha = alpha\n","            self.per_beta = beta\n","            self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        if self.per:\n","            return self.exp\n","        else:\n","            return len(self.memory)\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.per_epsilon) ** self.per_alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        if self.per: # If using PER, insert in the max_priority.\n","            max_priority = self.memory.max_leaf()\n","\n","            if max_priority == 0:\n","                max_priority = self.get_priority(0)\n","\n","            self.memory.insert(experience, max_priority)\n","            self.exp += 1\n","        else: # Else, just append the experience to the list.\n","            self.memory.append(experience)\n","\n","            if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","                self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        if self.per:\n","            batch = [None] * batch_size\n","            IS_weights = np.zeros((batch_size, ))\n","            tree_indices = [0] * batch_size\n","\n","            memory_sum = self.memory.sum()\n","            len_seg = memory_sum / batch_size\n","            min_prob = self.memory.min_leaf() / memory_sum\n","\n","            for i in range(batch_size):\n","                val = uniform(len_seg * i, len_seg * (i + 1))\n","                tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","                prob = priority / self.memory.sum()\n","                IS_weights[i] = np.power(prob / min_prob, -self.per_beta)\n","\n","            return np.array(batch), IS_weights, tree_indices\n","\n","        else:\n","            IS_weights = np.ones((batch_size, ))\n","            batch = sample(self.memory, batch_size)\n","            return np.array(batch), IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        r = r.repeat(nb_actions).reshape((batch_size, nb_actions))\n","        game_over = game_over.repeat(nb_actions)\\\n","                             .reshape((batch_size, nb_actions))\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","            for i in range(batch_size):\n","                Qsa[i] = Y_target[i][actions[i]]\n","            Qsa = np.array(Qsa).repeat(nb_actions).reshape((batch_size, nb_actions))\n","\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1).repeat(nb_actions)\\\n","                                                .reshape((batch_size, nb_actions))\n","\n","        # The targets here already take into account\n","        delta = np.zeros((batch_size, nb_actions))\n","        a = np.cast['int'](a)\n","        delta[np.arange(batch_size), a] = 1\n","        targets = ((1 - delta) * Y[:batch_size]\n","                  + delta * (r + gamma * (1 - game_over) * Qsa))\n","\n","        if self.per: # Update the Sum Tree with the absolute error.\n","            errors = np.abs((targets - Y[:batch_size]).max(axis = 1)).clip(max = 1.)\n","            self.update(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.per:\n","            if self.memory_size <= 0:\n","                self.memory_size = 150000\n","\n","            self.memory = SumTree(self.memory_size)\n","            self.exp = 0\n","        else:\n","            self.memory = []\n","\n","\n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms:\n","    * Simple DQN (with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double DQN;\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling DQN;\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * DQN + PER;\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns.\n","        Paper: https://arxiv.org/pdf/1703.01327\n","\n","Arguments:\n","    --load FILE.h5: load a previously trained model in '.h5' format.\n","    --board_size INT: assign the size of the board, default = 10\n","    --nb_frames INT: assign the number of frames per stack, default = 4.\n","    --nb_actions INT: assign the number of actions possible, default = 5.\n","    --update_freq INT: assign how often, in epochs, to update the target,\n","      default = 500.\n","    --visual: select wheter or not to draw the game in pygame.\n","    --double: use a target network with double DQN logic.\n","    --dueling: use dueling network logic, Q(s,a) = A + V.\n","    --per: use Prioritized Experience Replay (based on Sum Trees).\n","    --local_state: Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from os import path, environ, sys\n","import random\n","\n","import inspect # Making relative imports from parallel folders possible\n","currentdir = path.dirname(path.abspath(inspect.getfile(inspect.currentframe())))\n","parentdir = path.dirname(currentdir)\n","sys.path.insert(0, parentdir)\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import load_model, Sequential\n","from keras.layers import *\n","from keras import backend as K\n","K.set_image_data_format('channels-first')\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes:\n","    memory: memory used in the model. Input memory or ExperienceReplay.\n","    model: the input model, Conv2D in Keras.\n","    target: the target model, used to calculade the fixed Q-targets.\n","    nb_frames: ammount of frames for each sars.\n","    frames: the frames in each sars.\n","    per: flag for PER usage.\n","    \"\"\"\n","    def __init__(self, model, target, memory = None, memory_size = 150000,\n","                 nb_frames = 4, board_size = 10, per = False):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if memory:\n","            self.memory = memory\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size, per = per)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.frames = None\n","        self.target_updates = 0\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\"\"\"\n","        if game.game_over:\n","            frame = np.zeros((self.board_size, self.board_size))\n","        else:\n","            frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        return np.expand_dims(self.frames, 0)\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target_updates += 1\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, history_loss,\n","                      history_step, history_reward, policy, value, win_count,\n","                      verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    sum(history_size[-10:]) / 10,\n","                                    max(history_size[-10:]),\n","                                    sum(history_step[-10:]) / 10,\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}' # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            # Print training performance\n","            text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                          + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                          + 'Mean loss - 100 episodes: {:.4f}')\n","            print(text_perf.format(history_loss[-1],\n","                                   history_loss[-1] / history_step[-1],\n","                                   sum(history_loss[-100:]) / 100))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   sum(history_step[-100:]) / 100))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\"\"\"\n","        loss = 0.\n","\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, update_target_freq = 0.001, optim_rounds = 1,\n","              policy = \"EpsGreedyQPolicy\", verbose = 1, n_steps = None):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        history_size = []\n","        history_step = []\n","        history_loss = []\n","        history_reward = []\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        if policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                    text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                                  + '| Loss: {:.4f}')\n","                    print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    if n_steps is not None:\n","                        n_step_buffer = []\n","                    game.reset_game()\n","                    self.clear_frames()\n","\n","                    S = self.get_game_data(game)\n","\n","                    while not game.game_over:\n","                        game.food_pos = game.generate_food()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","\n","                        game.play(action)\n","\n","                        r = game.get_reward()\n","                        total_reward += r\n","                        if n_steps is not None:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience) # Add to the memory\n","                        S = S_prime # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe: # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1 # Counter for metric purposes\n","\n","                    if self.per: # Advance beta, used in PER\n","                        self.memory.per_beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None: # Update the target model\n","                        if epoch % update_target_freq == 0:\n","                            self.update_target_model()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch, nb_epoch, history_size,\n","                                           history_loss, history_step,\n","                                           history_reward, policy, value,\n","                                           win_count, verbose)\n","\n","    def play(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","        result_size = []\n","        result_step = []\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","            S = self.get_game_data(game)\n","\n","            if visual:\n","                game.create_window()\n","                # The main loop, it pump key_presses and update every tick.\n","                environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","                previous_size = game.snake.length # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","\n","            while not game.game_over:\n","                action, value = q_policy.select_action(self.model, S, epoch, nb_actions)\n","                game.play(action)\n","                current_size = game.snake.length # Update the body size\n","\n","                if visual:\n","                    game.draw(color_list)\n","\n","                    if current_size > previous_size:\n","                        color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   game.snake.length)\n","\n","                        previous_size = current_size\n","\n","                S = self.get_game_data(game)\n","\n","                if game.game_over:\n","                    result_size.append(current_size)\n","                    result_step.append(game.step)\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(result_size), np.max(result_size),\n","                      np.min(result_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(result_step), np.max(result_step),\\\n","                      np.min(result_step)))\n","\n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","#!/usr/bin/env python\n","\n","\"\"\"clipped_error: L1 for errors < clip_value else L2 error.\n","\n","Functions:\n","    huber_loss: Return L1 error if absolute error is less than clip_value, else\n","                return L2 error.\n","    clipped_error: Call huber_loss with default clip_value to 1.0.\n","\"\"\"\n","\n","import numpy as np\n","from keras import backend as K\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\tif K.backend() == 'tensorflow':\n","\t\tif hasattr(tf, 'select'):\n","\t\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\t\telse:\n","\t\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\telif K.backend() == 'theano':\n","\t\tfrom theano import tensor as T\n","\t\treturn T.switch(condition, squared_loss, linear_loss)\n","\telse:\n","\t\traise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","#def CNN1(optimizer, loss, stack, input_size, output_size):\n"," #   model = Sequential()\n","  #  model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (stack,\n","   #                                                                  input_size,\n","    #                                                                 input_size)))\n","#    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n"," #   model.add(Conv2D(128, (3, 3), activation = 'relu'))\n","  #  model.add(Conv2D(256, (3, 3), activation = 'relu'))\n","   # model.add(Flatten())\n","    #model.add(Dense(1024, activation = 'relu'))\n","    #model.add(Dense(output_size))\n","    #model.compile(optimizer = optimizer, loss = loss)\n","\n","    #return model\n","    \n","def CNN4(optimizer, loss, stack, input_size, output_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    model = Sequential()\n","    model.add(Conv2D(32, (4, 4), activation = 'relu', input_shape = (stack,\n","                                                                    input_size,\n","                                                                    input_size)))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Flatten())\n","    model.add(Dense(3136, activation = 'relu'))\n","    model.add(Dense(output_size))\n","    model.compile(optimizer = optimizer, loss = loss)\n","\n","    return model\n","  \n","board_size = 10\n","nb_frames = 4\n","  \n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","model = CNN4(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = -1,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","agent.train(game, batch_size = 128, nb_epoch = 10000, gamma = 0.99, policy = \"EpsGreedyQPolicy\")"],"execution_count":1,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-815dc29aaec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_image_dim_ordering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'th'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0m__author__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Victor Neves\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'set_image_dim_ordering'"]}]},{"cell_type":"code","metadata":{"id":"MG3kLZDaeHyG","executionInfo":{"status":"aborted","timestamp":1612963375256,"user_tz":180,"elapsed":12339,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["model.save('keras.h5')\n","\n","!zip -r model-epsgreedy-benchmark.zip keras.h5 \n","from google.colab import files\n","files.download('model-epsgreedy-benchmark.zip')\n","model = load_model('keras.h5', custom_objects={'clipped_error': clipped_error})\n","\n","board_size = 10\n","nb_frames = 4\n","nb_actions = 5\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = 1500000,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","agent.play(game, visual = False, nb_epoch = 10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_l6c8QZJrftP","executionInfo":{"status":"aborted","timestamp":1612963375260,"user_tz":180,"elapsed":12335,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by AI algorithms.\n","\"\"\"\n","\n","import sys # To close the window when the game is over\n","from os import environ, path # To center the game window the best possible\n","import random # Random numbers used for the food\n","import logging # Logging function for movements and errors\n","from itertools import tee # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4, 'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes:\n","        BLOCK_SIZE: The size in pixels of a block.\n","        HEAD_COLOR: Color of the head.\n","        BODY_COLOR: Color of the body.\n","        FOOD_COLOR: Color of the food.\n","        GAME_SPEED: Speed in ticks of the game. The higher the faster.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Initialize all global variables.\"\"\"\n","        self.BOARD_SIZE = 30\n","        self.BLOCK_SIZE = 20\n","        self.HEAD_COLOR = (0, 0, 0)\n","        self.BODY_COLOR = (0, 200, 0)\n","        self.FOOD_COLOR = (200, 0, 0)\n","        self.GAME_SPEED = 10\n","        self.BENCHMARK = 10\n","\n","        if self.BOARD_SIZE > 50:\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                return pygame.Color(42, 42, 42)\n","            else:\n","                return pygame.Color(152, 152, 152)\n","\n","        return pygame.Color(42, 42, 42)\n","\n","    def get_background(self):\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                return pygame.Color(152, 152, 152)\n","\n","        return None\n","\n","    def set_rect(self):\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes:\n","        head: The head of the snake, located according to the board size.\n","        body: Starts with 3 parts and grows when food is eaten.\n","        orientation: Current orientation where head is pointing.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else (food), return without popping.\"\"\"\n","        if action == actions['IDLE']\\\n","            or (action, self.previous_action) in forbidden_moves:\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            return True\n","        else:\n","            self.body.pop()\n","\n","            return False\n","\n","    def return_body(self):\n","        \"\"\"Return the whole body.\"\"\"\n","        return self.body\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes:\n","        pos: Current position of food.\n","        is_food_on_screen: Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","    def set_food_on_screen(self, bool_value):\n","        \"\"\"Set flag for existence (or not) of food.\"\"\"\n","        self.is_food_on_screen = bool_value\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes:\n","        window: pygame window to show the game.\n","        fps: Define Clock and ticks in which the game will be displayed.\n","        snake: The actual snake who is going to be played.\n","        food_generator: Generator of food which responds to the snake.\n","        food_pos: Position of the food on the board.\n","        game_over: Flag for game_over.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score.\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.fps = pygame.time.Clock()\n","\n","    def menu(self):\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images/snake_logo.png\"))\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE, int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","\n","        self.screen_rect = self.window.get_rect()\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                                  4 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                                  6 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                     8 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","\n","        while True:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                option.draw()\n","\n","                if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                    option.hovered = True\n","\n","                    if option == menu_options[0]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['PLAY']\n","                    elif option == menu_options[1]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['BENCHMARK']\n","                    elif option == menu_options[2]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['LEADERBOARDS']\n","                    elif option == menu_options[3]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['QUIT']\n","                else:\n","                    option.hovered = False\n","\n","            self.window.blit(img, img_rect.bottomleft)\n","            pygame.display.update()\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                                 4 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                                 12 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                self.select_speed()\n","                self.reset_game()\n","                self.start_match()\n","                score = self.single_player()\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                self.select_speed()\n","                score = []\n","\n","                for i in range(var.BENCHMARK):\n","                    self.reset_game()\n","                    self.start_match()\n","                    score.append(self.single_player())\n","\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game.\"\"\"\n","        menu_options = [None] * 5\n","\n","        menu_options[0] = TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                     4 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[1] = TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                     6 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[3] = TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                               10 * self.screen_rect.centery / 10),\n","                                               self.window, (1 / 15), \"menu\")\n","\n","        if isinstance(score, int):\n","            text_score = 'SCORE: ' + str(score)\n","\n","        else:\n","            text_score = 'MEAN SCORE: ' + str(sum(score) / var.BENCHMARK)\n","\n","            menu_options[2] = TextBlock(' ADD TO LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                                  8 * self.screen_rect.centery / 10),\n","                                                                  self.window, (1 / 15), \"menu\")\n","\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","\n","        menu_options[4] = TextBlock(text_score, (self.screen_rect.centerx,\n","                                                 15 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\")\n","\n","        while True:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['PLAY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['MENU']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['ADD_LEADERBOARDS']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    pygame.quit()\n","                                    sys.exit()\n","                    else:\n","                        option.hovered = False\n","\n","            pygame.display.update()\n","\n","    def single_player(self):\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where the snake keeps going each tick. It generate food, check\n","        # collisions and draw.\n","        while True:\n","            action = self.handle_input()\n","\n","            if self.play(action):\n","                return current_size\n","\n","            self.draw(color_list)\n","            current_size = self.snake.length # Update the body size\n","\n","            if current_size > previous_size:\n","                color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                           current_size)\n","\n","                previous_size = current_size\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body and re-\n","        turn.\"\"\"\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","\n","            return True\n","\n","        return False\n","\n","    def is_won(self):\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        return self.food_generator.generate_food(self.snake.body)\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            return actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            return actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            return actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            return actions['DOWN']\n","        else:\n","            return self.snake.previous_action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\"\"\"\n","        body = self.snake.return_body()\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        for part in body:\n","            canvas[part[0], part[1]] = point_type['BODY']\n","\n","        canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","        if self.local_state:\n","            canvas = self.eval_local_safety(canvas, body)\n","\n","        canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.set_food_on_screen(False)\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\"\"\"\n","        if self.game_over:\n","            return -1\n","        elif self.scored:\n","            return self.snake.length\n","\n","        return -0.005\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps.\n","\n","        If component is changed to 4, it does the same to RGBA colors.\"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","        pygame.display.update()\n","        self.fps.tick(var.GAME_SPEED)\n","\n","def resource_path(relative_path):\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","import numpy as np\n","from random import sample, uniform\n","from array import array  # Efficient numeric arrays\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes\n","    ----------\n","    memory: list of experiences\n","        Memory list to insert experiences.\n","    memory_size: int, optional, default = 150000\n","        The ammount of experiences to be stored in the memory.\n","    input_shape: tuple of 3 * int\n","        The shape of the input which will be stored.\n","    \"\"\"\n","    def __init__(self, memory_size = 150000):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        self.memory.append(experience)\n","\n","        if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","            self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\n","\n","        Return\n","        ----------\n","        batch: np.array of batch_size experiences\n","            The batched experiences from memory.\n","        IS_weights: np.array of batch_size of the weights\n","            As it's used only in PER, is an array of ones in this case.\n","        Indexes: list of batch_size * int\n","            As it's used only in PER, return None.\n","        \"\"\"\n","        IS_weights = np.ones((batch_size, ))\n","        batch = np.array(sample(self.memory, batch_size))\n","\n","        return batch, IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        self.memory = []\n","\n","\n","class PrioritizedExperienceReplayNaive:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 150000, alpha = 0.6, epsilon = 0.001,\n","                 beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return self.exp\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        max_priority = self.memory.max_leaf()\n","\n","        if max_priority == 0:\n","            max_priority = self.get_priority(0)\n","\n","        self.memory.insert(experience, max_priority)\n","        self.exp += 1\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        batch = [None] * batch_size\n","        IS_weights = np.zeros((batch_size, ))\n","        tree_indices = [0] * batch_size\n","\n","        memory_sum = self.memory.sum()\n","        len_seg = memory_sum / batch_size\n","        min_prob = self.memory.min_leaf() / memory_sum\n","\n","        for i in range(batch_size):\n","            val = uniform(len_seg * i, len_seg * (i + 1))\n","            tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","            prob = priority / self.memory.sum()\n","            IS_weights[i] = np.power(prob / min_prob, -self.beta)\n","\n","        return np.array(batch), IS_weights, tree_indices\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = SumTree(self.memory_size)\n","        self.exp = 0\n","\n","\n","class PrioritizedExperienceReplay:\n","    def __init__(self, memory_size, nb_epoch = 10000, epsilon = 0.001,\n","                 alpha = 0.6, beta = 0.4, decay = 0.5):\n","        self.memory_size = memory_size\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.max_priority = 1.0\n","        self.reset_memory()\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","        if self.exp_size() < self.memory_size:\n","            self.memory.append(experience)\n","            self.pos += 1\n","        else:\n","            self.memory[self.pos] = experience\n","            self.pos = (self.pos + 1) % self.memory_size\n","\n","        self._it_sum[self.pos] = self.max_priority ** self.alpha\n","        self._it_min[self.pos] = self.max_priority ** self.alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = array('i')\n","\n","        for _ in range(batch_size):\n","            mass = random.random() * self._it_sum.sum(0, self.exp_size() - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","\n","        return res\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def get_samples(self, batch_size):\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = array('f')\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * self.exp_size()) ** (-self.beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * self.exp_size()) ** (-self.beta)\n","            weights.append(weight / max_weight)\n","\n","        weights = np.array(weights, dtype=np.float32)\n","        samples = [self.memory[idx] for idx in idxes]\n","\n","        return np.array(samples), weights, idxes\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def update_priorities(self, idxes, errors):\n","        priorities = self.get_priority(errors)\n","\n","        for idx, priority in zip(idxes, priorities):\n","            self._it_sum[idx] = priority ** self.alpha\n","            self._it_min[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = []\n","        self.pos = 0\n","\n","        it_capacity = 1\n","\n","        while it_capacity < self.memory_size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n","\n","\n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms:\n","    * Simple DQN (with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double DQN;\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling DQN;\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * DQN + PER;\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns.\n","        Paper: https://arxiv.org/pdf/1703.01327\n","\n","Arguments:\n","    --load FILE.h5: load a previously trained model in '.h5' format.\n","    --board_size INT: assign the size of the board, default = 10\n","    --nb_frames INT: assign the number of frames per stack, default = 4.\n","    --nb_actions INT: assign the number of actions possible, default = 5.\n","    --update_freq INT: assign how often, in epochs, to update the target,\n","      default = 500.\n","    --visual: select wheter or not to draw the game in pygame.\n","    --double: use a target network with double DQN logic.\n","    --dueling: use dueling network logic, Q(s,a) = A + V.\n","    --per: use Prioritized Experience Replay (based on Sum Trees).\n","    --local_state: Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from os import path, environ, sys\n","import random\n","\n","import inspect # Making relative imports from parallel folders possible\n","currentdir = path.dirname(path.abspath(inspect.getfile(inspect.currentframe())))\n","parentdir = path.dirname(currentdir)\n","sys.path.insert(0, parentdir)\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import load_model, Sequential\n","from keras.layers import *\n","from keras import backend as K\n","K.set_image_dim_ordering('th')\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes:\n","    memory: memory used in the model. Input memory or ExperienceReplay.\n","    model: the input model, Conv2D in Keras.\n","    target: the target model, used to calculade the fixed Q-targets.\n","    nb_frames: ammount of frames for each sars.\n","    frames: the frames in each sars.\n","    per: flag for PER usage.\n","    \"\"\"\n","    def __init__(self, model, target, memory = None, memory_size = 150000,\n","                 nb_frames = 4, board_size = 10, per = False):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if memory:\n","            self.memory = memory\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.frames = None\n","        self.target_updates = 0\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\"\"\"\n","        if game.game_over:\n","            frame = np.zeros((self.board_size, self.board_size))\n","        else:\n","            frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        return np.expand_dims(self.frames, 0)\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target_updates += 1\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, history_loss,\n","                      history_step, history_reward, policy, value, win_count,\n","                      verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    sum(history_size[-10:]) / 10,\n","                                    max(history_size[-10:]),\n","                                    sum(history_step[-10:]) / 10,\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}' # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            # Print training performance\n","            text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                          + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                          + 'Mean loss - 100 episodes: {:.4f}')\n","            print(text_perf.format(history_loss[-1],\n","                                   history_loss[-1] / history_step[-1],\n","                                   sum(history_loss[-100:]) / 100))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   sum(history_step[-100:]) / 100))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\"\"\"\n","        loss = 0.\n","\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, update_target_freq = 0.001, optim_rounds = 1,\n","              policy = \"EpsGreedyQPolicy\", verbose = 1, n_steps = None):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        history_size = []\n","        history_step = []\n","        history_loss = []\n","        history_reward = []\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        if policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                    text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                                  + '| Loss: {:.4f}')\n","                    print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    if n_steps is not None:\n","                        n_step_buffer = []\n","                    game.reset_game()\n","                    self.clear_frames()\n","\n","                    S = self.get_game_data(game)\n","\n","                    while not game.game_over:\n","                        game.food_pos = game.generate_food()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","\n","                        game.play(action)\n","\n","                        r = game.get_reward()\n","                        total_reward += r\n","                        if n_steps is not None:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience) # Add to the memory\n","                        S = S_prime # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe: # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1 # Counter for metric purposes\n","\n","                    if self.per: # Advance beta, used in PER\n","                        self.memory.per_beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None: # Update the target model\n","                        if epoch % update_target_freq == 0:\n","                            self.update_target_model()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch, nb_epoch, history_size,\n","                                           history_loss, history_step,\n","                                           history_reward, policy, value,\n","                                           win_count, verbose)\n","\n","    def play(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","        result_size = []\n","        result_step = []\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","            S = self.get_game_data(game)\n","\n","            if visual:\n","                game.create_window()\n","                # The main loop, it pump key_presses and update every tick.\n","                environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","                previous_size = game.snake.length # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","\n","            while not game.game_over:\n","                action, value = q_policy.select_action(self.model, S, epoch, nb_actions)\n","                game.play(action)\n","                current_size = game.snake.length # Update the body size\n","\n","                if visual:\n","                    game.draw(color_list)\n","\n","                    if current_size > previous_size:\n","                        color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   game.snake.length)\n","\n","                        previous_size = current_size\n","\n","                S = self.get_game_data(game)\n","\n","                if game.game_over:\n","                    result_size.append(current_size)\n","                    result_step.append(game.step)\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(result_size), np.max(result_size),\n","                      np.min(result_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(result_step), np.max(result_step),\\\n","                      np.min(result_step)))\n","\n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","#!/usr/bin/env python\n","\n","\"\"\"clipped_error: L1 for errors < clip_value else L2 error.\n","\n","Functions:\n","    huber_loss: Return L1 error if absolute error is less than clip_value, else\n","                return L2 error.\n","    clipped_error: Call huber_loss with default clip_value to 1.0.\n","\"\"\"\n","\n","import numpy as np\n","from keras import backend as K\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\tif K.backend() == 'tensorflow':\n","\t\tif hasattr(tf, 'select'):\n","\t\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\t\telse:\n","\t\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\telif K.backend() == 'theano':\n","\t\tfrom theano import tensor as T\n","\t\treturn T.switch(condition, squared_loss, linear_loss)\n","\telse:\n","\t\traise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","#def CNN1(optimizer, loss, stack, input_size, output_size):\n"," #   model = Sequential()\n","  #  model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (stack,\n","   #                                                                  input_size,\n","    #                                                                 input_size)))\n","#    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n"," #   model.add(Conv2D(128, (3, 3), activation = 'relu'))\n","  #  model.add(Conv2D(256, (3, 3), activation = 'relu'))\n","   # model.add(Flatten())\n","    #model.add(Dense(1024, activation = 'relu'))\n","    #model.add(Dense(output_size))\n","    #model.compile(optimizer = optimizer, loss = loss)\n","\n","    #return model\n","    \n","def CNN4(optimizer, loss, stack, input_size, output_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    model = Sequential()\n","    model.add(Conv2D(32, (4, 4), activation = 'relu', input_shape = (stack,\n","                                                                    input_size,\n","                                                                    input_size)))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Flatten())\n","    model.add(Dense(3136, activation = 'relu'))\n","    model.add(Dense(output_size))\n","    model.compile(optimizer = optimizer, loss = loss)\n","\n","    return model\n","  \n","board_size = 10\n","nb_frames = 4\n","  \n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","model = CNN4(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = -1,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","agent.train(game, batch_size = 64, nb_epoch = 10000, gamma = 0.95, policy = \"EpsGreedyQPolicy\")\n","\n","model.save('keras.h5')\n","\n","!zip -r model-epsgreedy-bench-newmemory.zip keras.h5 \n","from google.colab import files\n","files.download('model-epsgreedy-bench-newmemory.zip')\n","model = load_model('keras.h5', custom_objects={'clipped_error': clipped_error})\n","\n","board_size = 10\n","nb_frames = 4\n","nb_actions = 5\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = 1500000,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","agent.play(game, visual = False, nb_epoch = 10000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDiMrfWRDM8Q"},"source":["first cnn\n","https://medium.com/@kylepob61392/airplane-image-classification-using-a-keras-cnn-22be506fdb53\n","\n","conv model above.\n","https://github.com/MateLabs/All-Conv-Keras\n","\n","https://github.com/Kuax-Meat/Snake-Reinforcement-Deep-Q-Learning/blob/master/snaky.py\n","Apple = 2\n","Dead = -1\n","Alive = -0.1\n","\n","https://github.com/chuyangliu/Snake/blob/master/snake/solver/dqn/__init__.py\n","        self._RWD_EMPTY = -0.005\n","        self._RWD_DEAD = -0.5\n","        self._RWD_FOOD = 1.0\n","        \n"," "]},{"cell_type":"markdown","metadata":{"id":"bIYkKP659xU1"},"source":["# ACER"]},{"cell_type":"code","metadata":{"id":"GADkVOi8moM6","executionInfo":{"status":"aborted","timestamp":1612963375262,"user_tz":180,"elapsed":12332,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n","!pip3 install torchvision\n","#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by AI algorithms.\n","\"\"\"\n","\n","import sys # To close the window when the game is over\n","from os import environ, path # To center the game window the best possible\n","import random # Random numbers used for the food\n","import logging # Logging function for movements and errors\n","from itertools import tee # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4, 'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes:\n","        BLOCK_SIZE: The size in pixels of a block.\n","        HEAD_COLOR: Color of the head.\n","        BODY_COLOR: Color of the body.\n","        FOOD_COLOR: Color of the food.\n","        GAME_SPEED: Speed in ticks of the game. The higher the faster.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Initialize all global variables.\"\"\"\n","        self.BOARD_SIZE = 30\n","        self.BLOCK_SIZE = 20\n","        self.HEAD_COLOR = (0, 0, 0)\n","        self.BODY_COLOR = (0, 200, 0)\n","        self.FOOD_COLOR = (200, 0, 0)\n","        self.GAME_SPEED = 10\n","        self.BENCHMARK = 10\n","\n","        if self.BOARD_SIZE > 50:\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                return pygame.Color(42, 42, 42)\n","            else:\n","                return pygame.Color(152, 152, 152)\n","\n","        return pygame.Color(42, 42, 42)\n","\n","    def get_background(self):\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                return pygame.Color(152, 152, 152)\n","\n","        return None\n","\n","    def set_rect(self):\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes:\n","        head: The head of the snake, located according to the board size.\n","        body: Starts with 3 parts and grows when food is eaten.\n","        orientation: Current orientation where head is pointing.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else (food), return without popping.\"\"\"\n","        if action == actions['IDLE']\\\n","            or (action, self.previous_action) in forbidden_moves:\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            return True\n","        else:\n","            self.body.pop()\n","\n","            return False\n","\n","    def return_body(self):\n","        \"\"\"Return the whole body.\"\"\"\n","        return self.body\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes:\n","        pos: Current position of food.\n","        is_food_on_screen: Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","    def set_food_on_screen(self, bool_value):\n","        \"\"\"Set flag for existence (or not) of food.\"\"\"\n","        self.is_food_on_screen = bool_value\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes:\n","        window: pygame window to show the game.\n","        fps: Define Clock and ticks in which the game will be displayed.\n","        snake: The actual snake who is going to be played.\n","        food_generator: Generator of food which responds to the snake.\n","        food_pos: Position of the food on the board.\n","        game_over: Flag for game_over.\n","    \"\"\"\n","    def __init__(self, player, board_size = 10, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score.\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.fps = pygame.time.Clock()\n","\n","    def menu(self):\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images/snake_logo.png\"))\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE, int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","\n","        self.screen_rect = self.window.get_rect()\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                                  4 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                                  6 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                     8 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","\n","        while True:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                option.draw()\n","\n","                if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                    option.hovered = True\n","\n","                    if option == menu_options[0]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['PLAY']\n","                    elif option == menu_options[1]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['BENCHMARK']\n","                    elif option == menu_options[2]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['LEADERBOARDS']\n","                    elif option == menu_options[3]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['QUIT']\n","                else:\n","                    option.hovered = False\n","\n","            self.window.blit(img, img_rect.bottomleft)\n","            pygame.display.update()\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                                 4 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                                 12 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                self.select_speed()\n","                self.reset_game()\n","                self.start_match()\n","                score = self.single_player()\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                self.select_speed()\n","                score = []\n","\n","                for i in range(var.BENCHMARK):\n","                    self.reset_game()\n","                    self.start_match()\n","                    score.append(self.single_player())\n","\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game.\"\"\"\n","        menu_options = [None] * 5\n","\n","        menu_options[0] = TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                     4 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[1] = TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                     6 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[3] = TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                               10 * self.screen_rect.centery / 10),\n","                                               self.window, (1 / 15), \"menu\")\n","\n","        if isinstance(score, int):\n","            text_score = 'SCORE: ' + str(score)\n","\n","        else:\n","            text_score = 'MEAN SCORE: ' + str(sum(score) / var.BENCHMARK)\n","\n","            menu_options[2] = TextBlock(' ADD TO LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                                  8 * self.screen_rect.centery / 10),\n","                                                                  self.window, (1 / 15), \"menu\")\n","\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","\n","        menu_options[4] = TextBlock(text_score, (self.screen_rect.centerx,\n","                                                 15 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\")\n","\n","        while True:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['PLAY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['MENU']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['ADD_LEADERBOARDS']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    pygame.quit()\n","                                    sys.exit()\n","                    else:\n","                        option.hovered = False\n","\n","            pygame.display.update()\n","\n","    def single_player(self):\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where the snake keeps going each tick. It generate food, check\n","        # collisions and draw.\n","        while True:\n","            action = self.handle_input()\n","\n","            if self.play(action):\n","                return current_size\n","\n","            self.draw(color_list)\n","            current_size = self.snake.length # Update the body size\n","\n","            if current_size > previous_size:\n","                color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                           current_size)\n","\n","                previous_size = current_size\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body and re-\n","        turn.\"\"\"\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","\n","            return True\n","\n","        return False\n","\n","    def is_won(self):\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        return self.food_generator.generate_food(self.snake.body)\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            return actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            return actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            return actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            return actions['DOWN']\n","        else:\n","            return self.snake.previous_action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\"\"\"\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.return_body()\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = point_type['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas.flatten()\n","\n","    def relative_to_absolute(self, action):\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.set_food_on_screen(False)\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\"\"\"\n","        if self.game_over:\n","            return -1\n","        elif self.scored:\n","            return (self.snake.length - 3)\n","\n","        return -0.005\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps.\n","\n","        If component is changed to 4, it does the same to RGBA colors.\"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","        pygame.display.update()\n","        self.fps.tick(var.GAME_SPEED)\n","\n","def resource_path(relative_path):\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","import torch\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from os import path, environ, sys\n","from collections import namedtuple\n","\n","import inspect # Making relative imports from parallel folders possible\n","currentdir = path.dirname(path.abspath(inspect.getfile(inspect.currentframe())))\n","parentdir = path.dirname(currentdir)\n","sys.path.insert(0, parentdir)\n","\n","Transition = namedtuple('Transition', ('states', 'actions', 'rewards', 'next_states',\n","                                       'done', 'exploration_statistics'))\n","\n","######\n","# GAME PARAM\n","######\n","ACTION_SPACE_DIM = 5\n","CONTROL = 'discrete'\n","STATE_SPACE_DIM = 100\n","\n","# Parameters that work well for CartPole-v0\n","LEARNING_RATE = 1e-1\n","REPLAY_BUFFER_SIZE = 1000000\n","TRUNCATION_PARAMETER = 10\n","DISCOUNT_FACTOR = 0.99\n","REPLAY_RATIO = 4\n","MAX_EPISODES = 1000\n","MAX_STEPS_BEFORE_UPDATE = 20\n","NUMBER_OF_AGENTS = 12\n","OFF_POLICY_MINIBATCH_SIZE = 16\n","TRUST_REGION_CONSTRAINT = 1.\n","TRUST_REGION_DECAY = 0.99\n","ENTROPY_REGULARIZATION = 1e-3\n","MAX_REPLAY_SIZE = 200\n","ACTOR_LOSS_WEIGHT = 0.1\n","\n","######\n","# REPLAY\n","######\n","import random\n","from itertools import zip_longest\n","from collections import deque, namedtuple\n","\n","Transition = namedtuple('Transition', ('states', 'actions', 'rewards', 'next_states',\n","                                       'done', 'exploration_statistics'))\n","\n","\n","class ReplayBuffer:\n","    \"\"\"\n","    Replay buffer for the agents.\n","    \"\"\"\n","    def __init__(self):\n","        self.episodes = deque([[]], maxlen=REPLAY_BUFFER_SIZE)\n","\n","    def add(self, transition):\n","        \"\"\"\n","        Add a new transition to the buffer.\n","        Parameters\n","        ----------\n","        transition : Transition\n","            The transition to add.\n","        \"\"\"\n","        if self.episodes[-1] and self.episodes[-1][-1].done[0, 0]:\n","            self.episodes.append([])\n","        self.episodes[-1].append(transition)\n","\n","    def sample(self, batch_size, window_length=float('inf')):\n","        \"\"\"\n","        Sample a batch of trajectories from the buffer. If they are of unequal length\n","        (which is likely), the trajectories will be padded with zero-reward transitions.\n","        Parameters\n","        ----------\n","        batch_size : int\n","            The batch size of the sample.\n","        window_length : int, optional\n","            The window length.\n","        Returns\n","        -------\n","        list of Transition's\n","            A batched sampled trajectory.\n","        \"\"\"\n","        batched_trajectory = []\n","        trajectory_indices = random.sample(range(len(self.episodes) - 1), min(batch_size, len(self.episodes)-1))\n","        trajectories = []\n","        for trajectory in [self.episodes[index] for index in trajectory_indices]:\n","            start = random.sample(range(len(trajectory)), 1)[0]\n","            trajectories.append(trajectory[start:start + window_length])\n","        smallest_trajectory_length = min([len(trajectory) for trajectory in trajectories]) if trajectories else 0\n","        for index in range(len(trajectories)):\n","            trajectories[index] = trajectories[index][-smallest_trajectory_length:]\n","        for transitions in zip(*trajectories):\n","            batched_transition = Transition(*[torch.cat(data, dim=0) for data in zip(*transitions)])\n","            batched_trajectory.append(batched_transition)\n","\n","        return batched_trajectory\n","\n","class ActorCritic(torch.nn.Module):\n","    \"\"\"\n","    Actor-critic network used in A3C and ACER.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, *input):\n","        raise NotImplementedError\n","\n","    def copy_parameters_from(self, source, decay=0.):\n","        \"\"\"\n","        Copy the parameters from another network.\n","        Parameters\n","        ----------\n","        source : ActorCritic\n","            The network from which to copy the parameters.\n","        decay : float, optional\n","            How much decay should be applied? Default is 0., which means the parameters\n","            are completely copied.\n","        \"\"\"\n","        for parameter, source_parameter in zip(self.parameters(), source.parameters()):\n","            parameter.data.copy_(decay * parameter.data + (1 - decay) * source_parameter.data)\n","\n","    def copy_gradients_from(self, source):\n","        \"\"\"\n","        Copy the gradients from another network.\n","        Parameters\n","        ----------\n","        source : ActorCritic\n","            The network from which to copy the gradients.\n","        \"\"\"\n","        for parameter, source_parameter in zip(self.parameters(), source.parameters()):\n","            parameter._grad = source_parameter.grad\n","\n","\n","class DiscreteActorCritic(ActorCritic):\n","    \"\"\"\n","    Discrete actor-critic network used in A3C and ACER.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.input_layer = torch.nn.Linear(STATE_SPACE_DIM, 32)\n","        self.hidden_layer = torch.nn.Linear(32, 32)\n","        self.action_layer = torch.nn.Linear(32, ACTION_SPACE_DIM)\n","        self.action_value_layer = torch.nn.Linear(32, ACTION_SPACE_DIM)\n","\n","    def forward(self, states):\n","        \"\"\"\n","        Compute a forward pass in the network.\n","        Parameters\n","        ----------\n","        states : torch.Tensor\n","            The states for which the action probabilities and the action-values must be computed.\n","        Returns\n","        -------\n","        action_probabilities : torch.Tensor\n","            The action probabilities of the policy according to the actor.\n","        action_probabilities : torch.Tensor\n","            The action-values of the policy according to the critic.\n","        \"\"\"\n","        hidden = F.relu(self.input_layer(states))\n","        hidden = F.relu(self.hidden_layer(hidden))\n","        action_probabilities = F.softmax(self.action_layer(hidden))\n","        action_values = self.action_value_layer(hidden)\n","\n","        return action_probabilities, action_values\n","\n","\n","class Brain:\n","    \"\"\"\n","    A centralized brain for the agents.\n","    \"\"\"\n","    def __init__(self):\n","        self.actor_critic = None\n","        self.average_actor_critic = None\n","\n","class DiscreteBrain(Brain):\n","    def __init__(self):\n","        super().__init__()\n","        self.actor_critic = DiscreteActorCritic()\n","        self.actor_critic.share_memory()\n","        self.average_actor_critic = DiscreteActorCritic()\n","        self.average_actor_critic.share_memory()\n","        self.average_actor_critic.copy_parameters_from(self.actor_critic)\n","\n","brain = DiscreteBrain()\n","\n","class Agent:\n","    \"\"\"\n","    Agent that learns an optimal policy using ACER.\n","    Parameters\n","    ----------\n","    brain : brain.Brain\n","        The brain to update.\n","    render : boolean, optional\n","        Should the agent render its actions in the on-policy phase?\n","    verbose : boolean, optional\n","        Should the agent print progress to the console?\n","    \"\"\"\n","    def __init__(self, brain, render=False, verbose=True):\n","        self.env = Game(board_size = int(np.sqrt(STATE_SPACE_DIM)), player = \"ROBOT\")\n","        self.env.reset_game()\n","        self.verbose = verbose\n","        self.buffer = ReplayBuffer()\n","        self.brain = brain\n","        self.optimizer = torch.optim.Adam(brain.actor_critic.parameters(),\n","                                          lr=LEARNING_RATE)\n","\n","\n","class DiscreteAgent(Agent):\n","    def __init__(self, brain, render=False, verbose=True):\n","        super().__init__(brain, render, verbose)\n","\n","    def run(self):\n","        \"\"\"\n","        Run the agent for several episodes.\n","        \"\"\"\n","        for episode in range(MAX_EPISODES):\n","            episode_rewards = 0.\n","            end_of_episode = False\n","            while not end_of_episode:\n","                trajectory = self.explore(self.brain.actor_critic)\n","                self.learning_iteration(trajectory)\n","                end_of_episode = trajectory[-1].done[0, 0]\n","                episode_rewards += sum([transition.rewards[0, 0] for transition in trajectory])\n","                for trajectory_count in range(np.random.poisson(REPLAY_RATIO)):\n","                    trajectory = self.buffer.sample(OFF_POLICY_MINIBATCH_SIZE, MAX_REPLAY_SIZE)\n","                    if trajectory:\n","                        self.learning_iteration(trajectory)\n","            if self.verbose:\n","                print(\"Episode: {:04d} | Reward: {:.3f}\".format(episode, episode_rewards))\n","\n","    def learning_iteration(self, trajectory):\n","        \"\"\"\n","        Conduct a single discrete learning iteration. Analogue of Algorithm 2 in the paper.\n","        \"\"\"\n","        actor_critic = DiscreteActorCritic()\n","        actor_critic.copy_parameters_from(self.brain.actor_critic)\n","\n","        _, _, _, next_states, _, _ = trajectory[-1]\n","        action_probabilities, action_values = actor_critic(Variable(next_states))\n","        retrace_action_value = (action_probabilities * action_values).data.sum(-1).unsqueeze(-1)\n","\n","        for states, actions, rewards, _, done, exploration_probabilities in reversed(trajectory):\n","            action_probabilities, action_values = actor_critic(Variable(states))\n","            average_action_probabilities, _ = self.brain.average_actor_critic(Variable(states))\n","            value = (action_probabilities * action_values).data.sum(-1).unsqueeze(-1) * (1. - done)\n","            action_indices = Variable(actions.long())\n","\n","            importance_weights = action_probabilities.data / exploration_probabilities\n","\n","            naive_advantage = action_values.gather(-1, action_indices).data - value\n","            retrace_action_value = rewards + DISCOUNT_FACTOR * retrace_action_value * (1. - done)\n","            retrace_advantage = retrace_action_value - value\n","\n","            # Actor\n","            actor_loss = - ACTOR_LOSS_WEIGHT * Variable(\n","                importance_weights.gather(-1, action_indices.data).clamp(max=TRUNCATION_PARAMETER) * retrace_advantage) \\\n","                * action_probabilities.gather(-1, action_indices).log()\n","            bias_correction = - ACTOR_LOSS_WEIGHT * Variable((1 - TRUNCATION_PARAMETER / importance_weights).clamp(min=0.) *\n","                                      naive_advantage * action_probabilities.data) * action_probabilities.log()\n","            actor_loss += bias_correction.sum(-1).unsqueeze(-1)\n","            actor_gradients = torch.autograd.grad(actor_loss.mean(), action_probabilities, retain_graph=True)\n","            actor_gradients = self.discrete_trust_region_update(actor_gradients, action_probabilities,\n","                                                       Variable(average_action_probabilities.data))\n","            action_probabilities.backward(actor_gradients, retain_graph=True)\n","\n","            # Critic\n","            critic_loss = (action_values.gather(-1, action_indices) - Variable(retrace_action_value)).pow(2)\n","            critic_loss.mean().backward(retain_graph=True)\n","\n","            # Entropy\n","            entropy_loss = ENTROPY_REGULARIZATION * (action_probabilities * action_probabilities.log()).sum(-1)\n","            entropy_loss.mean().backward(retain_graph=True)\n","\n","            retrace_action_value = importance_weights.gather(-1, action_indices.data).clamp(max=1.) * \\\n","                                   (retrace_action_value - action_values.gather(-1, action_indices).data) + value\n","        self.brain.actor_critic.copy_gradients_from(actor_critic)\n","        self.optimizer.step()\n","        self.brain.average_actor_critic.copy_parameters_from(self.brain.actor_critic, decay=TRUST_REGION_DECAY)\n","\n","    def explore(self, actor_critic):\n","        \"\"\"\n","        Explore an environment by taking a sequence of actions and saving the results in the memory.\n","        Parameters\n","        ----------\n","        actor_critic : ActorCritic\n","            The actor-critic model to use to explore.\n","        \"\"\"\n","        state = torch.FloatTensor(self.env.state().flatten())\n","        trajectory = []\n","        for step in range(MAX_STEPS_BEFORE_UPDATE):\n","            self.env.food_pos = self.env.generate_food()\n","            action_probabilities, *_ = actor_critic(Variable(state))\n","            action = action_probabilities.multinomial(num_samples = 1)\n","            action = action.data\n","            exploration_statistics = action_probabilities.data.view(1, -1)\n","            self.env.play(action.numpy()[0])\n","\n","            reward = self.env.get_reward()\n","            next_state = self.env.state().flatten()\n","            done = self.env.game_over\n","\n","            next_state = torch.from_numpy(next_state).float()\n","            transition = Transition(states=state.view(1, -1),\n","                                                  actions=action.view(1, -1),\n","                                                  rewards=torch.FloatTensor([[reward]]),\n","                                                  next_states=next_state.view(1, -1),\n","                                                  done=torch.FloatTensor([[done]]),\n","                                                  exploration_statistics=exploration_statistics)\n","            self.buffer.add(transition)\n","            trajectory.append(transition)\n","            if done:\n","                print(\"Score: {:02d} | \".format(self.env.snake.length), end = \"\")\n","                self.env.reset_game()\n","                break\n","            else:\n","                state = next_state\n","        return trajectory\n","\n","    @staticmethod\n","    def discrete_trust_region_update(actor_gradients, action_probabilities, average_action_probabilities):\n","        \"\"\"\n","        Update the actor gradients so that they satisfy a linearized KL constraint with respect\n","        to the average actor-critic network. See Section 3.3 of the paper for details.\n","        Parameters\n","        ----------\n","        actor_gradients : tuple of torch.Tensor's\n","            The original gradients.\n","        action_probabilities\n","            The action probabilities according to the current actor-critic network.\n","        average_action_probabilities\n","            The action probabilities according to the average actor-critic network.\n","        Returns\n","        -------\n","        tuple of torch.Tensor's\n","            The updated gradients.\n","        \"\"\"\n","        negative_kullback_leibler = - ((average_action_probabilities.log() - action_probabilities.log())\n","                                       * average_action_probabilities).sum(-1)\n","        kullback_leibler_gradients = torch.autograd.grad(negative_kullback_leibler.mean(),\n","                                                         action_probabilities, retain_graph=True)\n","        updated_actor_gradients = []\n","        for actor_gradient, kullback_leibler_gradient in zip(actor_gradients, kullback_leibler_gradients):\n","            scale = actor_gradient.mul(kullback_leibler_gradient).sum(-1).unsqueeze(-1) - TRUST_REGION_CONSTRAINT\n","            scale = torch.div(scale, actor_gradient.mul(actor_gradient).sum(-1).unsqueeze(-1)).clamp(min=0.)\n","            updated_actor_gradients.append(actor_gradient - scale * kullback_leibler_gradient)\n","\n","        return updated_actor_gradients\n","\n","######\n","# MAIN\n","######\n","from torch import multiprocessing as mp\n","\n","def run_agent(shared_brain, render=False, verbose=True):\n","    \"\"\"\n","    Run the agent.\n","    Parameters\n","    ----------\n","    shared_brain : brain.Brain\n","        The shared brain the agents will use and update.\n","    render : boolean, optional\n","        Should the agent render its actions in the on-policy phase?\n","    verbose : boolean, optional\n","        Should the agent print progress to the console?\n","    \"\"\"\n","    local_agent = DiscreteAgent(shared_brain, render, verbose)\n","    local_agent.run()\n","\n","\n","if __name__ == \"__main__\":\n","    if NUMBER_OF_AGENTS == 1:\n","        # Don't bother with multiprocessing if only one agent\n","        run_agent(brain, render=False)\n","    else:\n","        processes = [mp.Process(target=run_agent, args=(brain, False, True))\n","                     for _ in range(NUMBER_OF_AGENTS)]\n","        for process in processes:\n","            process.start()\n","\n","        for process in processes:\n","            process.join()\n","\n","\n","def test():\n","    run_agent(brain.brain, render=False)\n","\n","\n","######\n","# ORNS\n","######\n","class OrnsteinUhlenbeckProcess:\n","    def __init__(self, theta, mu, sigma, time_scale=1e-1,\n","                 size=1, initial_value=None):\n","        self.theta = theta\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.time_scale = time_scale\n","        self.size = size\n","        self.initial_value = initial_value if initial_value is not None else np.zeros(size)\n","        self.previous_value = self.initial_value\n","\n","    def sample(self):\n","        value = self.previous_value\n","        value += self.theta * (self.mu - self.previous_value) * self.time_scale\n","        value += self.sigma * np.sqrt(self.time_scale) * np.random.normal(size=self.size)\n","        return value\n","\n","    def reset(self):\n","        self.previous_value = self.initial_value\n","\n","    def sampling_parameters(self):\n","        mean = self.previous_value + self.theta * (self.mu - self.previous_value) * self.time_scale\n","        sd = self.sigma * np.sqrt(self.time_scale) * np.ones((self.size,))\n","        return mean, sd\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VtaAViZn91ch"},"source":["# After commentary rehaul"]},{"cell_type":"code","metadata":{"id":"ITjzcPO_XyBC","executionInfo":{"status":"aborted","timestamp":1612963375264,"user_tz":180,"elapsed":12330,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI.\n","\"\"\"\n","\n","import sys # To close the window when the game is over\n","from os import environ, path # To center the game window the best possible\n","import random # Random numbers used for the food\n","import logging # Logging function for movements and errors\n","from itertools import tee # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4,\n","           'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","rewards = {'MOVE': -0.005, 'GAME_OVER': -1, 'SCORED': 1}\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","# Speed levels possible to human players\n","levels = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    BOARD_SIZE: int, optional, default = 30\n","        The size of the board.\n","    BLOCK_SIZE: int, optional, default = 20\n","        The size in pixels of a block.\n","    HEAD_COLOR: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    TAIL_COLOR: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    FOOD_COLOR: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    GAME_SPEED: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    BENCHMARK: int, optional, default = 10\n","        Ammount of matches to BENCHMARK and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, BOARD_SIZE = 30, BLOCK_SIZE = 20,\n","                 HEAD_COLOR = (42, 42, 42), TAIL_COLOR = (152, 152, 152),\n","                 FOOD_COLOR = (200, 0, 0), GAME_SPEED = 10, BENCHMARK = 10):\n","        \"\"\"Initialize all global variables. Can be updated with argument_handler.\n","        \"\"\"\n","        self.BOARD_SIZE = BOARD_SIZE\n","        self.BLOCK_SIZE = BLOCK_SIZE\n","        self.HEAD_COLOR = HEAD_COLOR\n","        self.TAIL_COLOR = TAIL_COLOR\n","        self.FOOD_COLOR = FOOD_COLOR\n","        self.GAME_SPEED = GAME_SPEED\n","        self.BENCHMARK = BENCHMARK\n","\n","        if self.BOARD_SIZE > 50: # Warn the user about performance\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                pass\n","            else:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [BOARD_SIZE / 4, BOARD_SIZE / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if action == actions['IDLE']\\\n","            or (action, self.previous_action) in forbidden_moves:\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","      \n","import numpy as np\n","\n","class SumTree:\n","    def __init__(self, capacity):\n","        self._capacity = capacity\n","        self._tree = np.zeros(2 * self._capacity - 1)\n","        self._data = np.zeros(self._capacity, dtype = object)\n","        self._data_idx = 0\n","\n","    @property\n","    def capacity(self):\n","        return self._capacity\n","\n","    @property\n","    def tree(self):\n","        return self._tree\n","\n","    @property\n","    def data(self):\n","        return self._data\n","\n","    def sum(self):\n","        return self._tree[0]\n","\n","    def insert(self, data, priority):\n","#        print(\"Data shape: {}\".format(data.shape))\n","#        print(\"Stored data shape: {}\".format(self._data.shape))\n","        self._data[self._data_idx] = data\n","        tree_idx = self._data_idx + self._capacity - 1\n","        self.update(tree_idx, priority)\n","        self._data_idx += 1\n","        if self._data_idx >= self._capacity:\n","            self._data_idx = 0\n","\n","    def update(self, tree_idx, priority):\n","        delta = priority - self._tree[tree_idx]\n","        self._tree[tree_idx] = priority\n","        while tree_idx != 0:\n","            tree_idx = (tree_idx - 1) // 2  # Get parent\n","            self._tree[tree_idx] += delta\n","\n","    def retrieve(self, val):\n","        tree_idx, parent = None, 0\n","        while True:\n","            left = 2 * parent + 1\n","            right = left + 1\n","            if left >= len(self._tree):  # Leaf\n","                tree_idx = parent\n","                break\n","            else:\n","                if val <= self._tree[left]:\n","                    parent = left\n","                else:\n","                    val -= self._tree[left]\n","                    parent = right\n","\n","        priority = self._tree[tree_idx]\n","        data = self._data[tree_idx - self._capacity + 1]\n","\n","        return tree_idx, priority, data\n","\n","    def max_leaf(self):\n","        return np.max(self.leaves())\n","\n","    def min_leaf(self):\n","        return np.min(self.leaves())\n","\n","    def leaves(self):\n","        return self._tree[-self._capacity:]      \n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with BOARD_SIZE * BLOCK_SIZE dimension.\"\"\"\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images/snake_logo.png\"))\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE, int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                                  4 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                                  6 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                     8 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                option.draw()\n","\n","                if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                    option.hovered = True\n","\n","                    if option == menu_options[0]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['PLAY']\n","                    elif option == menu_options[1]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['BENCHMARK']\n","                    elif option == menu_options[2]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['LEADERBOARDS']\n","                    elif option == menu_options[3]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['QUIT']\n","                else:\n","                    option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            self.window.blit(img, img_rect.bottomleft)\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                                 4 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                                 12 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                var.GAME_SPEED = self.select_speed()\n","                self.reset_game()\n","                self.start_match()\n","                score = self.single_player()\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                var.GAME_SPEED = self.select_speed()\n","                score = []\n","\n","                for i in range(var.BENCHMARK):\n","                    self.reset_game()\n","                    self.start_match()\n","                    score.append(self.single_player())\n","\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        menu_options = [None] * 5\n","        menu_options[0] = TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                     4 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[1] = TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                     6 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[3] = TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                               10 * self.screen_rect.centery / 10),\n","                                               self.window, (1 / 15), \"menu\")\n","\n","        if isinstance(score, int):\n","            text_score = 'SCORE: ' + str(score)\n","        else:\n","            text_score = 'MEAN SCORE: ' + str(sum(score) / var.BENCHMARK)\n","            menu_options[2] = TextBlock(' ADD TO LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                                  8 * self.screen_rect.centery / 10),\n","                                                                  self.window, (1 / 15), \"menu\")\n","\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","        menu_options[4] = TextBlock(text_score, (self.screen_rect.centerx,\n","                                                 15 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\")\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['PLAY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['MENU']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['ADD_LEADERBOARDS']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    pygame.quit()\n","                                    sys.exit()\n","                    else:\n","                        option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        menu_options = [TextBlock(levels[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\")]\n","        selected = False\n","        speed = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 10\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 20\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 30\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 45\n","                    else:\n","                        option.hovered = False\n","\n","            if speed is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return speed\n","\n","    def single_player(self):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where the snake keeps going each tick. It generate food,\n","        # check collisions and draw.\n","        while not self.game_over:\n","            action = self.handle_input()\n","            self.game_over = self.play(action)\n","            self.draw(color_list)\n","            current_size = self.snake.length # Update the body size\n","\n","            if current_size > previous_size:\n","                color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                           current_size)\n","\n","                previous_size = current_size\n","\n","        score = current_size - 3\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = self.snake.previous_action\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            action = actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            action = actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            action = actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            action = actions['DOWN']\n","\n","        return action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = point_type['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = rewards['MOVE']\n","\n","        if self.game_over:\n","            reward = rewards['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","        pygame.display.update()\n","        self.fps.tick(var.GAME_SPEED)\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","    \n","import sys\n","import time\n","import operator\n","from datetime import timedelta\n","import numpy as np\n","import collections\n","\n","class SegmentTree(object):\n","    def __init__(self, capacity, operation, neutral_element):\n","        \"\"\"Build a Segment Tree data structure.\n","        https://en.wikipedia.org/wiki/Segment_tree\n","        Can be used as regular array, but with two\n","        important differences:\n","            a) setting item's value is slightly slower.\n","               It is O(lg capacity) instead of O(1).\n","            b) user has access to an efficient `reduce`\n","               operation which reduces `operation` over\n","               a contiguous subsequence of items in the\n","               array.\n","        Paramters\n","        ---------\n","        capacity: int\n","            Total size of the array - must be a power of two.\n","        operation: lambda obj, obj -> obj\n","            and operation for combining elements (eg. sum, max)\n","            must for a mathematical group together with the set of\n","            possible values for array elements.\n","        neutral_element: obj\n","            neutral element for the operation above. eg. float('-inf')\n","            for max and 0 for sum.\n","        \"\"\"\n","        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n","        self._capacity = capacity\n","        self._value = [neutral_element for _ in range(2 * capacity)]\n","        self._operation = operation\n","\n","    def _reduce_helper(self, start, end, node, node_start, node_end):\n","        if start == node_start and end == node_end:\n","            return self._value[node]\n","        mid = (node_start + node_end) // 2\n","        if end <= mid:\n","            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n","        else:\n","            if mid + 1 <= start:\n","                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n","            else:\n","                return self._operation(\n","                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n","                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n","                )\n","\n","    def reduce(self, start=0, end=None):\n","        \"\"\"Returns result of applying `self.operation`\n","        to a contiguous subsequence of the array.\n","            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n","        Parameters\n","        ----------\n","        start: int\n","            beginning of the subsequence\n","        end: int\n","            end of the subsequences\n","        Returns\n","        -------\n","        reduced: obj\n","            result of reducing self.operation over the specified range of array elements.\n","        \"\"\"\n","        if end is None:\n","            end = self._capacity\n","        if end < 0:\n","            end += self._capacity\n","        end -= 1\n","        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n","\n","    def __setitem__(self, idx, val):\n","        # index of the leaf\n","        idx += self._capacity\n","        self._value[idx] = val\n","        idx //= 2\n","        while idx >= 1:\n","            self._value[idx] = self._operation(\n","                self._value[2 * idx],\n","                self._value[2 * idx + 1]\n","            )\n","            idx //= 2\n","\n","    def __getitem__(self, idx):\n","        assert 0 <= idx < self._capacity\n","        return self._value[self._capacity + idx]\n","\n","\n","class SumSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(SumSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=operator.add,\n","            neutral_element=0.0\n","        )\n","\n","    def sum(self, start=0, end=None):\n","        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n","        return super(SumSegmentTree, self).reduce(start, end)\n","\n","    def find_prefixsum_idx(self, prefixsum):\n","        \"\"\"Find the highest index `i` in the array such that\n","            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n","        if array values are probabilities, this function\n","        allows to sample indexes according to the discrete\n","        probability efficiently.\n","        Parameters\n","        ----------\n","        perfixsum: float\n","            upperbound on the sum of array prefix\n","        Returns\n","        -------\n","        idx: int\n","            highest index satisfying the prefixsum constraint\n","        \"\"\"\n","        assert 0 <= prefixsum <= self.sum() + 1e-5\n","        idx = 1\n","        while idx < self._capacity:  # while non-leaf\n","            if self._value[2 * idx] > prefixsum:\n","                idx = 2 * idx\n","            else:\n","                prefixsum -= self._value[2 * idx]\n","                idx = 2 * idx + 1\n","        return idx - self._capacity\n","\n","\n","class MinSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(MinSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=min,\n","            neutral_element=float('inf')\n","        )\n","\n","    def min(self, start=0, end=None):\n","        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n","\n","        return super(MinSegmentTree, self).reduce(start, end)\n","\n","      \n","import numpy as np\n","\n","from random import sample, uniform\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes\n","    ----------\n","    memory: list of experiences\n","        Memory list to insert experiences.\n","    memory_size: int, optional, default = 150000\n","        The ammount of experiences to be stored in the memory.\n","    input_shape: tuple of 3 * int\n","        The shape of the input which will be stored.\n","    \"\"\"\n","    def __init__(self, memory_size = 150000):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        self.memory.append(experience)\n","\n","        if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","            self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\n","\n","        Return\n","        ----------\n","        batch: np.array of batch_size experiences\n","            The batched experiences from memory.\n","        IS_weights: np.array of batch_size of the weights\n","            As it's used only in PER, is an array of ones in this case.\n","        Indexes: list of batch_size * int\n","            As it's used only in PER, return None.\n","        \"\"\"\n","        IS_weights = np.ones((batch_size, ))\n","        batch = np.array(sample(self.memory, batch_size))\n","\n","        return batch, IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","            \n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        targets[range(batch_size), a.astype(int)] = r + gamma * (1 - game_over) * Qsa\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        self.memory = []\n","\n","\n","class PrioritizedExperienceReplayNaive:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 100, alpha = 0.6, epsilon = 0.001,\n","                 beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return self.exp\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        max_priority = self.memory.max_leaf()\n","\n","        if max_priority == 0:\n","            max_priority = self.get_priority(0)\n","\n","        self.memory.insert(experience, max_priority)\n","        self.exp += 1\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        batch = [None] * batch_size\n","        IS_weights = np.zeros((batch_size, ))\n","        tree_indices = [0] * batch_size\n","\n","        memory_sum = self.memory.sum()\n","        len_seg = memory_sum / batch_size\n","        min_prob = self.memory.min_leaf() / memory_sum\n","\n","        for i in range(batch_size):\n","            val = uniform(len_seg * i, len_seg * (i + 1))\n","            tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","            prob = priority / self.memory.sum()\n","            IS_weights[i] = np.power(prob / min_prob, -self.beta)\n","\n","        return np.array(batch), IS_weights, tree_indices\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        r = r.repeat(nb_actions).reshape((batch_size, nb_actions))\n","        game_over = game_over.repeat(nb_actions)\\\n","                             .reshape((batch_size, nb_actions))\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","            for i in range(batch_size):\n","                Qsa[i] = Y_target[i][actions[i]]\n","            Qsa = np.array(Qsa).repeat(nb_actions).reshape((batch_size, nb_actions))\n","\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1).repeat(nb_actions)\\\n","                                                .reshape((batch_size, nb_actions))\n","\n","        # The targets here already take into account\n","        targets = Y[:batch_size]\n","        targets[range(batch_size, a)] = r + gamma * (1 - game_over) * Qsa\n","\n","        errors = np.abs((targets - Y[:batch_size]).max(axis = 1)).clip(max = 1.)\n","        self.update(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 0:\n","            self.memory_size = 150000\n","\n","        self.memory = SumTree(self.memory_size)\n","        self.exp = 0\n","\n","\n","class PrioritizedExperienceReplay:\n","    def __init__(self, memory_size, nb_epoch = 10000, epsilon = 0.001,\n","                 alpha = 0.6, beta = 0.4, decay = 0.5):\n","        self.memory_size = memory_size\n","        self._alpha = alpha\n","        self.memory = []\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.pos = 0\n","\n","        it_capacity = 1\n","\n","        while it_capacity < memory_size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n","        self._max_priority = 1.0\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","        if len(self.memory) < self.memory_size:\n","            self.memory.append(experience)\n","            self.pos += 1\n","        else:\n","            self.memory[self.pos] = experience\n","            self.pos = (self.pos + 1) % self.memory_size\n","\n","        self._it_sum[self.pos] = self._max_priority ** self._alpha\n","        self._it_min[self.pos] = self._max_priority ** self._alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = []\n","        for _ in range(batch_size):\n","            mass = random.random() * self._it_sum.sum(0, len(self) - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","        return res\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def get_samples(self, batch_size):\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = []\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * len(self)) ** (-self.beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * len(self)) ** (-self.beta)\n","            weights.append(weight / max_weight)\n","        weights = np.array(weights, dtype=np.float32)\n","        samples = [self.memory[idx] for idx in idxes]\n","\n","        return samples, weights, idxes\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        r = r.repeat(nb_actions).reshape((batch_size, nb_actions))\n","        game_over = game_over.repeat(nb_actions)\\\n","                             .reshape((batch_size, nb_actions))\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","            for i in range(batch_size):\n","                Qsa[i] = Y_target[i][actions]\n","            Qsa = np.array(Qsa).repeat(nb_actions).reshape((batch_size, nb_actions))\n","\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1).repeat(nb_actions)\\\n","                                                .reshape((batch_size, nb_actions))\n","\n","        # The targets here already take into account\n","        targets = Y[:batch_size]\n","        targets[range(batch_size, a)] = r + gamma * (1 - game_over) * Qsa\n","\n","        errors = np.abs((targets - Y[:batch_size]).max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def update_priorities(self, idxes, errors):\n","        priorities = self.get_priority(errors)\n","\n","        for idx, priority in zip(idxes, priorities):\n","            self._it_sum[idx] = priority ** self._alpha\n","            self._it_min[idx] = priority ** self._alpha\n","\n","            self._max_priority = max(self._max_priority, priority)\n","\n","            \n","\n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","      \n","#!/usr/bin/env python\n","\n","\"\"\"clipped_error: L1 for errors < clip_value else L2 error.\n","\n","Functions:\n","    huber_loss: Return L1 error if absolute error is less than clip_value, else\n","                return L2 error.\n","    clipped_error: Call huber_loss with default clip_value to 1.0.\n","\"\"\"\n","\n","import numpy as np\n","from keras import backend as K\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\tif K.backend() == 'tensorflow':\n","\t\tif hasattr(tf, 'select'):\n","\t\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\t\telse:\n","\t\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\telif K.backend() == 'theano':\n","\t\tfrom theano import tensor as T\n","\t\treturn T.switch(condition, squared_loss, linear_loss)\n","\telse:\n","\t\traise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","import numpy as np\n","from keras.models import Sequential, load_model, Model\n","from keras.layers import *\n","import tensorflow as tf\n","\n","def CNN4(optimizer, loss, stack, input_size, output_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    model = Sequential()\n","    model.add(Conv2D(32, (4, 4), activation = 'relu', input_shape = (stack,\n","                                                                    input_size,\n","                                                                    input_size)))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Flatten())\n","    model.add(Dense(3136, activation = 'relu'))\n","    model.add(Dense(output_size))\n","    model.compile(optimizer = optimizer, loss = loss)\n","\n","    return model\n","  \n","  \n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms:\n","    * Simple DQN (with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double DQN;\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling DQN;\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * DQN + PER;\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns.\n","        Paper: https://arxiv.org/pdf/1703.01327\n","\n","Arguments:\n","    --load FILE.h5: load a previously trained model in '.h5' format.\n","    --board_size INT: assign the size of the board, default = 10\n","    --nb_frames INT: assign the number of frames per stack, default = 4.\n","    --nb_actions INT: assign the number of actions possible, default = 5.\n","    --update_freq INT: assign how often, in epochs, to update the target,\n","      default = 500.\n","    --visual: select wheter or not to draw the game in pygame.\n","    --double: use a target network with double DQN logic.\n","    --dueling: use dueling network logic, Q(s,a) = A + V.\n","    --per: use Prioritized Experience Replay (based on Sum Trees).\n","    --local_state: Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from os import path, environ, sys\n","import random\n","\n","import inspect # Making relative imports from parallel folders possible\n","currentdir = path.dirname(path.abspath(inspect.getfile(inspect.currentframe())))\n","parentdir = path.dirname(currentdir)\n","sys.path.insert(0, parentdir)\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import load_model\n","from keras import backend as K\n","K.set_image_dim_ordering('th')\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes:\n","    memory: memory used in the model. Input memory or ExperienceReplay.\n","    model: the input model, Conv2D in Keras.\n","    target: the target model, used to calculade the fixed Q-targets.\n","    nb_frames: ammount of frames for each sars.\n","    frames: the frames in each sars.\n","    per: flag for PER usage.\n","    \"\"\"\n","    def __init__(self, model, target, memory = None, memory_size = 150000,\n","                 nb_frames = 4, board_size = 10, per = True):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if memory:\n","            self.memory = memory\n","        else:\n","            if not per:\n","                self.memory = ExperienceReplay(memory_size = memory_size)\n","            else:\n","                self.memory = PrioritizedExperienceReplay(memory_size = memory_size)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.frames = None\n","        self.target_updates = 0\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\"\"\"\n","        frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        return np.expand_dims(self.frames, 0)\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target_updates += 1\n","        self.target.set_weights(self.model.get_weights())\n","        \n","    def transfer_weights(self, update_target_frequency):\n","        \"\"\" Transfer Weights from Model to Target at rate Tau\n","        \"\"\"\n","        W = self.model.get_weights()\n","        tgt_W = self.target.get_weights()\n","        for i in range(len(W)):\n","            tgt_W[i] = update_target_frequency * W[i] + (1 - update_target_frequency) * tgt_W[i]\n","        self.target.set_weights(tgt_W)\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, history_loss,\n","                      history_step, history_reward, policy, value, win_count,\n","                      verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    sum(history_size[-10:]) / 10,\n","                                    max(history_size[-10:]),\n","                                    sum(history_step[-10:]) / 10,\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}' # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            # Print training performance\n","            text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                          + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                          + 'Mean loss - 100 episodes: {:.4f}')\n","            print(text_perf.format(history_loss[-1],\n","                                   history_loss[-1] / history_step[-1],\n","                                   sum(history_loss[-100:]) / 100))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   sum(history_step[-100:]) / 100))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\"\"\"\n","        loss = 0.\n","\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, update_target_freq = 0.001, optim_rounds = 1,\n","              policy = \"EpsGreedyQPolicy\", verbose = 1, n_steps = None):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","\n","        history_size = []\n","        history_step = []\n","        history_loss = []\n","        history_reward = []\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        if policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                    text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                                  + '| Loss: {:.4f}')\n","                    print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    if n_steps is not None:\n","                        n_step_buffer = []\n","                    game.reset_game()\n","                    self.clear_frames()\n","\n","                    S = self.get_game_data(game)\n","\n","                    while not game.game_over:\n","                        game.food_pos = game.generate_food()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","\n","                        game.play(action)\n","\n","                        r = game.get_reward()\n","                        total_reward += r\n","                        if n_steps is not None:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(S, action, R, S_prime, game.game_over) # Add to the memory\n","                        S = S_prime # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe: # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1 # Counter for metric purposes\n","\n","                    if self.per: # Advance beta, used in PER\n","                        self.memory.beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None: # Update the target model\n","                        if update_target_freq >= 1:\n","                            if epoch % update_target_freq == 0: \n","                                self.update_target_model()\n","                        elif update_target_freq < 1.:\n","                            self.transfer_weights(update_target_freq)\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch, nb_epoch, history_size,\n","                                           history_loss, history_step,\n","                                           history_reward, policy, value,\n","                                           win_count, verbose)\n","\n","    def play(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","        result_size = []\n","        result_step = []\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","            S = self.get_game_data(game)\n","\n","            if visual:\n","                game.create_window()\n","                # The main loop, it pump key_presses and update every tick.\n","                environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","                previous_size = game.snake.length # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","\n","            while not game.game_over:\n","                action, value = q_policy.select_action(self.model, S, epoch, nb_actions)\n","                game.play(action)\n","                current_size = game.snake.length # Update the body size\n","\n","                if visual:\n","                    game.draw(color_list)\n","\n","                    if current_size > previous_size:\n","                        color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   game.snake.length)\n","\n","                        previous_size = current_size\n","\n","                S = self.get_game_data(game)\n","\n","                if game.game_over:\n","                    result_size.append(current_size)\n","                    result_step.append(game.step)\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(result_size), np.max(result_size),\n","                      np.min(result_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(result_step), np.max(result_step),\\\n","                      np.min(result_step)))\n","\n","    \n","        \n","        \n","board_size = 10\n","nb_frames = 4\n","  \n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","model = CNN4(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = -1,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","agent.train(game, batch_size = 64, nb_epoch = 10000, gamma = 0.95, update_target_freq = 0.001, policy = \"EpsGreedyQPolicy\")        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xmcWlQ1An7T8","executionInfo":{"status":"aborted","timestamp":1612963375266,"user_tz":180,"elapsed":12328,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["model.save('keras.h5')\n","\n","!zip -r model-epsgreedy-after-all.zip keras.h5 \n","from google.colab import files\n","files.download('model-epsgreedy-after-all.zip')\n","model = load_model('keras.h5', custom_objects={'clipped_error': clipped_error})\n","\n","board_size = 10\n","nb_frames = 4\n","nb_actions = 5\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = 1500000,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","agent.play(game, visual = False, nb_epoch = 10000) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rZ1iBM-WcuQG"},"source":["# PCL"]},{"cell_type":"code","metadata":{"id":"bfZwyWy_b7DM","executionInfo":{"status":"aborted","timestamp":1612963375267,"user_tz":180,"elapsed":12325,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI.\n","\"\"\"\n","\n","import sys # To close the window when the game is over\n","from os import environ, path # To center the game window the best possible\n","import random # Random numbers used for the food\n","import logging # Logging function for movements and errors\n","from itertools import tee # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4,\n","           'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","rewards = {'MOVE': -0.005, 'GAME_OVER': -1, 'SCORED': 1}\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","# Speed levels possible to human players\n","levels = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    BOARD_SIZE: int, optional, default = 30\n","        The size of the board.\n","    BLOCK_SIZE: int, optional, default = 20\n","        The size in pixels of a block.\n","    HEAD_COLOR: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    TAIL_COLOR: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    FOOD_COLOR: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    GAME_SPEED: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    BENCHMARK: int, optional, default = 10\n","        Ammount of matches to BENCHMARK and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, BOARD_SIZE = 30, BLOCK_SIZE = 20,\n","                 HEAD_COLOR = (42, 42, 42), TAIL_COLOR = (152, 152, 152),\n","                 FOOD_COLOR = (200, 0, 0), GAME_SPEED = 10, BENCHMARK = 10):\n","        \"\"\"Initialize all global variables. Can be updated with argument_handler.\n","        \"\"\"\n","        self.BOARD_SIZE = BOARD_SIZE\n","        self.BLOCK_SIZE = BLOCK_SIZE\n","        self.HEAD_COLOR = HEAD_COLOR\n","        self.TAIL_COLOR = TAIL_COLOR\n","        self.FOOD_COLOR = FOOD_COLOR\n","        self.GAME_SPEED = GAME_SPEED\n","        self.BENCHMARK = BENCHMARK\n","\n","        if self.BOARD_SIZE > 50: # Warn the user about performance\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                pass\n","            else:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [BOARD_SIZE / 4, BOARD_SIZE / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if action == actions['IDLE']\\\n","            or (action, self.previous_action) in forbidden_moves:\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","      \n","import numpy as np\n","\n","class SumTree:\n","    def __init__(self, capacity):\n","        self._capacity = capacity\n","        self._tree = np.zeros(2 * self._capacity - 1)\n","        self._data = np.zeros(self._capacity, dtype = object)\n","        self._data_idx = 0\n","\n","    @property\n","    def capacity(self):\n","        return self._capacity\n","\n","    @property\n","    def tree(self):\n","        return self._tree\n","\n","    @property\n","    def data(self):\n","        return self._data\n","\n","    def sum(self):\n","        return self._tree[0]\n","\n","    def insert(self, data, priority):\n","#        print(\"Data shape: {}\".format(data.shape))\n","#        print(\"Stored data shape: {}\".format(self._data.shape))\n","        self._data[self._data_idx] = data\n","        tree_idx = self._data_idx + self._capacity - 1\n","        self.update(tree_idx, priority)\n","        self._data_idx += 1\n","        if self._data_idx >= self._capacity:\n","            self._data_idx = 0\n","\n","    def update(self, tree_idx, priority):\n","        delta = priority - self._tree[tree_idx]\n","        self._tree[tree_idx] = priority\n","        while tree_idx != 0:\n","            tree_idx = (tree_idx - 1) // 2  # Get parent\n","            self._tree[tree_idx] += delta\n","\n","    def retrieve(self, val):\n","        tree_idx, parent = None, 0\n","        while True:\n","            left = 2 * parent + 1\n","            right = left + 1\n","            if left >= len(self._tree):  # Leaf\n","                tree_idx = parent\n","                break\n","            else:\n","                if val <= self._tree[left]:\n","                    parent = left\n","                else:\n","                    val -= self._tree[left]\n","                    parent = right\n","\n","        priority = self._tree[tree_idx]\n","        data = self._data[tree_idx - self._capacity + 1]\n","\n","        return tree_idx, priority, data\n","\n","    def max_leaf(self):\n","        return np.max(self.leaves())\n","\n","    def min_leaf(self):\n","        return np.min(self.leaves())\n","\n","    def leaves(self):\n","        return self._tree[-self._capacity:]      \n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with BOARD_SIZE * BLOCK_SIZE dimension.\"\"\"\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images/snake_logo.png\"))\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE, int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                                  4 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                                  6 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                     8 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                option.draw()\n","\n","                if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                    option.hovered = True\n","\n","                    if option == menu_options[0]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['PLAY']\n","                    elif option == menu_options[1]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['BENCHMARK']\n","                    elif option == menu_options[2]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['LEADERBOARDS']\n","                    elif option == menu_options[3]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['QUIT']\n","                else:\n","                    option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            self.window.blit(img, img_rect.bottomleft)\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                                 4 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                                 12 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                var.GAME_SPEED = self.select_speed()\n","                self.reset_game()\n","                self.start_match()\n","                score = self.single_player()\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                var.GAME_SPEED = self.select_speed()\n","                score = []\n","\n","                for i in range(var.BENCHMARK):\n","                    self.reset_game()\n","                    self.start_match()\n","                    score.append(self.single_player())\n","\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        menu_options = [None] * 5\n","        menu_options[0] = TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                     4 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[1] = TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                     6 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[3] = TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                               10 * self.screen_rect.centery / 10),\n","                                               self.window, (1 / 15), \"menu\")\n","\n","        if isinstance(score, int):\n","            text_score = 'SCORE: ' + str(score)\n","        else:\n","            text_score = 'MEAN SCORE: ' + str(sum(score) / var.BENCHMARK)\n","            menu_options[2] = TextBlock(' ADD TO LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                                  8 * self.screen_rect.centery / 10),\n","                                                                  self.window, (1 / 15), \"menu\")\n","\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","        menu_options[4] = TextBlock(text_score, (self.screen_rect.centerx,\n","                                                 15 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\")\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['PLAY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['MENU']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['ADD_LEADERBOARDS']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    pygame.quit()\n","                                    sys.exit()\n","                    else:\n","                        option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        menu_options = [TextBlock(levels[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\")]\n","        selected = False\n","        speed = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 10\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 20\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 30\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 45\n","                    else:\n","                        option.hovered = False\n","\n","            if speed is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return speed\n","\n","    def single_player(self):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where the snake keeps going each tick. It generate food,\n","        # check collisions and draw.\n","        while not self.game_over:\n","            action = self.handle_input()\n","            self.game_over = self.play(action)\n","            self.draw(color_list)\n","            current_size = self.snake.length # Update the body size\n","\n","            if current_size > previous_size:\n","                color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                           current_size)\n","\n","                previous_size = current_size\n","\n","        score = current_size - 3\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = self.snake.previous_action\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            action = actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            action = actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            action = actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            action = actions['DOWN']\n","\n","        return action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = point_type['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = rewards['MOVE']\n","\n","        if self.game_over:\n","            reward = rewards['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","        pygame.display.update()\n","        self.fps.tick(var.GAME_SPEED)\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","      \n","      \n","\n","from keras.layers import Dense, Conv2D, Flatten\n","from keras.models import Sequential, Model\n","import tensorflow as tf\n","from keras import backend as K\n","from keras.layers import Input\n","import numpy as np\n","import random\n","\n","class Net(object):\n","    def __init__(self, nb_frames, board_size):\n","        model = Sequential()\n","        model.add(Dense(64, activation='relu'))\n","        model.add(Dense(64, activation='relu'))\n","        model.add(Dense(64, activation='relu'))\n","        model.add(Dense(64, activation='relu'))  \n","        self.pi_model = Sequential([model])\n","        self.pi_model.add(Dense(64, activation='relu'))\n","        self.pi_model.add(Dense(5, activation='softmax'))\n","        self.v_model = Sequential([model])\n","        self.v_model.add(Dense(64, activation='relu'))\n","        self.v_model.add(Dense(1))\n","\n","\n","class PCL(object):\n","    def __init__(self, epoch, env, replay_buffer, sess=None, net=None,\n","            pi_optimizer=None, v_optimizer=None, off_policy_rate=20,\n","            pi_lr=7e-4, v_rate=0.5, entropy_tau=0.5, rollout_d=20, gamma=1):\n","        self.epoch = epoch\n","        self.env = env\n","        self.replay_buffer = replay_buffer\n","        self.sess = sess\n","        self.net = net\n","        if pi_optimizer is None:\n","            self.pi_optimizer = tf.train.AdamOptimizer(pi_lr)\n","        else:\n","            self.pi_optimizer = pi_optimizer\n","        if v_optimizer is None:\n","            self.v_optimizer = tf.train.AdamOptimizer(pi_lr*v_rate)\n","        else:\n","            self.v_optimizer = v_optimizer\n","        self.off_policy_rate = off_policy_rate\n","        self.entropy_tau = entropy_tau\n","        self.rollout_d = rollout_d\n","        self.gamma = gamma\n","        self.state_shape = 100\n","        self.action_shape = [5]\n","        self.built = False\n","        self.schedule = LinearSchedule(epoch/2, 0.01, 1.00)\n","        \n","    def build(self):\n","        pi_model = self.net.pi_model\n","        v_model = self.net.v_model\n","        self.state = tf.placeholder(tf.float32, shape=[None, None, 100], name='state')\n","        self.R = tf.placeholder(tf.float32, shape=[None, None], name='R')\n","        self.action = tf.placeholder(tf.float32, shape=[None, None, 5], name='action')\n","        self.discount = tf.placeholder(tf.float32, shape=[None], name='discount')\n","\n","        v_s_t = v_model(self.state[:, 0, :])\n","        v_s_t_d = v_model(self.state[:, -1, :])\n","        self.pi = pi_model(self.state)\n","        C = K.sum(-v_s_t + self.gamma ** self.rollout_d * v_s_t_d + \\\n","                K.sum(self.R, axis=1) - self.entropy_tau * K.sum(self.discount * \\\n","                K.sum(K.log(self.pi+K.epsilon()) * self.action, axis=2), axis=1), axis=0)\n","        self.loss = C ** 2\n","\n","        self.updater = [self.pi_optimizer.minimize(self.loss, var_list=pi_model.trainable_weights),\n","                self.v_optimizer.minimize(self.loss, var_list=v_model.trainable_weights)]\n","        self.sess.run(tf.global_variables_initializer())\n","        self.built = True\n","\n","    def optimize(self, episode):\n","        if not self.built:\n","            self.build()\n","        if len(episode['states']) < self.rollout_d:\n","            rollout_d = len(episode['states'])\n","        else:\n","            rollout_d = self.rollout_d\n","        discount = np.array([self.gamma**i for i in range(rollout_d)], dtype=np.float32)\n","        state = []\n","        action = []\n","        R = []\n","        for i in range(len(episode['states'])-rollout_d+1):\n","            state.append(episode['states'][i:i+rollout_d])\n","            a = episode['actions'][i:i+rollout_d]\n","            action.append(np.eye(*self.action_shape, dtype=np.int32)[a])\n","            R.append(episode['rewards'][i:i+rollout_d])\n","        feed_in = {self.state: state, self.action: action, self.R: R, self.discount: discount}\n","        self.sess.run(self.updater, feed_in)\n","\n","    def rollout(self, epoch, train = False):\n","        states = []\n","        actions = []\n","        rewards = []\n","\n","        self.env.reset_game()\n","        s = self.env.state().flatten()\n","\n","        while not self.env.game_over:\n","            game.food_pos = game.generate_food()\n","            if train:\n","                a = self.get_best_action(s)\n","            else:\n","                a = self.get_action(s, epoch)\n","\n","            self.env.play(a)\n","\n","            r = self.env.get_reward()\n","            next_s = self.env.state().flatten()\n","\n","            states.append(s)\n","            rewards.append(r)\n","            actions.append(a)\n","            s = next_s\n","\n","        size = self.env.snake.length\n","        step = self.env.step\n","\n","        return dict(\n","            states=np.array(states),\n","            actions=np.array(actions),\n","            rewards=np.array(rewards)\n","        ), size, step\n","\n","    def get_action(self, state, epoch):\n","        if not self.built:\n","            self.build()\n","            \n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","        pi = self.sess.run(self.pi, {self.state: [[state]]})[0][0]\n","\n","        if rand < self.eps:\n","            a = np.random.choice(np.arange(self.action_shape[0]), p=pi)\n","        else:\n","             a = np.argmax(pi)\n","        return a\n","\n","    def get_best_action(self, state):\n","        if not self.built:\n","            self.build()\n","        pi = self.sess.run(self.pi, {self.state: [[state]]})[0][0]\n","        a = np.argmax(pi)\n","        return a\n","\n","    def train(self):\n","        size = []\n","        step = []\n","        win_count = 0\n","        \n","        for i in range(self.epoch):\n","            episode, size_ep, step_ep = self.rollout(epoch = i)\n","            self.optimize(episode)\n","\n","            size.append(size_ep)\n","            step.append(step_ep)\n","            if (i + 1) % 10 == 0:\n","                print(\"Epoch: {:03d}/{:03d} | \".format(i + 1, self.epoch), end = \"\")\n","                print(\"Size 10: {:.1f} | Step 10: {:.1f} | Memory: {:d}/{:d}\".format(np.mean(size[-10:]), np.mean(step[-10:]), len(self.replay_buffer.buffer), self.replay_buffer.max_len))\n","            self.replay_buffer.add(episode)\n","            if self.replay_buffer.trainable:\n","                for _ in range(self.off_policy_rate):\n","                    episode = self.replay_buffer.sample()\n","                    self.optimize(episode)\n","\n","    def test(self):\n","        for i in range(self.epoch):\n","            episode, size, step = self.rollout(train = True, epoch = i)\n","            print(\"Test epoch: {:03d}/{:03d} | \".format(i, self.epoch), end = \"\")\n","            r = episode['rewards']\n","            print(\"Reward: {:.2f} | Size: {:03d} | Step: {:03d}\".format(r.sum(), size, step))\n","\n","class ReplayBuffer(object):\n","    def __init__(self, max_len=10000, alpha=1):\n","        self.max_len = max_len\n","        self.alpha = alpha\n","        self.buffer = []\n","        # weight is not normalized\n","        self.weight = np.array([])\n","\n","    def add(self, episode):\n","        self.buffer.append(episode)\n","        self.weight = np.append(self.weight, np.exp(self.alpha*episode['rewards'].sum()))\n","        if len(self.buffer) > self.max_len:\n","            delete_ind = np.random.randint(len(self.buffer))\n","            del self.buffer[delete_ind]\n","            self.weight = np.delete(self.weight, delete_ind)\n","\n","    def sample(self):\n","        return np.random.choice(self.buffer, p=self.weight/self.weight.sum())\n","\n","    @property\n","    def trainable(self):\n","        if len(self.buffer) > 32:\n","            return True\n","        else:\n","            return False\n","\n","board_size = 10\n","nb_frames = 4\n","\n","net = Net(1, board_size)\n","\n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","replay_buffer = ReplayBuffer()\n","\n","sess = tf.Session()\n","\n","agent = PCL(10000, game, replay_buffer, sess, net)\n","agent.train()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hcNNlCUoXxdI","executionInfo":{"status":"aborted","timestamp":1612963375269,"user_tz":180,"elapsed":12323,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["agent.test()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dw6BCueOcU4b"},"source":["# NOISY NET"]},{"cell_type":"code","metadata":{"id":"YDpwKRnvcUYH","executionInfo":{"status":"aborted","timestamp":1612963375270,"user_tz":180,"elapsed":12320,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI.\n","\"\"\"\n","\n","import sys  # To close the window when the game is over\n","from array import array  # Efficient numeric arrays\n","from os import environ, path  # To center the game window the best possible\n","import random  # Random numbers used for the food\n","import logging  # Logging function for movements and errors\n","from itertools import tee  # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4,\n","           'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","rewards = {'MOVE': -0.005, 'GAME_OVER': -1, 'SCORED': 1}\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","# Speed levels possible to human players, MEGA HARDCORE starts with MEDIUM and\n","# increases with snake size\n","levels = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","speeds = {'EASY': 80, 'MEDIUM': 60, 'HARD': 40, 'MEGA_HARDCORE': 65}\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    BOARD_SIZE: int, optional, default = 30\n","        The size of the board.\n","    BLOCK_SIZE: int, optional, default = 20\n","        The size in pixels of a block.\n","    HEAD_COLOR: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    TAIL_COLOR: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    FOOD_COLOR: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    GAME_SPEED: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    BENCHMARK: int, optional, default = 10\n","        Ammount of matches to BENCHMARK and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, BOARD_SIZE = 30, BLOCK_SIZE = 20,\n","                 HEAD_COLOR = (42, 42, 42), TAIL_COLOR = (152, 152, 152),\n","                 FOOD_COLOR = (200, 0, 0), GAME_SPEED = 80, GAME_FPS = 100,\n","                 BENCHMARK = 10):\n","        \"\"\"Initialize all global variables. Can be updated with argument_handler.\n","        \"\"\"\n","        self.BOARD_SIZE = BOARD_SIZE\n","        self.BLOCK_SIZE = BLOCK_SIZE\n","        self.HEAD_COLOR = HEAD_COLOR\n","        self.TAIL_COLOR = TAIL_COLOR\n","        self.FOOD_COLOR = FOOD_COLOR\n","        self.GAME_SPEED = GAME_SPEED\n","        self.GAME_FPS = GAME_FPS\n","        self.BENCHMARK = BENCHMARK\n","\n","        if self.BOARD_SIZE > 50: # Warn the user about performance\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                pass\n","            else:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [BOARD_SIZE / 4, BOARD_SIZE / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def is_movement_invalid(self, action):\n","        valid = False\n","\n","        if (action, self.previous_action) in forbidden_moves:\n","            valid = True\n","\n","        return valid\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if action == actions['IDLE'] or self.is_movement_invalid(action):\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with BOARD_SIZE * BLOCK_SIZE dimension.\"\"\"\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def cycle_menu(self, menu_options, list_menu, dict, img = None,\n","                   img_rect = None):\n","        \"\"\"\"\"\"\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            events = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for i, option in enumerate(menu_options):\n","                if option is not None:\n","                    option.draw()\n","                    option.hovered = False\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        for event in events:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = dict[list_menu[i]]\n","\n","            if selected_option is not None:\n","                selected = True\n","            if img is not None:\n","                self.window.blit(img, img_rect.bottomleft)\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def cycle_matches(self, n_matches = 10, mega_hardcore = False):\n","        \"\"\"\"\"\"\n","        self.reset_game()\n","        score = array('i')\n","\n","        for match in range(n_matches):\n","            score.append(self.single_player(mega_hardcore))\n","\n","        return score\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images\" +\n","                                              \"/snake_logo.png\")).convert()\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE,\n","                                     int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","        list_menu = ['PLAY', 'BENCHMARK', 'LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                             4 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                             6 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                             8 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","        selected_option = self.cycle_menu(menu_options, list_menu, options,\n","                                          img, img_rect)\n","\n","        return selected_option\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                            4 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                            12 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                var.GAME_SPEED, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = 1,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                var.GAME_SPEED, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = var.BENCHMARK,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        score_option = None\n","\n","        if len(score) == var.BENCHMARK:\n","            score_option = TextBlock(' ADD TO LEADERBOARDS ',\n","                                        (self.screen_rect.centerx,\n","                                         8 * self.screen_rect.centery / 10),\n","                                        self.window, (1 / 15), \"menu\")\n","\n","        text_score = 'SCORE: ' + str(np.mean(score))\n","        list_menu = ['PLAY', 'MENU', 'ADD_LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                            4 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 15), \"menu\"),\n","                           TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                            6 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 15), \"menu\"),\n","                           score_option,\n","                           TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                            10 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 15), \"menu\"),\n","                           TextBlock(text_score, (self.screen_rect.centerx,\n","                                             15 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 10), \"text\")]\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","        selected_option = self.cycle_menu(menu_options, list_menu, options)\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        list_menu = ['EASY', 'MEDIUM', 'HARD', 'MEGA_HARDCORE']\n","        menu_options = [TextBlock(levels[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\")]\n","\n","        speed = self.cycle_menu(menu_options, list_menu, speeds)\n","        mega_hardcore = False\n","\n","        if speed == speeds['MEGA_HARDCORE']:\n","            mega_hardcore = True\n","\n","        return speed, mega_hardcore\n","\n","    def single_player(self, mega_hardcore = False):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where snakes moves after elapsed time is bigger than the\n","        # move_wait time. The last_key pressed is recorded to make the game more\n","        # smooth for human players.\n","        elapsed = 0\n","        last_key = self.snake.previous_action\n","        move_wait = var.GAME_SPEED\n","\n","        while not self.game_over:\n","            elapsed += self.fps.get_time()  # Get elapsed time since last call.\n","\n","            if mega_hardcore:  # Progressive speed increments, the hardest.\n","                move_wait = var.GAME_SPEED - (2 * (self.snake.length - 3))\n","\n","            key_input = self.handle_input()  # Receive inputs with tick.\n","            invalid_key = self.snake.is_movement_invalid(key_input)\n","\n","            if key_input is not None and not invalid_key:\n","                last_key = key_input\n","\n","            if elapsed >= move_wait:  # Move and redraw\n","                elapsed = 0\n","                self.game_over = self.play(last_key)\n","                current_size = self.snake.length  # Update the body size\n","\n","                if current_size > previous_size:\n","                    color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   current_size)\n","\n","                    previous_size = current_size\n","\n","                self.draw(color_list)\n","\n","            pygame.display.update()\n","            self.fps.tick(100)  # Limit FPS to 100\n","\n","        score = current_size - 3  # After the game is over, record score\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = None\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            action = actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            action = actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            action = actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            action = actions['DOWN']\n","\n","        return action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = point_type['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = rewards['MOVE']\n","\n","        if self.game_over:\n","            reward = rewards['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","\n","        if self.player == \"ROBOT\":\n","            pygame.display.update()\n","            self.fps.tick(30)  # Limit FPS to 60\n","\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","#!/usr/bin/env python\n","import numpy as np\n","from argparse import ArgumentParser\n","\n","from keras import backend as K\n","import keras.optimizers as optimizers\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","class HandleArguments:\n","        \"\"\"Handle arguments provided in the command line when executing the model.\n","\n","        Attributes:\n","            args: arguments parsed in the command line.\n","            status_load: a flag for usage of --load argument.\n","            status_visual: a flag for usage of --visual argument.\n","\n","            NEED UPDATE!\n","        \"\"\"\n","        def __init__(self):\n","            self.parser = ArgumentParser() # Receive arguments\n","            self.parser.add_argument(\"-l\", \"--load\", help = \"load a previously trained model. the argument is the filename\", required = False, default = \"\")\n","            self.parser.add_argument(\"-v\", \"--visual\", help = \"define board size\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-du\", \"--dueling\", help = \"use dueling DQN\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-do\", \"--double\", help = \"use double DQN\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-p\", \"--per\", help = \"use Prioritized Experience Replay\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-ls\", \"--local_state\", help = \"define board size\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-g\", \"--board_size\", help = \"define board size\", required = False, default = 10, type = int)\n","            self.parser.add_argument(\"-nf\", \"--nb_frames\", help = \"define board size\", required = False, default = 4, type = int)\n","            self.parser.add_argument(\"-na\", \"--nb_actions\", help = \"define board size\", required = False, default = 5, type = int)\n","            self.parser.add_argument(\"-uf\", \"--update_freq\", help = \"frequency to update target\", required = False, default = 500, type = int)\n","\n","            self.args = self.parser.parse_args()\n","            self.status_load = False\n","            self.status_visual = False\n","            self.local_state = False\n","            self.dueling = False\n","            self.double = False\n","            self.per = False\n","\n","            if self.args.load:\n","                script_dir = path.dirname(__file__) # Absolute dir the script is in\n","                abs_file_path = path.join(script_dir, self.args.load)\n","                model = load_model(abs_file_path)\n","\n","                self.status_load = True\n","\n","            if self.args.visual:\n","                self.status_visual = True\n","\n","            if self.args.local_state:\n","                self.local_state = True\n","\n","            if self.args.dueling:\n","                self.dueling = True\n","\n","            if self.args.double:\n","                self.double = True\n","\n","            if self.args.per:\n","                self.per = True\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\tif K.backend() == 'tensorflow':\n","\t\tif hasattr(tf, 'select'):\n","\t\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\t\telse:\n","\t\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\telif K.backend() == 'theano':\n","\t\tfrom theano import tensor as T\n","\t\treturn T.switch(condition, squared_loss, linear_loss)\n","\telse:\n","\t\traise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","# pylint: disable=C0111\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.framework import tensor_shape\n","from tensorflow.python.layers import base\n","from tensorflow.python.ops.init_ops import Constant\n","\n","class NoisyDense(tf.keras.layers.Dense):\n","\n","    def build(self, input_shape):\n","        input_shape = tensor_shape.TensorShape(input_shape)\n","        if input_shape[-1].value is None:\n","            raise ValueError('The last dimension of the inputs to `Dense` '\n","                             'should be defined. Found `None`.')\n","        self.input_spec = base.InputSpec(min_ndim=2,\n","                                         axes={-1: input_shape[-1].value})\n","        kernel_shape = [input_shape[-1].value, self.units]\n","        kernel_quiet = self.add_variable('kernel_quiet',\n","                                         shape=kernel_shape,\n","                                         initializer=self.kernel_initializer,\n","                                         regularizer=self.kernel_regularizer,\n","                                         constraint=self.kernel_constraint,\n","                                         dtype=self.dtype,\n","                                         trainable=True)\n","        scale_init = Constant(value=(0.5 / np.sqrt(kernel_shape[0])))\n","        kernel_noise_scale = self.add_variable('kernel_noise_scale',\n","                                               shape=kernel_shape,\n","                                               initializer=scale_init,\n","                                               dtype=self.dtype,\n","                                               trainable=True)\n","        kernel_noise = self.make_kernel_noise(shape=kernel_shape)\n","        self.kernel = kernel_quiet + kernel_noise_scale * kernel_noise\n","        if self.use_bias:\n","            bias_shape = [self.units,]\n","            bias_quiet = self.add_variable('bias_quiet',\n","                                           shape=bias_shape,\n","                                           initializer=self.bias_initializer,\n","                                           regularizer=self.bias_regularizer,\n","                                           constraint=self.bias_constraint,\n","                                           dtype=self.dtype,\n","                                           trainable=True)\n","            bias_noise_scale = self.add_variable(name='bias_noise_scale',\n","                                                 shape=bias_shape,\n","                                                 initializer=scale_init,\n","                                                 dtype=self.dtype,\n","                                                 trainable=True)\n","            bias_noise = self.make_bias_noise(shape=bias_shape)\n","            self.bias = bias_quiet + bias_noise_scale * bias_noise\n","        else:\n","            self.bias = None\n","        self.built = True\n","\n","    def make_kernel_noise(self, shape):\n","        raise NotImplementedError\n","\n","    def make_bias_noise(self, shape):\n","        raise NotImplementedError\n","\n","\n","'''\n","Noisy dense layer with independent Gaussian noise\n","'''\n","class NoisyDenseIG(NoisyDense):\n","\n","    def make_kernel_noise(self, shape):\n","        noise = tf.random_normal(shape, dtype=self.dtype)\n","        kernel_noise = tf.Variable(noise, trainable=False, dtype=self.dtype)\n","        self.noise_list = [kernel_noise]\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        noise = tf.random_normal(shape, dtype=self.dtype)\n","        bias_noise = tf.Variable(noise, trainable=False, dtype=self.dtype)\n","        self.noise_list.append(bias_noise)\n","        return bias_noise\n","\n","\n","'''\n","Noisy dense layer with factorized Gaussian noise\n","'''\n","class NoisyDenseFG(NoisyDense):\n","\n","    def make_kernel_noise(self, shape):\n","        kernel_noise_input = self.make_fg_noise(shape=[shape[0]])\n","        kernel_noise_output = self.make_fg_noise(shape=[shape[1]])\n","        self.noise_list = [kernel_noise_input, kernel_noise_output]\n","        kernel_noise = kernel_noise_input[:, tf.newaxis] * kernel_noise_output\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        return self.noise_list[1] # kernel_noise_output\n","\n","    def make_fg_noise(self, shape):\n","        noise = tf.random_normal(shape, dtype=self.dtype)\n","        trans_noise = tf.sign(noise) * tf.sqrt(tf.abs(noise))\n","        return tf.Variable(trans_noise, trainable=False, dtype=self.dtype)\n","\n","      \n","#!/usr/bin/env python\n","\n","\"\"\" Needs update!\n","\"\"\"\n","\n","import numpy as np\n","from keras.models import Sequential, load_model, Model\n","from keras.layers import *\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def weird_CNN(optimizer, loss, stack, input_size, output_size, min_neurons = 16,\n","         max_neurons = 128, kernel_size = (3,3), layers = 4):\n","    # INPUTS\n","    # size     - size of the input images\n","    # n_layers - number of layers\n","    # OUTPUTS\n","    # model    - compiled CNN\n","\n","    # Define hyperparamters\n","    MIN_NEURONS = min_neurons\n","    MAX_NEURONS = max_neurons\n","    KERNEL = kernel_size\n","    n_layers = layers\n","\n","    # Determine the # of neurons in each convolutional layer\n","    steps = np.floor(MAX_NEURONS / (n_layers + 1))\n","    neurons = np.arange(MIN_NEURONS, MAX_NEURONS, steps)\n","    neurons = neurons.astype(np.int32)\n","\n","    # Define a model\n","    model = Sequential()\n","\n","    # Add convolutional layers\n","    for i in range(0, n_layers):\n","        if i == 0:\n","            model.add(Conv2D(neurons[i], KERNEL, input_shape = (stack,\n","                                                                input_size,\n","                                                                input_size)))\n","        else:\n","            model.add(Conv2D(neurons[i], KERNEL))\n","\n","        model.add(Activation('relu'))\n","\n","    # Add max pooling layer\n","    model.add(MaxPooling2D(pool_size = (2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(MAX_NEURONS * 4))\n","    model.add(Activation('relu'))\n","\n","    # Add output layer\n","    model.add(Dense(output_size))\n","    model.add(Activation('sigmoid'))\n","\n","    # Compile the model\n","    model.compile(loss = loss, optimizer = optimizer)\n","\n","    return model\n","\n","def CNN1(inputs, stack, input_size):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","\n","    return model\n","\n","def CNN2(inputs, stack, input_size):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = MaxPooling2D(pool_size = (2, 2))(net)\n","    net = Flatten()(net)\n","\n","    return model\n","\n","def CNN3(inputs, stack, input_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    net = tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu')(inputs)\n","    net = tf.keras.layers.Conv2D(64, (2, 2), activation = 'relu')(net)\n","    net = tf.keras.layers.Conv2D(64, (1, 1), activation = 'relu')(net)\n","    net = tf.keras.layers.Flatten()(net)\n","\n","    return net\n","\n","def create_cnn(cnn, inputs, stack, input_size):\n","    if cnn == \"CNN1\":\n","        net = CNN1(inputs, stack, input_size)\n","    elif cnn == \"CNN2\":\n","        net = CNN2(inputs, stack, input_size)\n","    else:\n","        net = CNN3(inputs, stack, input_size)\n","\n","    return net\n","\n","def create_model(optimizer, loss, stack, input_size, output_size,\n","                  dueling = False, cnn = \"CNN3\"):\n","    inputs = tf.keras.layers.Input(shape = (stack, input_size, input_size))\n","    net = create_cnn(cnn, inputs, stack, input_size)\n","\n","    if dueling:\n","        advt = Dense(3136, activation = 'relu')(net)\n","        advt = Dense(output_size)(advt)\n","        value = Dense(3136, activation = 'relu')(net)\n","        value = Dense(1)(value)\n","\n","        # now to combine the two streams\n","        advt = Lambda(lambda advt: advt - tf.reduce_mean(advt, axis = -1,\n","                                                         keepdims = True))(advt)\n","        value = Lambda(lambda value: tf.tile(value, [1, output_size]))(value)\n","        final = Add()([value, advt])\n","    else:\n","        final = tf.keras.layers.Dense(3136, activation = 'relu')(net)\n","        final = tf.keras.layers.Dense(output_size)(final)\n","\n","    model = tf.keras.models.Model(inputs = inputs, outputs = final)\n","    model.compile(optimizer = tf.keras.optimizers.RMSprop(), loss = loss)\n","\n","    return model\n","\n","  \n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","      \n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","# pylint: disable=C0111\n","\n","import numpy as np\n","import sys\n","import time\n","import operator\n","from datetime import timedelta\n","import collections\n","\n","class SegmentTree(object):\n","    def __init__(self, capacity, operation, neutral_element):\n","        \"\"\"Build a Segment Tree data structure.\n","        https://en.wikipedia.org/wiki/Segment_tree\n","        Can be used as regular array, but with two\n","        important differences:\n","            a) setting item's value is slightly slower.\n","               It is O(lg capacity) instead of O(1).\n","            b) user has access to an efficient `reduce`\n","               operation which reduces `operation` over\n","               a contiguous subsequence of items in the\n","               array.\n","        Paramters\n","        ---------\n","        capacity: int\n","            Total size of the array - must be a power of two.\n","        operation: lambda obj, obj -> obj\n","            and operation for combining elements (eg. sum, max)\n","            must for a mathematical group together with the set of\n","            possible values for array elements.\n","        neutral_element: obj\n","            neutral element for the operation above. eg. float('-inf')\n","            for max and 0 for sum.\n","        \"\"\"\n","        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n","        self._capacity = capacity\n","        self._value = [neutral_element for _ in range(2 * capacity)]\n","        self._operation = operation\n","\n","    def _reduce_helper(self, start, end, node, node_start, node_end):\n","        if start == node_start and end == node_end:\n","            return self._value[node]\n","        mid = (node_start + node_end) // 2\n","        if end <= mid:\n","            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n","        else:\n","            if mid + 1 <= start:\n","                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n","            else:\n","                return self._operation(\n","                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n","                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n","                )\n","\n","    def reduce(self, start=0, end=None):\n","        \"\"\"Returns result of applying `self.operation`\n","        to a contiguous subsequence of the array.\n","            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n","        Parameters\n","        ----------\n","        start: int\n","            beginning of the subsequence\n","        end: int\n","            end of the subsequences\n","        Returns\n","        -------\n","        reduced: obj\n","            result of reducing self.operation over the specified range of array elements.\n","        \"\"\"\n","        if end is None:\n","            end = self._capacity\n","        if end < 0:\n","            end += self._capacity\n","        end -= 1\n","        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n","\n","    def __setitem__(self, idx, val):\n","        # index of the leaf\n","        idx += self._capacity\n","        self._value[idx] = val\n","        idx //= 2\n","        while idx >= 1:\n","            self._value[idx] = self._operation(\n","                self._value[2 * idx],\n","                self._value[2 * idx + 1]\n","            )\n","            idx //= 2\n","\n","    def __getitem__(self, idx):\n","        assert 0 <= idx < self._capacity\n","        return self._value[self._capacity + idx]\n","\n","\n","class SumSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(SumSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=operator.add,\n","            neutral_element=0.0\n","        )\n","\n","    def sum(self, start=0, end=None):\n","        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n","        return super(SumSegmentTree, self).reduce(start, end)\n","\n","    def find_prefixsum_idx(self, prefixsum):\n","        \"\"\"Find the highest index `i` in the array such that\n","            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n","        if array values are probabilities, this function\n","        allows to sample indexes according to the discrete\n","        probability efficiently.\n","        Parameters\n","        ----------\n","        perfixsum: float\n","            upperbound on the sum of array prefix\n","        Returns\n","        -------\n","        idx: int\n","            highest index satisfying the prefixsum constraint\n","        \"\"\"\n","        assert 0 <= prefixsum <= self.sum() + 1e-5\n","        idx = 1\n","        while idx < self._capacity:  # while non-leaf\n","            if self._value[2 * idx] > prefixsum:\n","                idx = 2 * idx\n","            else:\n","                prefixsum -= self._value[2 * idx]\n","                idx = 2 * idx + 1\n","        return idx - self._capacity\n","\n","\n","class MinSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(MinSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=min,\n","            neutral_element=float('inf')\n","        )\n","\n","    def min(self, start=0, end=None):\n","        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n","\n","        return super(MinSegmentTree, self).reduce(start, end)\n","\n","\n","class SumTree:\n","    def __init__(self, capacity):\n","        self._capacity = capacity\n","        self._tree = np.zeros(2 * self._capacity - 1)\n","        self._data = np.zeros(self._capacity, dtype = object)\n","        self._data_idx = 0\n","\n","    @property\n","    def capacity(self):\n","        return self._capacity\n","\n","    @property\n","    def tree(self):\n","        return self._tree\n","\n","    @property\n","    def data(self):\n","        return self._data\n","\n","    def sum(self):\n","        return self._tree[0]\n","\n","    def insert(self, data, priority):\n","#        print(\"Data shape: {}\".format(data.shape))\n","#        print(\"Stored data shape: {}\".format(self._data.shape))\n","        self._data[self._data_idx] = data\n","        tree_idx = self._data_idx + self._capacity - 1\n","        self.update(tree_idx, priority)\n","        self._data_idx += 1\n","        if self._data_idx >= self._capacity:\n","            self._data_idx = 0\n","\n","    def update(self, tree_idx, priority):\n","        delta = priority - self._tree[tree_idx]\n","        self._tree[tree_idx] = priority\n","        while tree_idx != 0:\n","            tree_idx = (tree_idx - 1) // 2  # Get parent\n","            self._tree[tree_idx] += delta\n","\n","    def retrieve(self, val):\n","        tree_idx, parent = None, 0\n","        while True:\n","            left = 2 * parent + 1\n","            right = left + 1\n","            if left >= len(self._tree):  # Leaf\n","                tree_idx = parent\n","                break\n","            else:\n","                if val <= self._tree[left]:\n","                    parent = left\n","                else:\n","                    val -= self._tree[left]\n","                    parent = right\n","\n","        priority = self._tree[tree_idx]\n","        data = self._data[tree_idx - self._capacity + 1]\n","\n","        return tree_idx, priority, data\n","\n","    def max_leaf(self):\n","        return np.max(self.leaves())\n","\n","    def min_leaf(self):\n","        return np.min(self.leaves())\n","\n","    def leaves(self):\n","        return self._tree[-self._capacity:]\n","\n","      \n","import numpy as np\n","from random import sample, uniform\n","from random import random as rand\n","from array import array  # Efficient numeric arrays\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes\n","    ----------\n","    memory: list of experiences\n","        Memory list to insert experiences.\n","    memory_size: int, optional, default = 150000\n","        The ammount of experiences to be stored in the memory.\n","    input_shape: tuple of 3 * int\n","        The shape of the input which will be stored.\n","    \"\"\"\n","    def __init__(self, memory_size = 150000):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        self.memory.append(experience)\n","\n","        if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","            self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\n","\n","        Return\n","        ----------\n","        batch: np.array of batch_size experiences\n","            The batched experiences from memory.\n","        IS_weights: np.array of batch_size of the weights\n","            As it's used only in PER, is an array of ones in this case.\n","        Indexes: list of batch_size * int\n","            As it's used only in PER, return None.\n","        \"\"\"\n","        IS_weights = np.ones((batch_size, ))\n","        batch = np.array(sample(self.memory, batch_size))\n","\n","        return batch, IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            memory_size = 150000\n","\n","        self.memory = []\n","\n","\n","class PrioritizedExperienceReplayNaive:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 150000, alpha = 0.6, epsilon = 0.001,\n","                 beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return self.exp\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        max_priority = self.memory.max_leaf()\n","\n","        if max_priority == 0:\n","            max_priority = self.get_priority(0)\n","\n","        self.memory.insert(experience, max_priority)\n","        self.exp += 1\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        batch = [None] * batch_size\n","        IS_weights = np.zeros((batch_size, ))\n","        tree_indices = [0] * batch_size\n","\n","        memory_sum = self.memory.sum()\n","        len_seg = memory_sum / batch_size\n","        min_prob = self.memory.min_leaf() / memory_sum\n","\n","        for i in range(batch_size):\n","            val = uniform(len_seg * i, len_seg * (i + 1))\n","            tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","            prob = priority / self.memory.sum()\n","            IS_weights[i] = np.power(prob / min_prob, -self.beta)\n","\n","        return np.array(batch), IS_weights, tree_indices\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = SumTree(self.memory_size)\n","        self.exp = 0\n","\n","\n","class PrioritizedExperienceReplay:\n","    def __init__(self, memory_size, nb_epoch = 10000, epsilon = 0.001,\n","                 alpha = 0.6, beta = 0.4, decay = 0.5):\n","        self.memory_size = memory_size\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.max_priority = 1.0\n","        self.reset_memory()\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","        if self.exp_size() < self.memory_size:\n","            self.memory.append(experience)\n","            self.pos += 1\n","        else:\n","            self.pos = (self.pos) % self.memory_size\n","            self.memory[self.pos] = experience\n","            self.pos += 1\n","\n","        self._it_sum[self.pos] = self.max_priority ** self.alpha\n","        self._it_min[self.pos] = self.max_priority ** self.alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = array('i')\n","\n","        for _ in range(batch_size):\n","            _rand = rand()\n","            mass = _rand * self._it_sum.sum(0, self.exp_size() - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","\n","        return res\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def get_samples(self, batch_size):\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = array('f')\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * self.exp_size()) ** (-self.beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * self.exp_size()) ** (-self.beta)\n","            weights.append(weight / max_weight)\n","\n","        weights = np.array(weights, dtype=np.float32)\n","        samples = [self.memory[idx] for idx in idxes]\n","\n","        return np.array(samples), weights, idxes\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def update_priorities(self, idxes, errors):\n","        priorities = self.get_priority(errors)\n","\n","        for idx, priority in zip(idxes, priorities):\n","            self._it_sum[idx] = priority ** self.alpha\n","            self._it_min[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = []\n","        self.pos = 0\n","\n","        it_capacity = 1\n","\n","        while it_capacity < self.memory_size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n","\n","        \n","        \n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms\n","----------\n","    * Simple Deep Q-network (DQN with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double Deep Q-network (Double DQN);\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling Deep Q-network (Dueling DQN);\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * Prioritized Experience Replay (PER);\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns.\n","        Paper: https://arxiv.org/pdf/1703.01327\n","\n","Arguments\n","----------\n","--load: 'file.h5'\n","    Load a previously trained model in '.h5' format.\n","--board_size: int, optional, default = 10\n","    Assign the size of the board.\n","--nb_frames: int, optional, default = 4\n","    Assign the number of frames per stack, default = 4.\n","--nb_actions: int, optional, default = 5\n","    Assign the number of actions possible.\n","--update_freq: int, optional, default = 0.001\n","    Whether to soft or hard update the target. Epochs or ammount of the update.\n","--visual: boolean, optional, default = False\n","    Select wheter or not to draw the game in pygame.\n","--double: boolean, optional, default = False\n","    Use a target network with double DQN logic.\n","--dueling: boolean, optional, default = False\n","    Whether to use dueling network logic, Q(s,a) = A + V.\n","--per: boolean, optional, default = False\n","    Use Prioritized Experience Replay (based on Sum Trees).\n","--local_state: boolean, optional, default = True\n","    Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from array import array\n","from os import path, environ, sys\n","import random\n","import inspect\n","import tensorflow as tf\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import load_model\n","from keras import backend as K\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","K.set_image_dim_ordering('th')  # Setting keras ordering\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes\n","    ----------\n","    memory: object\n","        Memory used in training. ExperienceReplay or PrioritizedExperienceReplay\n","    memory_size: int, optional, default = -1\n","        Capacity of the memory used.\n","    model: keras model\n","        The input model in Keras.\n","    target: keras model, optional, default = None\n","        The target model, used to calculade the fixed Q-targets.\n","    nb_frames: int, optional, default = 4\n","        Ammount of frames for each experience (sars).\n","    board_size: int, optional, default = 10\n","        Size of the board used.\n","    frames: list of experiences\n","        The buffer of frames, store sars experiences.\n","    per: boolean, optional, default = False\n","        Flag for PER usage.\n","    update_target_freq: int or float, default = 0.001\n","        Whether soft or hard updates occur. If < 1, soft updated target model.\n","    n_steps: int, optional, default = 1\n","        Size of the rewards buffer, to use Multi-step returns.\n","    \"\"\"\n","    def __init__(self, model, target = None, memory_size = -1, nb_frames = 4,\n","                 board_size = 10, per = False, update_target_freq = 0.001):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if per:\n","            self.memory = PrioritizedExperienceReplay(memory_size = memory_size)\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size)\n","        \n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.update_target_freq = update_target_freq\n","        self.set_noise_list()\n","        self.clear_frames()\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def set_noise_list(self):\n","        \"\"\"Set a list of noise variables if NoisyNet is involved.\"\"\"\n","        self.noise_list = []\n","        for layer in self.model.layers:\n","            if type(layer) in {NoisyDenseFG}:\n","                self.noise_list.extend(layer.noise_list)\n","\n","    def sample_noise(self):\n","        \"\"\"Resample noise variables in NoisyNet.\"\"\"\n","        for noise in self.noise_list:\n","            self.sess.run(noise.initializer)\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\n","\n","        Return\n","        ----------\n","        expanded_frames: list of experiences\n","            The buffer of frames, shape = (nb_frames, board_size, board_size)\n","        \"\"\"\n","        frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        expanded_frames = np.expand_dims(self.frames, 0)\n","\n","        return expanded_frames\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model_hard(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def transfer_weights(self):\n","        \"\"\"Transfer Weights from Model to Target at rate update_target_freq.\"\"\"\n","        model_weights = self.model.get_weights()\n","        target_weights = self.target.get_weights()\n","\n","        for i in range(len(W)):\n","            target_weights[i] = (self.update_target_freq * model_weights[i]\n","                                 + ((1 - self.update_target_frequency)\n","                                    * target_weights[i]))\n","\n","        self.target.set_weights(target_weights)\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, policy, value,\n","                      win_count, history_step, history_reward,\n","                      history_loss = None, verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    np.mean(history_size[-10:]),\n","                                    max(history_size[-10:]),\n","                                    np.mean(history_step[-10:]),\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}'  # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            if loss is not None:  # Print training performance\n","                text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                              + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                              + 'Mean loss - 100 episodes: {:.4f}')\n","                print(text_perf.format(history_loss[-1],\n","                                       history_loss[-1] / history_step[-1],\n","                                       np.mean(history_loss[-100:])))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   np.mean(history_step[-100:])))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\n","\n","        Return\n","        ----------\n","        loss: float\n","            Training loss of given batch.\n","        \"\"\"\n","        loss = 0.\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions,\n","                                        n_steps = self.n_steps)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, sess, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, optim_rounds = 1, policy = \"EpsGreedyQPolicy\",\n","              verbose = 1, n_steps = 1):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        if not hasattr(self, 'n_steps'):\n","            self.n_steps = n_steps  # Set attribute only once\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_loss = array('f')  # Holds all the losses\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        # Select exploration policy. EpsGreedyQPolicy runs faster, but takes\n","        # longer to converge. BoltzmannGumbelQPolicy is the slowest, but\n","        # converge really fast (0.1 * nb_epoch used in EpsGreedyQPolicy).\n","        # BoltzmannQPolicy is in the middle.\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        elif policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        elif policy == \"GreedyQPolicy\":\n","            q_policy = GreedyQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","        self.sess = sess\n","        \n","        # If optim_rounds is bigger than one, the model will keep optimizing\n","        # after the exploration, in turns of nb_epoch size.\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):             \n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                    text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                                  + '| Loss: {:.4f}')\n","                    print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:  # Exploration and training\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    game.reset_game()\n","                    self.clear_frames()\n","                    S = self.get_game_data(game)\n","\n","                    if n_steps > 1:  # Create multi-step returns buffer.\n","                        n_step_buffer = array('f')\n","                    i = 0\n","                    while not game.game_over:  # Main loop, until game_over\n","                        game.food_pos = game.generate_food()\n","                        rand = random.random()\n","                        #self.sample_noise()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","                        game.play(action)\n","                        r = game.get_reward()\n","                        total_reward += r\n","\n","                        if n_steps > 1:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience)  # Add to the memory\n","                        S = S_prime  # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe:  # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1  # Counter of wins for metrics\n","\n","                    if self.per:  # Advance beta, used in PER\n","                        self.memory.beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None:  # Update the target model\n","                        if update_target_freq >= 1: # Hard updates\n","                            if epoch % self.update_target_freq == 0:\n","                                self.update_target_model_hard()\n","                        elif update_target_freq < 1.:  # Soft updates\n","                            self.transfer_weights()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch = epoch, nb_epoch = nb_epoch,\n","                                           history_size = history_size,\n","                                           history_loss = history_loss,\n","                                           history_step = history_step,\n","                                           history_reward = history_reward,\n","                                           policy = policy, value = value,\n","                                           win_count = win_count,\n","                                           verbose = verbose)\n","\n","    def play(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","            S = self.get_game_data(game)\n","\n","            if visual:\n","                game.create_window()\n","                # The main loop, it pump key_presses and update every tick.\n","                environ['SDL_VIDEO_CENTERED'] = '1'  # Centering the window\n","                previous_size = game.snake.length  # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","\n","            while not game.game_over:\n","                action, value = q_policy.select_action(self.model, S, epoch, nb_actions)\n","                game.play(action)\n","                current_size = game.snake.length  # Update the body size\n","\n","                if visual:\n","                    game.draw(color_list)\n","\n","                    if current_size > previous_size:\n","                        color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   game.snake.length)\n","\n","                        previous_size = current_size\n","\n","                S = self.get_game_data(game)\n","\n","                if game.game_over:\n","                    history_size.append(current_size)\n","                    history_step.append(game.step)\n","                    history_reward.append(game.get_reward())\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(history_size), np.max(history_size),\n","                      np.min(history_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(history_step), np.max(history_step),\n","                      np.min(history_step)))\n","        print(\"Mean rewards: {} | Biggest reward: {} | Smallest reward: {}\"\\\n","              .format(np.mean(history_reward), np.max(history_reward),\n","                      np.min(history_reward)))\n","     \n","        \n","board_size = 10\n","nb_frames = 4\n","\n","game = Game(board_size = board_size, player = \"ROBOT\",\n","                        local_state = True, relative_pos = False)\n","\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","\n","with tf.Session(config = config) as sess:\n","    model = create_model(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","    target = None\n","    sess.run(tf.global_variables_initializer())\n","    agent = Agent(model = model, target = target, memory_size = -1,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","    #%prun agent.train(game, batch_size = 64, sess = sess, nb_epoch = 100, gamma = 0.95, policy = \"GreedyQPolicy\")\n","    agent.train(game, batch_size = 64, sess = sess, nb_epoch = 10000, gamma = 0.95, policy = \"EpsGreedyQPolicy\")\n","    \n","    model.save('keras.h5')\n","\n","    !zip -r model-epsgreedy-bench.zip keras.h5 \n","    from google.colab import files\n","    files.download('model-epsgreedy-bench.zip')\n","    \n","with tf.Session() as sess:\n","  \n","    model = create_model(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","    sess.run(tf.global_variables_initializer())\n","    \n","    function = tf.keras.models.load_model('keras.h5', custom_objects={'clipped_error': clipped_error,\n","                                                   'NoisyDenseFG': NoisyDenseFG,\n","                                                   'GlorotUniform': tf.keras.initializers.glorot_uniform()})\n","\n","    model.set_weights(function.get_weights())\n","    \n","    board_size = 10\n","    nb_frames = 4\n","    nb_actions = 5\n","\n","    target = None\n","\n","    agent = Agent(model = model, target = target, memory_size = -1,\n","                              nb_frames = nb_frames, board_size = board_size,\n","                              per = False)\n","    #%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","    agent.play(game, visual = False, nb_epoch = 10000)\n","       "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0mTfuxkKUDD","executionInfo":{"status":"aborted","timestamp":1612963375270,"user_tz":180,"elapsed":12315,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["with tf.Session() as sess:\n","  \n","    model = create_model(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","    sess.run(tf.global_variables_initializer())\n","    \n","    function = tf.keras.models.load_model('keras.h5', custom_objects={'clipped_error': clipped_error,\n","                                                   'NoisyDenseFG': NoisyDenseFG,\n","                                                   'GlorotUniform': tf.keras.initializers.glorot_uniform()})\n","\n","    model.set_weights(function.get_weights())\n","    \n","    board_size = 10\n","    nb_frames = 4\n","    nb_actions = 5\n","\n","    target = None\n","\n","    agent = Agent(model = model, target = target, memory_size = -1,\n","                              nb_frames = nb_frames, board_size = board_size,\n","                              per = False)\n","    #%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","    agent.play(game, visual = False, nb_epoch = 10000)\n","       "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7kElfQ8y2K9","executionInfo":{"status":"aborted","timestamp":1612963375271,"user_tz":180,"elapsed":12312,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI.\n","\"\"\"\n","\n","import sys  # To close the window when the game is over\n","from array import array  # Efficient numeric arrays\n","from os import environ, path  # To center the game window the best possible\n","import random  # Random numbers used for the food\n","import logging  # Logging function for movements and errors\n","from itertools import tee  # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4,\n","           'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","rewards = {'MOVE': -0.005, 'GAME_OVER': -1, 'SCORED': 1}\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","# Speed levels possible to human players, MEGA HARDCORE starts with MEDIUM and\n","# increases with snake size\n","levels = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","speeds = {'EASY': 80, 'MEDIUM': 60, 'HARD': 40, 'MEGA_HARDCORE': 65}\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    BOARD_SIZE: int, optional, default = 30\n","        The size of the board.\n","    BLOCK_SIZE: int, optional, default = 20\n","        The size in pixels of a block.\n","    HEAD_COLOR: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    TAIL_COLOR: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    FOOD_COLOR: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    GAME_SPEED: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    BENCHMARK: int, optional, default = 10\n","        Ammount of matches to BENCHMARK and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, BOARD_SIZE = 30, BLOCK_SIZE = 20,\n","                 HEAD_COLOR = (42, 42, 42), TAIL_COLOR = (152, 152, 152),\n","                 FOOD_COLOR = (200, 0, 0), GAME_SPEED = 80, GAME_FPS = 100,\n","                 BENCHMARK = 10):\n","        \"\"\"Initialize all global variables. Can be updated with argument_handler.\n","        \"\"\"\n","        self.BOARD_SIZE = BOARD_SIZE\n","        self.BLOCK_SIZE = BLOCK_SIZE\n","        self.HEAD_COLOR = HEAD_COLOR\n","        self.TAIL_COLOR = TAIL_COLOR\n","        self.FOOD_COLOR = FOOD_COLOR\n","        self.GAME_SPEED = GAME_SPEED\n","        self.GAME_FPS = GAME_FPS\n","        self.BENCHMARK = BENCHMARK\n","\n","        if self.BOARD_SIZE > 50: # Warn the user about performance\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                pass\n","            else:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [BOARD_SIZE / 4, BOARD_SIZE / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def is_movement_invalid(self, action):\n","        valid = False\n","\n","        if (action, self.previous_action) in forbidden_moves:\n","            valid = True\n","\n","        return valid\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if action == actions['IDLE'] or self.is_movement_invalid(action):\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with BOARD_SIZE * BLOCK_SIZE dimension.\"\"\"\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def cycle_menu(self, menu_options, list_menu, dict, img = None,\n","                   img_rect = None):\n","        \"\"\"\"\"\"\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            events = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for i, option in enumerate(menu_options):\n","                if option is not None:\n","                    option.draw()\n","                    option.hovered = False\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        for event in events:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = dict[list_menu[i]]\n","\n","            if selected_option is not None:\n","                selected = True\n","            if img is not None:\n","                self.window.blit(img, img_rect.bottomleft)\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def cycle_matches(self, n_matches = 10, mega_hardcore = False):\n","        \"\"\"\"\"\"\n","        self.reset_game()\n","        score = array('i')\n","\n","        for match in range(n_matches):\n","            score.append(self.single_player(mega_hardcore))\n","\n","        return score\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images\" +\n","                                              \"/snake_logo.png\")).convert()\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE,\n","                                     int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","        list_menu = ['PLAY', 'BENCHMARK', 'LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                             4 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                             6 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                             8 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","        selected_option = self.cycle_menu(menu_options, list_menu, options,\n","                                          img, img_rect)\n","\n","        return selected_option\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                            4 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                            12 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                var.GAME_SPEED, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = 1,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                var.GAME_SPEED, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = var.BENCHMARK,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        score_option = None\n","\n","        if len(score) == var.BENCHMARK:\n","            score_option = TextBlock(' ADD TO LEADERBOARDS ',\n","                                        (self.screen_rect.centerx,\n","                                         8 * self.screen_rect.centery / 10),\n","                                        self.window, (1 / 15), \"menu\")\n","\n","        text_score = 'SCORE: ' + str(np.mean(score))\n","        list_menu = ['PLAY', 'MENU', 'ADD_LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                            4 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 15), \"menu\"),\n","                           TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                            6 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 15), \"menu\"),\n","                           score_option,\n","                           TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                            10 * self.screen_rect.centery / 10),\n","                                            self.window, (1 / 15), \"menu\"),\n","                           TextBlock(text_score, (self.screen_rect.centerx,\n","                                             15 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 10), \"text\")]\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","        selected_option = self.cycle_menu(menu_options, list_menu, options)\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        list_menu = ['EASY', 'MEDIUM', 'HARD', 'MEGA_HARDCORE']\n","        menu_options = [TextBlock(levels[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\")]\n","\n","        speed = self.cycle_menu(menu_options, list_menu, speeds)\n","        mega_hardcore = False\n","\n","        if speed == speeds['MEGA_HARDCORE']:\n","            mega_hardcore = True\n","\n","        return speed, mega_hardcore\n","\n","    def single_player(self, mega_hardcore = False):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where snakes moves after elapsed time is bigger than the\n","        # move_wait time. The last_key pressed is recorded to make the game more\n","        # smooth for human players.\n","        elapsed = 0\n","        last_key = self.snake.previous_action\n","        move_wait = var.GAME_SPEED\n","\n","        while not self.game_over:\n","            elapsed += self.fps.get_time()  # Get elapsed time since last call.\n","\n","            if mega_hardcore:  # Progressive speed increments, the hardest.\n","                move_wait = var.GAME_SPEED - (2 * (self.snake.length - 3))\n","\n","            key_input = self.handle_input()  # Receive inputs with tick.\n","            invalid_key = self.snake.is_movement_invalid(key_input)\n","\n","            if key_input is not None and not invalid_key:\n","                last_key = key_input\n","\n","            if elapsed >= move_wait:  # Move and redraw\n","                elapsed = 0\n","                self.game_over = self.play(last_key)\n","                current_size = self.snake.length  # Update the body size\n","\n","                if current_size > previous_size:\n","                    color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   current_size)\n","\n","                    previous_size = current_size\n","\n","                self.draw(color_list)\n","\n","            pygame.display.update()\n","            self.fps.tick(100)  # Limit FPS to 100\n","\n","        score = current_size - 3  # After the game is over, record score\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = None\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            action = actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            action = actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            action = actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            action = actions['DOWN']\n","\n","        return action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = point_type['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = rewards['MOVE']\n","\n","        if self.game_over:\n","            reward = rewards['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","\n","        if self.player == \"ROBOT\":\n","            pygame.display.update()\n","            self.fps.tick(30)  # Limit FPS to 60\n","\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","#!/usr/bin/env python\n","import numpy as np\n","from argparse import ArgumentParser\n","\n","from keras import backend as K\n","import keras.optimizers as optimizers\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","class HandleArguments:\n","        \"\"\"Handle arguments provided in the command line when executing the model.\n","\n","        Attributes:\n","            args: arguments parsed in the command line.\n","            status_load: a flag for usage of --load argument.\n","            status_visual: a flag for usage of --visual argument.\n","\n","            NEED UPDATE!\n","        \"\"\"\n","        def __init__(self):\n","            self.parser = ArgumentParser() # Receive arguments\n","            self.parser.add_argument(\"-l\", \"--load\", help = \"load a previously trained model. the argument is the filename\", required = False, default = \"\")\n","            self.parser.add_argument(\"-v\", \"--visual\", help = \"define board size\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-du\", \"--dueling\", help = \"use dueling DQN\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-do\", \"--double\", help = \"use double DQN\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-p\", \"--per\", help = \"use Prioritized Experience Replay\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-ls\", \"--local_state\", help = \"define board size\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-g\", \"--board_size\", help = \"define board size\", required = False, default = 10, type = int)\n","            self.parser.add_argument(\"-nf\", \"--nb_frames\", help = \"define board size\", required = False, default = 4, type = int)\n","            self.parser.add_argument(\"-na\", \"--nb_actions\", help = \"define board size\", required = False, default = 5, type = int)\n","            self.parser.add_argument(\"-uf\", \"--update_freq\", help = \"frequency to update target\", required = False, default = 500, type = int)\n","\n","            self.args = self.parser.parse_args()\n","            self.status_load = False\n","            self.status_visual = False\n","            self.local_state = False\n","            self.dueling = False\n","            self.double = False\n","            self.per = False\n","\n","            if self.args.load:\n","                script_dir = path.dirname(__file__) # Absolute dir the script is in\n","                abs_file_path = path.join(script_dir, self.args.load)\n","                model = load_model(abs_file_path)\n","\n","                self.status_load = True\n","\n","            if self.args.visual:\n","                self.status_visual = True\n","\n","            if self.args.local_state:\n","                self.local_state = True\n","\n","            if self.args.dueling:\n","                self.dueling = True\n","\n","            if self.args.double:\n","                self.double = True\n","\n","            if self.args.per:\n","                self.per = True\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\tif K.backend() == 'tensorflow':\n","\t\tif hasattr(tf, 'select'):\n","\t\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\t\telse:\n","\t\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\telif K.backend() == 'theano':\n","\t\tfrom theano import tensor as T\n","\t\treturn T.switch(condition, squared_loss, linear_loss)\n","\telse:\n","\t\traise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","# pylint: disable=C0111\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.framework import tensor_shape\n","from tensorflow.python.layers import base\n","from tensorflow.python.ops.init_ops import Constant\n","from keras.layers import *\n","\n","class NoisyDense(Dense):\n","\n","    def build(self, input_shape):\n","        input_shape = tensor_shape.TensorShape(input_shape)\n","        if input_shape[-1].value is None:\n","            raise ValueError('The last dimension of the inputs to `Dense` '\n","                             'should be defined. Found `None`.')\n","        self.input_spec = base.InputSpec(min_ndim=2,\n","                                         axes={-1: input_shape[-1].value})\n","        kernel_shape = [input_shape[-1].value, self.units]\n","        kernel_quiet = self.add_weight('kernel_quiet',\n","                                         shape=kernel_shape,\n","                                         initializer=self.kernel_initializer,\n","                                         regularizer=self.kernel_regularizer,\n","                                         constraint=self.kernel_constraint,\n","                                         trainable=True)\n","        scale_init = Constant(value=(0.5 / np.sqrt(kernel_shape[0])))\n","        kernel_noise_scale = self.add_weight('kernel_noise_scale',\n","                                               shape=kernel_shape,\n","                                               initializer=scale_init,\n","                                               trainable=True)\n","        kernel_noise = self.make_kernel_noise(shape=kernel_shape)\n","        self.kernel = kernel_quiet + kernel_noise_scale * kernel_noise\n","        if self.use_bias:\n","            bias_shape = [self.units,]\n","            bias_quiet = self.add_weight('bias_quiet',\n","                                           shape=bias_shape,\n","                                           initializer=self.bias_initializer,\n","                                           regularizer=self.bias_regularizer,\n","                                           constraint=self.bias_constraint,\n","                                           trainable=True)\n","            bias_noise_scale = self.add_weight(name='bias_noise_scale',\n","                                                 shape=bias_shape,\n","                                                 initializer=scale_init,\n","                                                 trainable=True)\n","            bias_noise = self.make_bias_noise(shape=bias_shape)\n","            self.bias = bias_quiet + bias_noise_scale * bias_noise\n","        else:\n","            self.bias = None\n","        self.built = True\n","\n","    def make_kernel_noise(self, shape):\n","        raise NotImplementedError\n","\n","    def make_bias_noise(self, shape):\n","        raise NotImplementedError\n","\n","\n","'''\n","Noisy dense layer with independent Gaussian noise\n","'''\n","class NoisyDenseIG(NoisyDense):\n","\n","    def make_kernel_noise(self, shape):\n","        noise = tf.random_normal(shape)\n","        kernel_noise = tf.Variable(noise, trainable=False)\n","        self.noise_list = [kernel_noise]\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        noise = tf.random_normal(shape)\n","        bias_noise = tf.Variable(noise, trainable=False)\n","        self.noise_list.append(bias_noise)\n","        return bias_noise\n","\n","\n","'''\n","Noisy dense layer with factorized Gaussian noise\n","'''\n","class NoisyDenseFG(NoisyDense):\n","\n","    def make_kernel_noise(self, shape):\n","        kernel_noise_input = self.make_fg_noise(shape = [shape[0]])\n","        kernel_noise_output = self.make_fg_noise(shape = [shape[1]])\n","        self.noise_list = [kernel_noise_input, kernel_noise_output]\n","        kernel_noise = kernel_noise_input[:, tf.newaxis] * kernel_noise_output\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        return self.noise_list[1] # kernel_noise_output\n","\n","    def make_fg_noise(self, shape):\n","        noise = tf.random_normal(shape)\n","        trans_noise = tf.sign(noise) * tf.sqrt(tf.abs(noise))\n","        return tf.Variable(trans_noise, trainable=False)\n","\n","\n","      \n","#!/usr/bin/env python\n","\n","\"\"\" Needs update!\n","\"\"\"\n","\n","import numpy as np\n","from keras.models import Sequential, load_model, Model\n","from keras.layers import *\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def weird_CNN(optimizer, loss, stack, input_size, output_size, min_neurons = 16,\n","         max_neurons = 128, kernel_size = (3,3), layers = 4):\n","    # INPUTS\n","    # size     - size of the input images\n","    # n_layers - number of layers\n","    # OUTPUTS\n","    # model    - compiled CNN\n","\n","    # Define hyperparamters\n","    MIN_NEURONS = min_neurons\n","    MAX_NEURONS = max_neurons\n","    KERNEL = kernel_size\n","    n_layers = layers\n","\n","    # Determine the # of neurons in each convolutional layer\n","    steps = np.floor(MAX_NEURONS / (n_layers + 1))\n","    neurons = np.arange(MIN_NEURONS, MAX_NEURONS, steps)\n","    neurons = neurons.astype(np.int32)\n","\n","    # Define a model\n","    model = Sequential()\n","\n","    # Add convolutional layers\n","    for i in range(0, n_layers):\n","        if i == 0:\n","            model.add(Conv2D(neurons[i], KERNEL, input_shape = (stack,\n","                                                                input_size,\n","                                                                input_size)))\n","        else:\n","            model.add(Conv2D(neurons[i], KERNEL))\n","\n","        model.add(Activation('relu'))\n","\n","    # Add max pooling layer\n","    model.add(MaxPooling2D(pool_size = (2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(MAX_NEURONS * 4))\n","    model.add(Activation('relu'))\n","\n","    # Add output layer\n","    model.add(Dense(output_size))\n","    model.add(Activation('sigmoid'))\n","\n","    # Compile the model\n","    model.compile(loss = loss, optimizer = optimizer)\n","\n","    return model\n","\n","def CNN1(inputs, stack, input_size):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","\n","    return model\n","\n","def CNN2(inputs, stack, input_size):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = MaxPooling2D(pool_size = (2, 2))(net)\n","    net = Flatten()(net)\n","\n","    return model\n","\n","def CNN3(inputs, stack, input_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    net = Conv2D(32, (3, 3), activation = 'relu')(inputs)\n","    #net = Conv2D(64, (2, 2), activation = 'relu')(net)\n","    #net = Conv2D(64, (1, 1), activation = 'relu')(net)\n","    net = Reshape(target_shape = (1, 32, 8, 8))(net)\n","    net = ConvLSTM2D(64, (2, 2))(net)\n","    net = Reshape(target_shape = (1, 64, 7, 7))(net)\n","    net = ConvLSTM2D(64, (2, 2))(net)\n","    net = Flatten()(net)\n","    #net = Reshape((1, 3136))(net)\n","    #net = LSTM(6)(net)\n","    #net = Reshape((3136))(net)\n","\n","    return net\n","\n","def create_cnn(cnn, inputs, stack, input_size):\n","    if cnn == \"CNN1\":\n","        net = CNN1(inputs, stack, input_size)\n","    elif cnn == \"CNN2\":\n","        net = CNN2(inputs, stack, input_size)\n","    else:\n","        net = CNN3(inputs, stack, input_size)\n","\n","    return net\n","\n","def create_model(optimizer, loss, stack, input_size, output_size,\n","                  dueling = False, cnn = \"CNN3\"):\n","    inputs = Input(shape = (stack, input_size, input_size))\n","    net = create_cnn(cnn, inputs, stack, input_size)\n","\n","    if dueling:\n","        advt = Dense(3136, activation = 'relu')(net)\n","        advt = Dense(output_size)(advt)\n","        value = Dense(3136, activation = 'relu')(net)\n","        value = Dense(1)(value)\n","\n","        # now to combine the two streams\n","        advt = Lambda(lambda advt: advt - tf.reduce_mean(advt, axis = -1,\n","                                                         keepdims = True))(advt)\n","        value = Lambda(lambda value: tf.tile(value, [1, output_size]))(value)\n","        final = Add()([value, advt])\n","    else:\n","        final = Dense(3136, activation = 'relu')(net)\n","        final = Dense(output_size)(final)\n","\n","    model = Model(inputs = inputs, outputs = final)\n","    model.compile(optimizer = optimizer, loss = loss)\n","    model.summary()\n","    \n","    return model\n","\n","  \n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","      \n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","# pylint: disable=C0111\n","\n","import numpy as np\n","import sys\n","import time\n","import operator\n","from datetime import timedelta\n","import collections\n","\n","class SegmentTree(object):\n","    def __init__(self, capacity, operation, neutral_element):\n","        \"\"\"Build a Segment Tree data structure.\n","        https://en.wikipedia.org/wiki/Segment_tree\n","        Can be used as regular array, but with two\n","        important differences:\n","            a) setting item's value is slightly slower.\n","               It is O(lg capacity) instead of O(1).\n","            b) user has access to an efficient `reduce`\n","               operation which reduces `operation` over\n","               a contiguous subsequence of items in the\n","               array.\n","        Paramters\n","        ---------\n","        capacity: int\n","            Total size of the array - must be a power of two.\n","        operation: lambda obj, obj -> obj\n","            and operation for combining elements (eg. sum, max)\n","            must for a mathematical group together with the set of\n","            possible values for array elements.\n","        neutral_element: obj\n","            neutral element for the operation above. eg. float('-inf')\n","            for max and 0 for sum.\n","        \"\"\"\n","        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n","        self._capacity = capacity\n","        self._value = [neutral_element for _ in range(2 * capacity)]\n","        self._operation = operation\n","\n","    def _reduce_helper(self, start, end, node, node_start, node_end):\n","        if start == node_start and end == node_end:\n","            return self._value[node]\n","        mid = (node_start + node_end) // 2\n","        if end <= mid:\n","            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n","        else:\n","            if mid + 1 <= start:\n","                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n","            else:\n","                return self._operation(\n","                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n","                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n","                )\n","\n","    def reduce(self, start=0, end=None):\n","        \"\"\"Returns result of applying `self.operation`\n","        to a contiguous subsequence of the array.\n","            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n","        Parameters\n","        ----------\n","        start: int\n","            beginning of the subsequence\n","        end: int\n","            end of the subsequences\n","        Returns\n","        -------\n","        reduced: obj\n","            result of reducing self.operation over the specified range of array elements.\n","        \"\"\"\n","        if end is None:\n","            end = self._capacity\n","        if end < 0:\n","            end += self._capacity\n","        end -= 1\n","        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n","\n","    def __setitem__(self, idx, val):\n","        # index of the leaf\n","        idx += self._capacity\n","        self._value[idx] = val\n","        idx //= 2\n","        while idx >= 1:\n","            self._value[idx] = self._operation(\n","                self._value[2 * idx],\n","                self._value[2 * idx + 1]\n","            )\n","            idx //= 2\n","\n","    def __getitem__(self, idx):\n","        assert 0 <= idx < self._capacity\n","        return self._value[self._capacity + idx]\n","\n","\n","class SumSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(SumSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=operator.add,\n","            neutral_element=0.0\n","        )\n","\n","    def sum(self, start=0, end=None):\n","        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n","        return super(SumSegmentTree, self).reduce(start, end)\n","\n","    def find_prefixsum_idx(self, prefixsum):\n","        \"\"\"Find the highest index `i` in the array such that\n","            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n","        if array values are probabilities, this function\n","        allows to sample indexes according to the discrete\n","        probability efficiently.\n","        Parameters\n","        ----------\n","        perfixsum: float\n","            upperbound on the sum of array prefix\n","        Returns\n","        -------\n","        idx: int\n","            highest index satisfying the prefixsum constraint\n","        \"\"\"\n","        assert 0 <= prefixsum <= self.sum() + 1e-5\n","        idx = 1\n","        while idx < self._capacity:  # while non-leaf\n","            if self._value[2 * idx] > prefixsum:\n","                idx = 2 * idx\n","            else:\n","                prefixsum -= self._value[2 * idx]\n","                idx = 2 * idx + 1\n","        return idx - self._capacity\n","\n","\n","class MinSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(MinSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=min,\n","            neutral_element=float('inf')\n","        )\n","\n","    def min(self, start=0, end=None):\n","        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n","\n","        return super(MinSegmentTree, self).reduce(start, end)\n","\n","\n","class SumTree:\n","    def __init__(self, capacity):\n","        self._capacity = capacity\n","        self._tree = np.zeros(2 * self._capacity - 1)\n","        self._data = np.zeros(self._capacity, dtype = object)\n","        self._data_idx = 0\n","\n","    @property\n","    def capacity(self):\n","        return self._capacity\n","\n","    @property\n","    def tree(self):\n","        return self._tree\n","\n","    @property\n","    def data(self):\n","        return self._data\n","\n","    def sum(self):\n","        return self._tree[0]\n","\n","    def insert(self, data, priority):\n","#        print(\"Data shape: {}\".format(data.shape))\n","#        print(\"Stored data shape: {}\".format(self._data.shape))\n","        self._data[self._data_idx] = data\n","        tree_idx = self._data_idx + self._capacity - 1\n","        self.update(tree_idx, priority)\n","        self._data_idx += 1\n","        if self._data_idx >= self._capacity:\n","            self._data_idx = 0\n","\n","    def update(self, tree_idx, priority):\n","        delta = priority - self._tree[tree_idx]\n","        self._tree[tree_idx] = priority\n","        while tree_idx != 0:\n","            tree_idx = (tree_idx - 1) // 2  # Get parent\n","            self._tree[tree_idx] += delta\n","\n","    def retrieve(self, val):\n","        tree_idx, parent = None, 0\n","        while True:\n","            left = 2 * parent + 1\n","            right = left + 1\n","            if left >= len(self._tree):  # Leaf\n","                tree_idx = parent\n","                break\n","            else:\n","                if val <= self._tree[left]:\n","                    parent = left\n","                else:\n","                    val -= self._tree[left]\n","                    parent = right\n","\n","        priority = self._tree[tree_idx]\n","        data = self._data[tree_idx - self._capacity + 1]\n","\n","        return tree_idx, priority, data\n","\n","    def max_leaf(self):\n","        return np.max(self.leaves())\n","\n","    def min_leaf(self):\n","        return np.min(self.leaves())\n","\n","    def leaves(self):\n","        return self._tree[-self._capacity:]\n","\n","      \n","import numpy as np\n","from random import sample, uniform\n","from array import array  # Efficient numeric arrays\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes\n","    ----------\n","    memory: list of experiences\n","        Memory list to insert experiences.\n","    memory_size: int, optional, default = 150000\n","        The ammount of experiences to be stored in the memory.\n","    input_shape: tuple of 3 * int\n","        The shape of the input which will be stored.\n","    \"\"\"\n","    def __init__(self, memory_size = 150000):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        self.memory.append(experience)\n","\n","        if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","            self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\n","\n","        Return\n","        ----------\n","        batch: np.array of batch_size experiences\n","            The batched experiences from memory.\n","        IS_weights: np.array of batch_size of the weights\n","            As it's used only in PER, is an array of ones in this case.\n","        Indexes: list of batch_size * int\n","            As it's used only in PER, return None.\n","        \"\"\"\n","        IS_weights = np.ones((batch_size, ))\n","        batch = np.array(sample(self.memory, batch_size))\n","\n","        return batch, IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            memory_size = 150000\n","\n","        self.memory = []\n","\n","\n","class PrioritizedExperienceReplayNaive:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 150000, alpha = 0.6, epsilon = 0.001,\n","                 beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return self.exp\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        max_priority = self.memory.max_leaf()\n","\n","        if max_priority == 0:\n","            max_priority = self.get_priority(0)\n","\n","        self.memory.insert(experience, max_priority)\n","        self.exp += 1\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        batch = [None] * batch_size\n","        IS_weights = np.zeros((batch_size, ))\n","        tree_indices = [0] * batch_size\n","\n","        memory_sum = self.memory.sum()\n","        len_seg = memory_sum / batch_size\n","        min_prob = self.memory.min_leaf() / memory_sum\n","\n","        for i in range(batch_size):\n","            val = uniform(len_seg * i, len_seg * (i + 1))\n","            tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","            prob = priority / self.memory.sum()\n","            IS_weights[i] = np.power(prob / min_prob, -self.beta)\n","\n","        return np.array(batch), IS_weights, tree_indices\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = SumTree(self.memory_size)\n","        self.exp = 0\n","\n","\n","class PrioritizedExperienceReplay:\n","    def __init__(self, memory_size, nb_epoch = 10000, epsilon = 0.001,\n","                 alpha = 0.6, beta = 0.4, decay = 0.5):\n","        self.memory_size = memory_size\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.max_priority = 1.0\n","        self.reset_memory()\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","        if self.exp_size() < self.memory_size:\n","            self.memory.append(experience)\n","            self.pos += 1\n","        else:\n","            self.memory[self.pos] = experience\n","            self.pos = (self.pos + 1) % self.memory_size\n","\n","        self._it_sum[self.pos] = self.max_priority ** self.alpha\n","        self._it_min[self.pos] = self.max_priority ** self.alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = array('i')\n","\n","        for _ in range(batch_size):\n","            mass = random() * self._it_sum.sum(0, self.exp_size() - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","\n","        return res\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def get_samples(self, batch_size):\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = array('f')\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * self.exp_size()) ** (-self.beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * self.exp_size()) ** (-self.beta)\n","            weights.append(weight / max_weight)\n","\n","        weights = np.array(weights, dtype=np.float32)\n","        samples = [self.memory[idx] for idx in idxes]\n","\n","        return np.array(samples), weights, idxes\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def update_priorities(self, idxes, errors):\n","        priorities = self.get_priority(errors)\n","\n","        for idx, priority in zip(idxes, priorities):\n","            self._it_sum[idx] = priority ** self.alpha\n","            self._it_min[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = []\n","        self.pos = 0\n","\n","        it_capacity = 1\n","\n","        while it_capacity < self.memory_size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n","\n","        \n","        \n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms\n","----------\n","    * Simple Deep Q-network (DQN with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double Deep Q-network (Double DQN);\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling Deep Q-network (Dueling DQN);\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * Prioritized Experience Replay (PER);\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns.\n","        Paper: https://arxiv.org/pdf/1703.01327\n","\n","Arguments\n","----------\n","--load: 'file.h5'\n","    Load a previously trained model in '.h5' format.\n","--board_size: int, optional, default = 10\n","    Assign the size of the board.\n","--nb_frames: int, optional, default = 4\n","    Assign the number of frames per stack, default = 4.\n","--nb_actions: int, optional, default = 5\n","    Assign the number of actions possible.\n","--update_freq: int, optional, default = 0.001\n","    Whether to soft or hard update the target. Epochs or ammount of the update.\n","--visual: boolean, optional, default = False\n","    Select wheter or not to draw the game in pygame.\n","--double: boolean, optional, default = False\n","    Use a target network with double DQN logic.\n","--dueling: boolean, optional, default = False\n","    Whether to use dueling network logic, Q(s,a) = A + V.\n","--per: boolean, optional, default = False\n","    Use Prioritized Experience Replay (based on Sum Trees).\n","--local_state: boolean, optional, default = True\n","    Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from array import array\n","from os import path, environ, sys\n","import random\n","import inspect\n","import tensorflow as tf\n","\n","from keras.optimizers import *\n","from keras.models import load_model\n","from keras import backend as K\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","K.set_image_dim_ordering('th')  # Setting keras ordering\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes\n","    ----------\n","    memory: object\n","        Memory used in training. ExperienceReplay or PrioritizedExperienceReplay\n","    memory_size: int, optional, default = -1\n","        Capacity of the memory used.\n","    model: keras model\n","        The input model in Keras.\n","    target: keras model, optional, default = None\n","        The target model, used to calculade the fixed Q-targets.\n","    nb_frames: int, optional, default = 4\n","        Ammount of frames for each experience (sars).\n","    board_size: int, optional, default = 10\n","        Size of the board used.\n","    frames: list of experiences\n","        The buffer of frames, store sars experiences.\n","    per: boolean, optional, default = False\n","        Flag for PER usage.\n","    update_target_freq: int or float, default = 0.001\n","        Whether soft or hard updates occur. If < 1, soft updated target model.\n","    n_steps: int, optional, default = 1\n","        Size of the rewards buffer, to use Multi-step returns.\n","    \"\"\"\n","    def __init__(self, model, target = None, memory_size = -1, nb_frames = 4,\n","                 board_size = 10, per = False, update_target_freq = 0.001):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if per:\n","            self.memory = PrioritizedExperienceReplay(memory_size = memory_size)\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size)\n","        \n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.update_target_freq = update_target_freq\n","        self.set_noise_list()\n","        self.clear_frames()\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def set_noise_list(self):\n","        \"\"\"Set a list of noise variables if NoisyNet is involved.\"\"\"\n","        self.noise_list = []\n","        for layer in self.model.layers:\n","            if type(layer) in {NoisyDenseFG}:\n","                self.noise_list.extend(layer.noise_list)\n","\n","    def sample_noise(self):\n","        \"\"\"Resample noise variables in NoisyNet.\"\"\"\n","        for noise in self.noise_list:\n","            self.sess.run(noise.initializer)\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\n","\n","        Return\n","        ----------\n","        expanded_frames: list of experiences\n","            The buffer of frames, shape = (nb_frames, board_size, board_size)\n","        \"\"\"\n","        frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        expanded_frames = np.expand_dims(self.frames, 0)\n","\n","        return expanded_frames\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model_hard(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def transfer_weights(self):\n","        \"\"\"Transfer Weights from Model to Target at rate update_target_freq.\"\"\"\n","        model_weights = self.model.get_weights()\n","        target_weights = self.target.get_weights()\n","\n","        for i in range(len(W)):\n","            target_weights[i] = (self.update_target_freq * model_weights[i]\n","                                 + ((1 - self.update_target_frequency)\n","                                    * target_weights[i]))\n","\n","        self.target.set_weights(target_weights)\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, policy, value,\n","                      win_count, history_step, history_reward,\n","                      history_loss = None, verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    np.mean(history_size[-50:]),\n","                                    max(history_size[-50:]),\n","                                    np.mean(history_step[-50:]),\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}'  # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            if loss is not None:  # Print training performance\n","                text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                              + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                              + 'Mean loss - 100 episodes: {:.4f}')\n","                print(text_perf.format(history_loss[-1],\n","                                       history_loss[-1] / history_step[-1],\n","                                       np.mean(history_loss[-100:])))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   np.mean(history_step[-100:])))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\n","\n","        Return\n","        ----------\n","        loss: float\n","            Training loss of given batch.\n","        \"\"\"\n","        loss = 0.\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions,\n","                                        n_steps = self.n_steps)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, sess, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, optim_rounds = 1, policy = \"EpsGreedyQPolicy\",\n","              verbose = 1, n_steps = 1):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        if not hasattr(self, 'n_steps'):\n","            self.n_steps = n_steps  # Set attribute only once\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_loss = array('f')  # Holds all the losses\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        # Select exploration policy. EpsGreedyQPolicy runs faster, but takes\n","        # longer to converge. BoltzmannGumbelQPolicy is the slowest, but\n","        # converge really fast (0.1 * nb_epoch used in EpsGreedyQPolicy).\n","        # BoltzmannQPolicy is in the middle.\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        elif policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        elif policy == \"GreedyQPolicy\":\n","            q_policy = GreedyQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","        self.sess = sess\n","        \n","        # If optim_rounds is bigger than one, the model will keep optimizing\n","        # after the exploration, in turns of nb_epoch size.\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):             \n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                    text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                                  + '| Loss: {:.4f}')\n","                    print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:  # Exploration and training\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    game.reset_game()\n","                    self.clear_frames()\n","                    S = self.get_game_data(game)\n","\n","                    if n_steps > 1:  # Create multi-step returns buffer.\n","                        n_step_buffer = array('f')\n","                    i = 0\n","                    while not game.game_over:  # Main loop, until game_over\n","                        game.food_pos = game.generate_food()\n","                        self.sample_noise()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","                        game.play(action)\n","                        r = game.get_reward()\n","                        total_reward += r\n","\n","                        if n_steps > 1:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience)  # Add to the memory\n","                        S = S_prime  # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe:  # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1  # Counter of wins for metrics\n","\n","                    if self.per:  # Advance beta, used in PER\n","                        self.memory.beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None:  # Update the target model\n","                        if update_target_freq >= 1: # Hard updates\n","                            if epoch % self.update_target_freq == 0:\n","                                self.update_target_model_hard()\n","                        elif update_target_freq < 1.:  # Soft updates\n","                            self.transfer_weights()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 50 == 0:\n","                        self.print_metrics(epoch = epoch, nb_epoch = nb_epoch,\n","                                           history_size = history_size,\n","                                           history_loss = history_loss,\n","                                           history_step = history_step,\n","                                           history_reward = history_reward,\n","                                           policy = policy, value = value,\n","                                           win_count = win_count,\n","                                           verbose = verbose)\n","\n","    def play(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","            S = self.get_game_data(game)\n","\n","            if visual:\n","                game.create_window()\n","                # The main loop, it pump key_presses and update every tick.\n","                environ['SDL_VIDEO_CENTERED'] = '1'  # Centering the window\n","                previous_size = game.snake.length  # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","\n","            while not game.game_over:\n","                action, value = q_policy.select_action(self.model, S, epoch, nb_actions)\n","                game.play(action)\n","                current_size = game.snake.length  # Update the body size\n","\n","                if visual:\n","                    game.draw(color_list)\n","\n","                    if current_size > previous_size:\n","                        color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   game.snake.length)\n","\n","                        previous_size = current_size\n","\n","                S = self.get_game_data(game)\n","\n","                if game.game_over:\n","                    history_size.append(current_size)\n","                    history_step.append(game.step)\n","                    history_reward.append(game.get_reward())\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(history_size), np.max(history_size),\n","                      np.min(history_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(history_step), np.max(history_step),\n","                      np.min(history_step)))\n","        print(\"Mean rewards: {} | Biggest reward: {} | Smallest reward: {}\"\\\n","              .format(np.mean(history_reward), np.max(history_reward),\n","                      np.min(history_reward)))\n","\n","    \n","!pip install line_profiler\n","%load_ext line_profiler \n","import timeit\n","        \n","board_size = 10\n","nb_frames = 4\n","\n","game = Game(board_size = board_size, player = \"ROBOT\",\n","                        local_state = True, relative_pos = False)\n","\n","# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\n","\"\"\"Adam for TensorFlow.\"\"\"\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","from tensorflow.python.eager import context\n","from tensorflow.python.framework import ops\n","from tensorflow.python.ops import control_flow_ops\n","from tensorflow.python.ops import math_ops\n","from tensorflow.python.ops import resource_variable_ops\n","from tensorflow.python.ops import state_ops\n","from tensorflow.python.ops import variable_scope\n","from tensorflow.python.training import optimizer\n","from tensorflow.python.training import training_ops\n","from tensorflow.python.ops.gradients import gradients\n","from tensorflow import zeros\n","\n","from tensorflow import placeholder\n","from tensorflow import placeholder\n","from tensorflow import subtract\n","from tensorflow import group\n","from tensorflow import control_dependencies\n","from tensorflow import no_op\n","from tensorflow import identity\n","import numpy as np\n","import tensorflow as tf\n","\n","\n","class SARAH_LBFGSOptimizer(optimizer.Optimizer):\n","\n","    GATE_NONE = 0\n","    GATE_OP = 1\n","    GATE_GRAPH = 2\n","\n","    def __init__(self,  learning_rate=0.001 ,\n","               alpha = 1.0,\n","               initial_hessian_type = 0, # 1 - learning_rate I\n","                                         # 0 - standard LBFGS\n","                                         # 2 - ADAM style\n","               lbfgs_history_size = 10,\n","               moving_average_parameter_for_hessian_type_1 = 0.99,\n","               diagonal_hessian_preconditioning_lr=0.001 ,\n","               use_locking=False, name=\"SARAH_LBFGS\"):\n","        super(SARAH_LBFGSOptimizer, self).__init__(use_locking, name)\n","\n","        self.fullGradientBatches = 1.0\n","        self._lr = learning_rate\n","        self.full_gradient_fraction = placeholder(1)\n","        # Tensor versions of the constructor arguments, created in _prepare().\n","        self._lr_t = None\n","        self.moving_average_parameter_for_hessian_type_1 = moving_average_parameter_for_hessian_type_1\n","        self.gamma_k = diagonal_hessian_preconditioning_lr\n","        self.initial_hessian_type = initial_hessian_type\n","        self.alpha = alpha\n","        self.lbfgs_history_size = lbfgs_history_size\n","        self.hs = lbfgs_history_size\n","        self.DP = np.zeros([2*lbfgs_history_size ,2*lbfgs_history_size ] )\n","        self.deltas = np.zeros((self.hs * 2, 1) , dtype='f')  # this will be the composition of final direction\n","        self.alphas = np.array([0.0 for x in range(self.hs)] , dtype='f')\n","\n","        self.current_ptr_s = -1\n","        self.current_ptr_y = self.hs-1\n","\n","        self.current_history_size = 0\n","        self.diagonal_hessian_preconditioning_lr = diagonal_hessian_preconditioning_lr\n","\n","\n","    def minimize(self, loss, global_step=None, var_list=None,\n","               gate_gradients=GATE_OP, aggregation_method=None,\n","               colocate_gradients_with_ops=False, name=None,\n","               grad_loss=None):\n","\n","        self.loss = loss\n","        grads_and_vars = self.compute_gradients(\n","            loss, var_list=var_list, gate_gradients=gate_gradients,\n","        aggregation_method=aggregation_method,\n","        colocate_gradients_with_ops=colocate_gradients_with_ops,\n","        grad_loss=grad_loss)\n","\n","        self.gradients = [g for g, v in grads_and_vars if g is not None]\n","        vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n","        if not vars_with_grad:\n","            raise ValueError(\n","          \"No gradients provided for any variable, check your graph for ops\"\n","          \" that do not support gradients, between variables %s and loss %s.\" %\n","          ([str(v) for _, v in grads_and_vars], loss))\n","\n","        return self._create_slots(vars_with_grad)\n","\n","    def compute_gradient_for_batch_before_updating_w(self):\n","        return self.computeNextGradient\n","\n","\n","    def restart(self):\n","        self.current_ptr_s = -1\n","        self.current_ptr_y = self.hs-1\n","        self.current_history_size = 0\n","\n","    def _create_slots(self, var_list):\n","        self.computeNextGradient = []\n","        self.nextGradient = []\n","        self.zeroV = []\n","        self.sarah_V = []\n","        self.assignBatchToV=[]\n","        batchFrac = 1/(0.0+self.fullGradientBatches)\n","        updateStep = []\n","        self.updateVt = []\n","        self.doSARAHStep=[]\n","\n","        for v, g in zip(var_list, self.gradients ):\n","            self._zeros_slot(v, \"p\", self._name)\n","            self._zeros_slot(v, \"s\", self._name)\n","            self._zeros_slot(v, \"y\", self._name)\n","            #self._zeros_slot(v,\"pi\",self._name)\n","            for i in range(self.lbfgs_history_size):\n","                self._zeros_slot(v, \"y_\"+str(i), self._name)\n","                self._zeros_slot(v, \"s_\"+str(i), self._name)\n","\n","\n","        #pOp = []\n","        self.grads = []\n","        for v, g in zip(var_list, self.gradients ):\n","            self._zeros_slot(v, \"sarah_v\", self._name)\n","            self._zeros_slot(v, \"next_grad\", self._name)\n","          #self._zeros_slot(v,\"pi\",self._name)\n","\n","            next_grad = self.get_slot(v, \"next_grad\")\n","            sarah_v = self.get_slot(v, \"sarah_v\")\n","          #Pk=self.get_slot(v,\"Pk\")\n","\n","            self.zeroV.append( sarah_v.assign( zeros(sarah_v.shape) ).op  )\n","            self.sarah_V.append(sarah_v)\n","\n","            self.computeNextGradient.append( next_grad.assign(g).op  )#2\n","            self.nextGradient.append( next_grad )\n","\n","            self.assignBatchToV.append( sarah_v.assign_add( self.full_gradient_fraction  *g  ).op  )# ?1\n","          #updateStep.append( v.assign_sub( self._lr  *  Pk).op   ) #3\n","            self.updateVt.append( sarah_v.assign_add(   g  -next_grad  ).op   )  #4\n","          #pi = self.get_slot(v, \"p\")\n","\n","\n","            self.grads.append(sarah_v)\n","          #pOp.append(  pi.assign(sarah_v).op   )\n","        #self.pOp = group(*pOp)\n","        # https://papers.nips.cc/paper/5333-large-scale-l-bfgs-using-mapreduce.pdf\n","        # We implement the \"vector-free\" LBFGS approach\n","        ss=[]\n","        ys=[]\n","\n","    #     pOp = []\n","    #     self.p = []\n","    #     for v, g in zip(var_list, self.grads ):\n","    #         pi = self.get_slot(v, \"p\")\n","    #         self.p.append(pi)\n","    #         pOp.append(  pi.assign(g).op   )\n","    #     self.pOp = group(*pOp)\n","\n","    #     with control_dependencies([self.pOp]):\n","        for i in range(self.lbfgs_history_size):\n","            dp_y = 0\n","            dp_s = 0\n","            for v, g  in zip(var_list, self.grads ):\n","                y = self.get_slot(v, \"y_\"+str(i))\n","                s = self.get_slot(v, \"s_\"+str(i))\n","                dp_y = dp_y + tf.reduce_sum(tf.multiply(g,y))\n","                dp_s = dp_s + tf.reduce_sum(tf.multiply(g,s))\n","            ys.append(dp_y)\n","            ss.append(dp_s)\n","        self.compute_new_dotproducts =  tf.stack(ss+ys)\n","\n","\n","        #-------------------------------------------------------------\n","        #   compute \"y, s\" part and decide if it should be included or not\n","        #with control_dependencies([self.pOp]):\n","        yisi = 0\n","        si2 = 0\n","        yi2 = 0\n","        compute_y = []\n","        for v, g in zip(var_list,self.gradients ):\n","            y = self.get_slot(v, \"y\")\n","            oldG = self.get_slot(v, \"next_grad\")\n","            s = self.get_slot(v, \"s\")\n","            yi = g - oldG\n","            compute_y.append( y.assign( yi ).op )\n","            yisi = yisi + tf.reduce_sum( yi* s )\n","            yi2 = yi2 + tf.reduce_sum( yi* yi )\n","            si2 = si2 + tf.reduce_sum( s* s )\n","        self.compute_y = [yisi,si2,yi2, group(*compute_y) ]\n","\n","\n","        # prepare stuctures to compute projection of new (s,y) pair to the history\n","        # and replacing history by new pair\n","        si_dot_s = [0 for i in range(self.hs)]\n","        yi_dot_s = [0 for i in range(self.hs)]\n","\n","        si_dot_y = [0 for i in range(self.hs)]\n","        yi_dot_y = [0 for i in range(self.hs)]\n","\n","        set_s_to_history = [ [] for i in range(self.hs)]\n","        set_y_to_history = [ [] for i in range(self.hs)]\n","\n","\n","        for v  in  var_list:\n","            y = self.get_slot(v, \"y\")\n","            s = self.get_slot(v, \"s\")\n","            for i in range(self.hs):\n","                yi = self.get_slot(v, \"y_\"+str(i))\n","                si = self.get_slot(v, \"s_\"+str(i))\n","                si_dot_s[i]+=tf.reduce_sum(si*s)\n","                si_dot_y[i]+=tf.reduce_sum(si*y)\n","                yi_dot_s[i]+=tf.reduce_sum(yi*s)\n","                yi_dot_y[i]+=tf.reduce_sum(yi*y)\n","\n","                set_s_to_history[i].append(   si.assign(s).op  )\n","                set_y_to_history[i].append(   yi.assign(y).op  )\n","\n","        self.set_s_to_history = set_s_to_history\n","        self.set_y_to_history = set_y_to_history\n","\n","        self.si_column =  si_dot_s + si_dot_y\n","        self.yi_column =  yi_dot_s + yi_dot_y\n","\n","        #-------------------------------------------------------------\n","        #   Compute update and update displacement\n","        self.delta_p_PH = tf.placeholder(tf.float32)\n","        self.deltas_PH = tf.placeholder(tf.float32)\n","\n","        compute_update=[]\n","        pvs = []\n","        for v in var_list:\n","            pi = self.get_slot(v, \"sarah_v\")\n","            pv = pi * self.delta_p_PH\n","            for i in range(self.lbfgs_history_size):\n","                y = self.get_slot(v, \"y_\"+str(i))\n","                s = self.get_slot(v, \"s_\"+str(i))\n","                pv = pv+ self.deltas_PH[i]*s\n","                pv = pv+ self.deltas_PH[i+self.hs]*y\n","            pvs.append(pv)\n","        #         compute_update.append(p.assign(pv).op)\n","        #     self.compute_update = group(*compute_update)\n","        perform_step = []\n","        for v, pvi in zip(var_list, pvs ):\n","            #updateStep.append( v.assign_sub( self._lr  *  Pk).op   )\n","            perform_step.append( v.assign_add( self._lr  *  pvi).op   )\n","            s = self.get_slot(v, \"s\")\n","            perform_step.append( s.assign(self._lr*pvi).op   )  #  s = w_{k+1} - w_k\n","        self.perform_step = group(*perform_step)\n","\n","        undo_step = []\n","        for v  in  var_list :\n","            s = self.get_slot(v, \"s\")\n","            undo_step.append( v.assign_sub(s).op   )\n","        self.undo_step = group(*undo_step)\n","\n","\n","\n","        return group(*perform_step)\n","\n","\n","    def set_sarah_v_to_zero(self):\n","        return self.zeroV\n","    def update_sarah_v(self):\n","        return self.updateVt\n","\n","    def add_batch_gradient_to_sarah_v(self):\n","        return self.assignBatchToV\n","\n","\n","    def update_LBFGS_history(self, sess,siyi, si2, yi2):\n","    # first we compute projection of \"s\" and \"y\" into the history\n","        si, yi = sess.run([self.si_column,self.yi_column])\n","\n","        self.current_ptr_s += 1\n","        if self.current_ptr_s == self.hs:\n","            self.current_ptr_s = 0\n","        self.current_ptr_y += 1\n","        if self.current_ptr_y == 2*self.hs:\n","            self.current_ptr_y = self.hs\n","        self.current_history_size+=1\n","        if self.current_history_size > self.hs:\n","            self.current_history_size = self.hs\n","        for i in range(self.hs * 2):\n","            self.DP[ (i, self.current_ptr_s)  ] = si[i]\n","            self.DP[ (i, self.current_ptr_y)  ] = yi[i]\n","            self.DP[ (self.current_ptr_s, i)  ] = si[i]\n","            self.DP[ (self.current_ptr_y, i)  ] = yi[i]\n","\n","        self.DP[ (self.current_ptr_s, self.current_ptr_s)  ] = si2\n","        self.DP[ (self.current_ptr_y, self.current_ptr_y)  ] = yi2\n","        self.DP[ (self.current_ptr_s, self.current_ptr_y)  ] = siyi\n","        self.DP[ (self.current_ptr_y, self.current_ptr_s)  ] = siyi\n","\n","        sess.run([self.set_s_to_history[self.current_ptr_s],self.set_y_to_history[self.current_ptr_s]])\n","\n","\n","    def run_two_loop_recursion(self,dot_products):\n","\n","        deltaP = -1\n","        self.deltas = self.deltas * 0\n","        # make a two loop recursion\n","        if self.current_history_size > 0:\n","\n","\n","#             if self.initial_hessian_type == 0 or self.initial_hessian_type == 1:\n","#                 rho = self.DP[ (self.current_ptr_s, self.current_ptr_y) ] / self.DP[ (self.current_ptr_y, self.current_ptr_y) ]\n","#                 if self.initial_hessian_type == 1:\n","#                     self.gamma_k = self.moving_average_parameter_for_hessian_type_1 * self.gamma_k \\\n","#                                   + (1-self.moving_average_parameter_for_hessian_type_1)* rho\n","#                     rho = self.gamma_k\n","# #                 for tmp in range(2 * self.hs):\n","# #                     self.deltas[tmp] = self.deltas[tmp] * rho\n","#                 deltaP = deltaP * rho\n","\n","\n","\n","            for tmp in range(self.current_history_size):\n","                si = self.current_ptr_s - tmp\n","                yi = self.current_ptr_y - tmp\n","                if si < 0:\n","                    si = si + self.hs\n","                if yi < self.hs:\n","                    yi = yi + self.hs\n","\n","                self.alphas[tmp] = deltaP * dot_products[si]\n","                for i in range(2 * self.hs):\n","                    self.alphas[tmp] = self.alphas[tmp] + self.deltas[i] * self.DP[(i, si)]\n","                self.alphas[tmp] = self.alphas[tmp] / self.DP[ (si, yi)  ] #error here, nan\n","                self.deltas[yi] = self.deltas[yi] - self.alphas[tmp]\n","\n","\n","            if self.initial_hessian_type == 0 or self.initial_hessian_type == 1:\n","                rho = self.DP[ (self.current_ptr_s, self.current_ptr_y) ] / self.DP[ (self.current_ptr_y, self.current_ptr_y) ]\n","                if self.initial_hessian_type == 1:\n","                    self.gamma_k = self.moving_average_parameter_for_hessian_type_1 * self.gamma_k \\\n","                                  + (1-self.moving_average_parameter_for_hessian_type_1)* rho\n","                    rho = self.gamma_k\n","                for tmp in range(2 * self.hs):\n","                    self.deltas[tmp] = self.deltas[tmp] * rho\n","                deltaP = deltaP * rho\n","\n","\n","            for tmpR in range(self.current_history_size):\n","                tmp = self.current_history_size - 1 - tmpR\n","                si = self.current_ptr_s - tmp\n","                yi = self.current_ptr_y - tmp\n","                if si < 0:\n","                    si = si + self.hs\n","                if yi < self.hs:\n","                    yi = yi + self.hs\n","\n","                beta = deltaP * dot_products[yi]\n","                for i in range(2 * self.hs):\n","                    beta = beta + self.deltas[i] * self.DP[(i, yi)]\n","                beta = beta / self.DP[ (si, yi)  ]\n","                self.deltas[si] = self.deltas[si] + self.alphas[tmp] - beta\n","\n","        self.deltaP = deltaP*self.alpha\n","        self.deltas = self.deltas*self.alpha\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","\n","    model = create_model(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","    target = None\n","    policy = 'EpsGreedyQPolicy'\n","\n","    agent = Agent(model = model, target = target, memory_size = -1,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","    #print(timeit.timeit(\"agent.train(game, batch_size = 64, sess = sess, nb_epoch = 100, gamma = 0.95, policy = policy)\", setup=\"from __main__ import agent\"))\n","    agent.train(game, batch_size = 64, sess = sess, nb_epoch = 10000, gamma = 0.95, policy = \"EpsGreedyQPolicy\")\n","    \n","    #agent.train(game, batch_size = 64, sess = sess, nb_epoch = 100, gamma = 0.95, policy = \"GreedyQPolicy\")\n","\n","\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i6ErN45tfF9u"},"source":["GitHub - flyyufelix/VizDoom-Keras-RL: Reinforcement Learning in Keras on VizDoom"]},{"cell_type":"code","metadata":{"id":"A4AI3xe0ZTpf","executionInfo":{"status":"aborted","timestamp":1612963375272,"user_tz":180,"elapsed":12307,"user":{"displayName":"Victor Neves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEUEU1qs3bVzAx-wDU62hieaFOeUqZzw1x2AZ9EWE=s64","userId":"17687780415293205160"}}},"source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI.\n","\"\"\"\n","from __future__ import print_function\n","\n","import sys  # To close the window when the game is over\n","from array import array  # Efficient numeric arrays\n","from os import environ, path  # To center the game window the best possible\n","import random  # Random numbers used for the food\n","import logging  # Logging function for movements and errors\n","from itertools import tee  # For the color gradient on snake\n","\n","import numpy as np # Used in calculations and math\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","OPTIONS = {'QUIT': 0,\n","           'PLAY': 1,\n","           'BENCHMARK': 2,\n","           'LEADERBOARDS': 3,\n","           'MENU': 4,\n","           'ADD_LEADERBOARDS': 5}\n","RELATIVE_ACTIONS = {'LEFT': 0,\n","                    'FORWARD': 1,\n","                    'RIGHT': 2}\n","ABSOLUTE_ACTIONS = {'LEFT': 0,\n","                    'RIGHT': 1,\n","                    'UP': 2,\n","                    'DOWN': 3,\n","                    'IDLE': 4}\n","FORBIDDEN_MOVES = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","REWARDS = {'MOVE': -0.005,\n","           'GAME_OVER': -1,\n","           'SCORED': 1}\n","\n","# Types of point in the board\n","POINT_TYPE = {'EMPTY': 0,\n","              'FOOD': 1,\n","              'BODY': 2,\n","              'HEAD': 3,\n","              'DANGEROUS': 4}\n","\n","# Speed levels possible to human players. MEGA HARDCORE starts with MEDIUM and\n","# increases with snake size\n","LEVELS = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","SPEEDS = {'EASY': 80,\n","          'MEDIUM': 60,\n","          'HARD': 40,\n","          'MEGA_HARDCORE': 65}\n","\n","# Set the constant FPS limit for the game. Smoothness depend on this.\n","GAME_FPS = 100\n","\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    block_size: int, optional, default = 20\n","        The size in pixels of a block.\n","    head_color: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    tail_color: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    food_color: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    game_speed: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    benchmark: int, optional, default = 10\n","        Ammount of matches to benchmark and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, board_size = 30, block_size = 20,\n","                 head_color = (42, 42, 42), tail_color = (152, 152, 152),\n","                 food_color = (200, 0, 0), game_speed = 80, benchmark = 10):\n","        \"\"\"Initialize all global variables. Updated with argument_handler.\"\"\"\n","        self.board_size = board_size\n","        self.block_size = block_size\n","        self.head_color = head_color\n","        self.tail_color = tail_color\n","        self.food_color = food_color\n","        self.game_speed = game_speed\n","        self.benchmark = benchmark\n","\n","        if self.board_size > 50: # Warn the user about performance\n","            LOGGER.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","    @property\n","    def canvas_size(self):\n","        \"\"\"Canvas size is updated with board_size and block_size.\"\"\"\n","        return self.board_size * self.block_size\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), block_type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.block_type = block_type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((VAR.canvas_size) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.block_type == \"menu\" and not self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.block_type == \"menu\" and self.hovered:\n","            color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [board_size / 4, board_size / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(VAR.board_size / 4), int(VAR.board_size / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def is_movement_invalid(self, action):\n","        \"\"\"Check if the movement is invalid, according to FORBIDDEN_MOVES.\"\"\"\n","        valid = False\n","\n","        if (action, self.previous_action) in FORBIDDEN_MOVES:\n","            valid = True\n","\n","        return valid\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if (action == ABSOLUTE_ACTIONS['IDLE'] or\n","            self.is_movement_invalid(action)):\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == ABSOLUTE_ACTIONS['LEFT']:\n","            self.head[0] -= 1\n","        elif action == ABSOLUTE_ACTIONS['RIGHT']:\n","            self.head[0] += 1\n","        elif action == ABSOLUTE_ACTIONS['UP']:\n","            self.head[1] -= 1\n","        elif action == ABSOLUTE_ACTIONS['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            LOGGER.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((VAR.board_size - 1) * random.random()),\n","                        int((VAR.board_size - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            LOGGER.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        VAR.board_size = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with board_size * block_size dimension.\"\"\"\n","        pygame.init()\n","        flags = pygame.DOUBLEBUF | pygame.HWSURFACE\n","        self.window = pygame.display.set_mode((VAR.canvas_size, VAR.canvas_size),\n","                                              flags)\n","        self.window.set_alpha(None)\n","\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def cycle_menu(self, menu_options, list_menu, dictionary, img = None,\n","                   img_rect = None):\n","        \"\"\"Cycle through a given menu, waiting for an option to be clicked.\"\"\"\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            events = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for i, option in enumerate(menu_options):\n","                if option is not None:\n","                    option.draw()\n","                    option.hovered = False\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        for event in events:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = dictionary[list_menu[i]]\n","\n","            if selected_option is not None:\n","                selected = True\n","            if img is not None:\n","                self.window.blit(img, img_rect.bottomleft)\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def cycle_matches(self, n_matches, mega_hardcore = False):\n","        \"\"\"Cycle through matches until the end.\"\"\"\n","        score = array('i')\n","\n","        for _ in range(n_matches):\n","            self.reset_game()\n","            self.start_match(wait = 3)\n","            score.append(self.single_player(mega_hardcore))\n","\n","        return score\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images\" +\n","                                              \"/snake_logo.png\")).convert()\n","        img = pygame.transform.scale(img, (VAR.canvas_size,\n","                                           int(VAR.canvas_size / 3)))\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","        list_menu = ['PLAY', 'BENCHMARK', 'LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY GAME ',\n","                                  (self.screen_rect.centerx,\n","                                   4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ',\n","                                  (self.screen_rect.centerx,\n","                                   6 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ',\n","                                  (self.screen_rect.centerx,\n","                                   8 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ',\n","                                  (self.screen_rect.centerx,\n","                                   10 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\")]\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS,\n","                                          img, img_rect)\n","\n","        return selected_option\n","\n","    def start_match(self, wait):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(wait):\n","            time = str(wait - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in',\n","                              (self.screen_rect.centerx,\n","                               4 * self.screen_rect.centery / 10),\n","                              self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                     12 * self.screen_rect.centery / 10),\n","                              self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","            pygame.time.wait(1000)\n","\n","        LOGGER.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == OPTIONS['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == OPTIONS['PLAY']:\n","                VAR.game_speed, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = 1,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == OPTIONS['BENCHMARK']:\n","                VAR.game_speed, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = VAR.benchmark,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == OPTIONS['LEADERBOARDS']:\n","                pass\n","            elif opt == OPTIONS['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == OPTIONS['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        score_option = None\n","\n","        if len(score) == VAR.benchmark:\n","            score_option = TextBlock(' ADD TO LEADERBOARDS ',\n","                                     (self.screen_rect.centerx,\n","                                      8 * self.screen_rect.centery / 10),\n","                                     self.window, (1 / 15), \"menu\")\n","\n","        text_score = 'SCORE: ' + str(int(np.mean(score)))\n","        list_menu = ['PLAY', 'MENU', 'ADD_LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                   4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                   6 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        score_option,\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        TextBlock(text_score, (self.screen_rect.centerx,\n","                                               15 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"text\")]\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        LOGGER.info('EVENT: GAME OVER | FINAL %s', text_score)\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS)\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        list_menu = ['EASY', 'MEDIUM', 'HARD', 'MEGA_HARDCORE']\n","        menu_options = [TextBlock(LEVELS[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\")]\n","\n","        speed = self.cycle_menu(menu_options, list_menu, SPEEDS)\n","        mega_hardcore = False\n","\n","        if speed == SPEEDS['MEGA_HARDCORE']:\n","            mega_hardcore = True\n","\n","        return speed, mega_hardcore\n","\n","    def single_player(self, mega_hardcore = False):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                   previous_size)\n","\n","        # Main loop, where snakes moves after elapsed time is bigger than the\n","        # move_wait time. The last_key pressed is recorded to make the game more\n","        # smooth for human players.\n","        elapsed = 0\n","        last_key = self.snake.previous_action\n","        move_wait = VAR.game_speed\n","\n","        while not self.game_over:\n","            elapsed += self.fps.get_time()  # Get elapsed time since last call.\n","\n","            if mega_hardcore:  # Progressive speed increments, the hardest.\n","                move_wait = VAR.game_speed - (2 * (self.snake.length - 3))\n","\n","            key_input = self.handle_input()  # Receive inputs with tick.\n","            invalid_key = self.snake.is_movement_invalid(key_input)\n","\n","            if key_input is not None and not invalid_key:\n","                last_key = key_input\n","\n","            if elapsed >= move_wait:  # Move and redraw\n","                elapsed = 0\n","                self.play(last_key)\n","                current_size = self.snake.length  # Update the body size\n","\n","                if current_size > previous_size:\n","                    color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                               current_size)\n","\n","                    previous_size = current_size\n","\n","                self.draw(color_list)\n","\n","            pygame.display.update()\n","            self.fps.tick(GAME_FPS)  # Limit FPS to 100\n","\n","        score = current_size - 3  # After the game is over, record score\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (VAR.board_size - 1) or self.snake.head[0] < 0:\n","            LOGGER.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (VAR.board_size - 1) or self.snake.head[1] < 0:\n","            LOGGER.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            LOGGER.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = None\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            LOGGER.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            LOGGER.info('ACTION: KEY PRESSED: LEFT')\n","            action = ABSOLUTE_ACTIONS['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            LOGGER.info('ACTION: KEY PRESSED: RIGHT')\n","            action = ABSOLUTE_ACTIONS['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            LOGGER.info('ACTION: KEY PRESSED: UP')\n","            action = ABSOLUTE_ACTIONS['UP']\n","        elif keys[pygame.K_DOWN]:\n","            LOGGER.info('ACTION: KEY PRESSED: DOWN')\n","            action = ABSOLUTE_ACTIONS['DOWN']\n","\n","        return action\n","\n","    @staticmethod\n","    def eval_local_safety(canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size board_size**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if ((body[0][0] + 1) > (VAR.board_size - 1)\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]):\n","            canvas[VAR.board_size - 1, 0] = POINT_TYPE['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[VAR.board_size - 1, 1] = POINT_TYPE['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[VAR.board_size - 1, 2] = POINT_TYPE['DANGEROUS']\n","        if ((body[0][1] + 1) > (VAR.board_size - 1)\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]):\n","            canvas[VAR.board_size - 1, 3] = POINT_TYPE['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size board_size**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((VAR.board_size, VAR.board_size))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = POINT_TYPE['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = POINT_TYPE['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = POINT_TYPE['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == RELATIVE_ACTIONS['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == RELATIVE_ACTIONS['LEFT']:\n","            if self.snake.previous_action == ABSOLUTE_ACTIONS['LEFT']:\n","                action = ABSOLUTE_ACTIONS['DOWN']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['RIGHT']:\n","                action = ABSOLUTE_ACTIONS['UP']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['UP']:\n","                action = ABSOLUTE_ACTIONS['LEFT']\n","            else:\n","                action = ABSOLUTE_ACTIONS['RIGHT']\n","        else:\n","            if self.snake.previous_action == ABSOLUTE_ACTIONS['LEFT']:\n","                action = ABSOLUTE_ACTIONS['UP']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['RIGHT']:\n","                action = ABSOLUTE_ACTIONS['DOWN']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['UP']:\n","                action = ABSOLUTE_ACTIONS['RIGHT']\n","            else:\n","                action = ABSOLUTE_ACTIONS['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                self.game_over = True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current reward. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = REWARDS['MOVE']\n","\n","        if self.game_over:\n","            reward = REWARDS['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    @staticmethod\n","    def gradient(colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for substep in range(1, substeps):\n","                yield tuple([(start[component]\n","                              + (float(substep) / (substeps - 1))\n","                              * (finish[component] - start[component]))\n","                             for component in range(components)])\n","\n","        def pairs(seq):\n","            first_color, second_color = tee(seq)\n","            next(second_color, None)\n","\n","            return zip(first_color, second_color)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for first_color, second_color in pairs(colors):\n","            for gradient_color in linear_gradient(first_color, second_color,\n","                                                  substeps):\n","                result.append(gradient_color)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect((part[0] *\n","                        VAR.block_size), part[1] * VAR.block_size,\n","                        VAR.block_size, VAR.block_size))\n","\n","        pygame.draw.rect(self.window, VAR.food_color,\n","                         pygame.Rect(self.food_pos[0] * VAR.block_size,\n","                         self.food_pos[1] * VAR.block_size, VAR.block_size,\n","                         VAR.block_size))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                   + str(self.snake.length - 3))\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","VAR = GlobalVariables() # Initializing GlobalVariables\n","LOGGER = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","import skimage as skimage\n","from skimage import transform, color, exposure\n","from skimage.viewer import ImageViewer\n","import random\n","from random import choice\n","import numpy as np\n","from collections import deque\n","import time\n","\n","import json\n","from tensorflow.keras.models import model_from_json\n","from tensorflow.keras.models import Sequential, load_model, Model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n","from tensorflow.keras import backend as K\n","\n","import tensorflow as tf\n","#tf.python.control_flow_ops = tf\n","\n","class Networks(object):\n","\n","    @staticmethod\n","    def actor_network(input_shape, action_size, learning_rate):\n","        \"\"\"Actor Network for A2C\n","        \"\"\"\n","\n","        model = Sequential()\n","        model.add(Conv2D(32, (4, 4), input_shape=(input_shape)))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Conv2D(64, (2, 2)))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Conv2D(64, (2, 2)))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Flatten())\n","        model.add(Dense(3136))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dense(64))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dense(action_size, activation = 'softmax'))\n","\n","        adam = Adam(lr=learning_rate)\n","        model.compile(loss='categorical_crossentropy',optimizer=adam)\n","\n","        return model\n","\n","    @staticmethod\n","    def critic_network(input_shape, value_size, learning_rate):\n","        \"\"\"Critic Network for A2C\n","        \"\"\"\n","\n","        model = Sequential()\n","        model.add(Conv2D(32, (4, 4), input_shape=(input_shape)))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Conv2D(64, (2, 2)))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Conv2D(64, (2, 2)))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Flatten())\n","        model.add(Dense(3136))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dense(64))\n","        model.add(BatchNormalization())\n","        model.add(Activation('relu'))\n","        model.add(Dense(value_size, activation='linear'))\n","\n","        adam = Adam(lr=learning_rate)\n","        model.compile(loss='mse',optimizer=adam)\n","\n","        return model\n","\n","\n","#!/usr/bin/env python\n","\n","import skimage as skimage\n","from skimage import transform, color, exposure\n","from skimage.viewer import ImageViewer\n","import random\n","from random import choice\n","import numpy as np\n","from collections import deque\n","import time\n","\n","import json\n","from tensorflow.keras.models import model_from_json\n","from tensorflow.keras.models import Sequential, load_model, Model\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n","\n","import itertools as it\n","from time import sleep\n","import tensorflow as tf\n","\n","\n","def preprocessImg(img, size):\n","\n","    img = np.rollaxis(img, 0, 3) # It becomes (640, 480, 3)\n","    img = skimage.transform.resize(img, size)\n","    img = skimage.color.rgb2gray(img)\n","\n","    return img\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)  \n","  \n","class A2CAgent:\n","\n","    def __init__(self, state_size, action_size):\n","        # get size of state and action\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.value_size = 1\n","        self.observe = 0\n","        self.frame_per_action = 4\n","        self.schedule = LinearSchedule(10000, 0.01, 1)\n","\n","        # These are hyper parameters for the Policy Gradient\n","        self.gamma = 0.99\n","        self.actor_lr = 0.0001\n","        self.critic_lr = 0.0001\n","\n","        # Model for policy and critic network\n","        self.actor = None\n","        self.critic = None\n","\n","        # lists for the states, actions and rewards\n","        self.states, self.actions, self.rewards = [], [], []\n","\n","        # Performance Statistics\n","        self.stats_window_size= 50 # window size for computing rolling statistics\n","        self.mavg_score = [] # Moving Average of Survival Time\n","        self.var_score = [] # Variance of Survival Time\n","        self.mavg_ammo_left = [] # Moving Average of Ammo used\n","        self.mavg_kill_counts = [] # Moving Average of Kill Counts        \n","        \n","    # using the output of policy network, pick action stochastically (Stochastic Policy)\n","    def get_action(self, state, epoch):\n","        policy = self.actor.predict(state).flatten()\n","        return np.random.choice(self.action_size, 1, p=policy)[0], policy\n","\n","    # Instead agent uses sample returns for evaluating policy\n","    # Use TD(1) i.e. Monte Carlo updates\n","    def discount_rewards(self, rewards):\n","        discounted_rewards = np.zeros_like(rewards)\n","        running_add = 0\n","        for t in reversed(range(0, len(rewards))):\n","            if rewards[t] != 0:\n","                running_add = 0\n","            running_add = running_add * self.gamma + rewards[t]\n","            discounted_rewards[t] = running_add\n","        return discounted_rewards\n","\n","    # save <s, a ,r> of each step\n","    def append_sample(self, state, action, reward):\n","        self.states.append(state)\n","        self.rewards.append(reward)\n","        self.actions.append(action)\n","\n","    # update policy network every episode\n","    def train_model(self):\n","        episode_length = len(self.states)\n","\n","        discounted_rewards = self.discount_rewards(self.rewards)\n","\n","        # Standardized discounted rewards\n","        discounted_rewards -= np.mean(discounted_rewards)\n","\n","        if np.std(discounted_rewards):\n","            discounted_rewards /= np.std(discounted_rewards)\n","        else:\n","            self.states, self.actions, self.rewards = [], [], []\n","            print ('std = 0!')\n","            return 0\n","\n","        update_inputs = np.zeros(((episode_length,) + self.state_size)) # Episode_lengthx64x64x4\n","\n","        # Episode length is like the minibatch size in DQN\n","        for i in range(episode_length):\n","            update_inputs[i,:,:,:] = self.states[i]\n","\n","        # Prediction of state values for each state appears in the episode\n","        values = self.critic.predict(update_inputs)\n","\n","        # Similar to one-hot target but the \"1\" is replaced by Advantage Function i.e. discounted_rewards R_t - Value\n","        advantages = np.zeros((episode_length, self.action_size))\n","\n","        for i in range(episode_length):\n","            advantages[i][self.actions[i]] = discounted_rewards[i] - values[i]\n","\n","        actor_loss = self.actor.fit(update_inputs, advantages, epochs=1, verbose=0)\n","        critic_loss = self.critic.fit(update_inputs, discounted_rewards, epochs=1, verbose=0)\n","\n","        self.states, self.actions, self.rewards = [], [], []\n","\n","        return actor_loss.history['loss'], critic_loss.history['loss']\n","\n","\n","    def save_model(self, name):\n","        self.actor.save_weights(name + \"_actor.h5\", overwrite=True)\n","        self.critic.save_weights(name + \"_critic.h5\", overwrite=True)\n","\n","    def load_model(self, name):\n","        self.actor.load_weights(name + \"_actor.h5\", overwrite=True)\n","        self.critic.load_weights(name + \"_critic.h5\", overwrite=True)\n","\n","if __name__ == \"__main__\":\n","\n","    tf.keras.backend.set_image_data_format('channels_last')\n","    # Avoid Tensorflow eats up GPU memory\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","    tf.keras.backend.set_session(sess)\n","\n","    game = Game(player = \"ROBOT\", board_size = 10,\n","                local_state = True, relative_pos = False)\n","\n","    # Maximum number of episodes\n","    max_episodes = 10000\n","\n","    game.reset_game()\n","    game_state = game.state()\n","\n","    action_size = 5\n","\n","    img_rows , img_cols = 10, 10\n","    # Convert image into Black and white\n","    img_channels = 4 # We stack 4 frames\n","\n","    state_size = (img_rows, img_cols, img_channels)\n","    agent = A2CAgent(state_size, action_size)\n","    agent.actor = Networks.actor_network(state_size, action_size, agent.actor_lr)\n","    agent.critic = Networks.critic_network(state_size, agent.value_size, agent.critic_lr)\n","\n","    # Start training\n","    GAME = 0\n","    t = 0\n","\n","    # Buffer to compute rolling statistics\n","    size, step = [], []\n","\n","    for i in range(max_episodes):\n","\n","        game.reset_game()\n","        game_state = game.state()\n","\n","        s_t = np.stack(([game_state]*4), axis=2) # It becomes 64x64x4\n","        s_t = np.expand_dims(s_t, axis=0) # 1x64x64x\n","        while not game.game_over:\n","\n","            loss = 0 # Training Loss at each update\n","            r_t = 0 # Initialize reward at time t\n","            a_t = np.zeros([action_size]) # Initialize action at time t\n","\n","            x_t = np.reshape(game_state, (1, img_rows, img_cols, 1))\n","            s_t = np.append(x_t, s_t[:, :, :, :3], axis=3)\n","\n","            # Sample action from stochastic softmax policy\n","            action_idx, policy  = agent.get_action(s_t, i)\n","            a_t[action_idx] = 1\n","\n","            a_t = a_t.astype(int)\n","            game.play(np.argmax(a_t))\n","\n","            r_t = game.get_reward()  # Each frame we get reward of 0.1, so 4 frames will be 0.4\n","            # Check if episode is terminated\n","            is_terminated = game.game_over\n","\n","            if (is_terminated):\n","                size.append(game.snake.length)\n","                step.append(game.step)\n","                #if i%50 == 0:\n","                    #print (\"Episode Finish \", policy)\n","            else:\n","                game_state = game.state()  # Observe again after we take the action\n","\n","            # Save trajactory sample <s, a, r> to the memory\n","            agent.append_sample(s_t, action_idx, r_t)\n","\n","            # Update the cache\n","            t += 1\n","\n","            if (is_terminated and t > agent.observe):\n","                # Every episode, agent learns from sample returns\n","                loss = agent.train_model()\n","\n","            # Save model every 10000 iterations\n","            #if t % 10000 == 0:\n","            #    print(\"Save model\")\n","            #    agent.save_model(\"models/a2c\")\n","\n","            state = \"\"\n","            if t <= agent.observe:\n","                state = \"Observe mode\"\n","            else:\n","                state = \"Train mode\"\n","\n","            if (is_terminated):\n","                if i%50 == 0:\n","                    # Print performance statistics at every episode end\n","                    print(\"TIME\", t, \"/ GAME\", GAME, \"/ STATE\", state, \"/ SIZE\", np.mean(size[-100:]), \"/ STEP\", np.mean(step[-100:]))\n","\n","                # Save Agent's Performance Statistics\n","                if GAME % agent.stats_window_size == 0 and t > agent.observe:\n","                    # print(\"Update Rolling Statistics\")\n","                    agent.mavg_score.append(np.mean(np.array(size)))\n","                    agent.var_score.append(np.var(np.array(step)))\n","\n","        # Episode Finish. Increment game count\n","        GAME += 1\n"],"execution_count":null,"outputs":[]}]}