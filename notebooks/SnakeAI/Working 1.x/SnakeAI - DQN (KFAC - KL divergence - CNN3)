{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"SnakeAI - DQN (KFAC - KL divergence - CNN3)","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"J7tURWxCmJHG"},"source":["SnakeAI\n","================="]},{"cell_type":"markdown","metadata":{"id":"fRsGcv_InURF"},"source":["### A snake game with human and AI players (DQN, ACER). Who wins? ðŸŽ±\n","\n","This is the notebook for the repository [SnakeAI](https://github.com/Neves4/SnakeAI), in which you could test the execution with GPU/CPU in Keras for a DQN model playing the snake game. \n","\n","Let's begin!"]},{"cell_type":"markdown","metadata":{"id":"QCus5J_6muj5"},"source":["Table of contents\n","=================\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zkdghiysnQwn"},"source":["[1. The game](#the-game)"]},{"cell_type":"markdown","metadata":{"id":"l3kYGkQ0ncwi"},"source":["## 1. The game <a name=\"the-game\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"XrkKfTsopKc1"},"source":["### 1.1 Imports <a name=\"the-game-imports\"></a>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MrlX6WDSpNAQ","executionInfo":{"status":"ok","timestamp":1614171089675,"user_tz":180,"elapsed":10726,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}},"outputId":"f76433d9-535e-4c16-e5ba-8b5c18fad50c"},"source":["import sys  # To close the window when the game is over\n","from array import array  # Efficient numeric arrays\n","from os import environ, path  # To center the game window the best possible\n","import random  # Random numbers used for the food\n","import logging  # Logging function for movements and errors\n","import json # For file handling (leaderboards)\n","from itertools import tee  # For the color gradient on snake\n","\n","import numpy as np # Used in calculations and math\n","\n","%tensorflow_version 1.x\n","\n","!pip install kfac\n","import kfac"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Requirement already satisfied: kfac in /tensorflow-1.15.2/python3.7 (0.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from kfac) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from kfac) (1.15.0)\n","Requirement already satisfied: tensorflow-probability in /tensorflow-1.15.2/python3.7 (from kfac) (0.7.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->kfac) (4.4.2)\n","Requirement already satisfied: cloudpickle>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability->kfac) (1.3.0)\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/kfac/python/ops/utils.py:38: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/kfac/python/ops/optimizer.py:48: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/kfac/python/ops/optimizer.py:526: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QME8kfJ1pa-k"},"source":["### 1.2 Global variables <a name=\"the-game-global-var\"></a>"]},{"cell_type":"code","metadata":{"id":"EvYx0zAupYcB","executionInfo":{"status":"ok","timestamp":1614171090230,"user_tz":180,"elapsed":11262,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["# Actions, options and forbidden moves\n","OPTIONS = {'QUIT': 0,\n","           'PLAY': 1,\n","           'BENCHMARK': 2,\n","           'LEADERBOARDS': 3,\n","           'MENU': 4,\n","           'ADD_TO_LEADERBOARDS': 5}\n","RELATIVE_ACTIONS = {'LEFT': 0,\n","                    'FORWARD': 1,\n","                    'RIGHT': 2}\n","ABSOLUTE_ACTIONS = {'LEFT': 0,\n","                    'RIGHT': 1,\n","                    'UP': 2,\n","                    'DOWN': 3,\n","                    'IDLE': 4}\n","FORBIDDEN_MOVES = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","REWARDS = {'MOVE': -0.005,\n","           'GAME_OVER': -1,\n","           'SCORED': 1}\n","\n","# Types of point in the board\n","POINT_TYPE = {'EMPTY': 0,\n","              'FOOD': 1,\n","              'BODY': 2,\n","              'HEAD': 3,\n","              'DANGEROUS': 4}\n","\n","# Speed levels possible to human players. MEGA HARDCORE starts with MEDIUM and\n","# increases with snake size\n","LEVELS = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","SPEEDS = {'EASY': 80,\n","          'MEDIUM': 60,\n","          'HARD': 40,\n","          'MEGA_HARDCORE': 65}\n","\n","# Set the constant FPS limit for the game. Smoothness depend on this.\n","GAME_FPS = 100\n","\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    block_size: int, optional, default = 20\n","        The size in pixels of a block.\n","    head_color: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    tail_color: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    food_color: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    game_speed: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    benchmark: int, optional, default = 10\n","        Ammount of matches to benchmark and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, board_size = 30, block_size = 20,\n","                 head_color = (42, 42, 42), tail_color = (152, 152, 152),\n","                 food_color = (200, 0, 0), game_speed = 80, benchmark = 10):\n","        \"\"\"Initialize all global variables. Updated with argument_handler.\"\"\"\n","        self.board_size = board_size\n","        self.block_size = block_size\n","        self.head_color = head_color\n","        self.tail_color = tail_color\n","        self.food_color = food_color\n","        self.game_speed = game_speed\n","        self.benchmark = benchmark\n","\n","        if self.board_size > 50: # Warn the user about performance\n","            LOGGER.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","    @property\n","    def canvas_size(self):\n","        \"\"\"Canvas size is updated with board_size and block_size.\"\"\"\n","        return self.board_size * self.block_size\n","    \n","VAR = GlobalVariables() # Initializing GlobalVariables\n","LOGGER = logging.getLogger(__name__) # Setting logger    "],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6nmwGMWtqBpV"},"source":["### 1.3 The snake <a name=\"the-game-the-snake\"></a>"]},{"cell_type":"code","metadata":{"id":"UFlv3bhurX4i","executionInfo":{"status":"ok","timestamp":1614171090234,"user_tz":180,"elapsed":11256,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [board_size / 4, board_size / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(VAR.board_size / 4), int(VAR.board_size / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def is_movement_invalid(self, action):\n","        \"\"\"Check if the movement is invalid, according to FORBIDDEN_MOVES.\"\"\"\n","        valid = False\n","\n","        if (action, self.previous_action) in FORBIDDEN_MOVES:\n","            valid = True\n","\n","        return valid\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if (action == ABSOLUTE_ACTIONS['IDLE'] or\n","            self.is_movement_invalid(action)):\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == ABSOLUTE_ACTIONS['LEFT']:\n","            self.head[0] -= 1\n","        elif action == ABSOLUTE_ACTIONS['RIGHT']:\n","            self.head[0] += 1\n","        elif action == ABSOLUTE_ACTIONS['UP']:\n","            self.head[1] -= 1\n","        elif action == ABSOLUTE_ACTIONS['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            LOGGER.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"okHW9gUdrYzb"},"source":["### 1.4 The game <a name=\"the-game-the-game\"></a>"]},{"cell_type":"code","metadata":{"id":"9buEp2SJnu8y","executionInfo":{"status":"ok","timestamp":1614171092037,"user_tz":180,"elapsed":13052,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), block_type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.block_type = block_type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((VAR.canvas_size) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.block_type == \"menu\" and not self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.block_type == \"menu\" and self.hovered:\n","            color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","        \n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((VAR.board_size - 1) * random.random()),\n","                        int((VAR.board_size - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            LOGGER.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        VAR.board_size = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with board_size * block_size dimension.\"\"\"\n","        pygame.init()\n","        flags = pygame.DOUBLEBUF | pygame.HWSURFACE\n","        self.window = pygame.display.set_mode((VAR.canvas_size, VAR.canvas_size),\n","                                              flags)\n","        self.window.set_alpha(None)\n","\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def cycle_menu(self, menu_options, list_menu, dictionary, img = None,\n","                   img_rect = None):\n","        \"\"\"Cycle through a given menu, waiting for an option to be clicked.\"\"\"\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            events = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for i, option in enumerate(menu_options):\n","                if option is not None:\n","                    option.draw()\n","                    option.hovered = False\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        for event in events:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = dictionary[list_menu[i]]\n","\n","            if selected_option is not None:\n","                selected = True\n","            if img is not None:\n","                self.window.blit(img, img_rect.bottomleft)\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def cycle_matches(self, n_matches, mega_hardcore = False):\n","        \"\"\"Cycle through matches until the end.\"\"\"\n","        score = array('i')\n","\n","        for _ in range(n_matches):\n","            self.reset_game()\n","            self.start_match(wait = 3)\n","            score.append(self.single_player(mega_hardcore))\n","\n","        return score\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images\" +\n","                                              \"/snake_logo.png\")).convert()\n","        img = pygame.transform.scale(img, (VAR.canvas_size,\n","                                           int(VAR.canvas_size / 3)))\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","        list_menu = ['PLAY', 'BENCHMARK', 'LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY GAME ',\n","                                  (self.screen_rect.centerx,\n","                                   4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ',\n","                                  (self.screen_rect.centerx,\n","                                   6 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ',\n","                                  (self.screen_rect.centerx,\n","                                   8 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ',\n","                                  (self.screen_rect.centerx,\n","                                   10 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\")]\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS,\n","                                          img, img_rect)\n","\n","        return selected_option\n","\n","    def start_match(self, wait):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(wait):\n","            time = str(wait - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in',\n","                              (self.screen_rect.centerx,\n","                               4 * self.screen_rect.centery / 10),\n","                              self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                     12 * self.screen_rect.centery / 10),\n","                              self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","            pygame.time.wait(1000)\n","\n","        LOGGER.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","\n","        while True:\n","            if opt == OPTIONS['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == OPTIONS['PLAY']:\n","                VAR.game_speed, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = 1,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == OPTIONS['BENCHMARK']:\n","                VAR.game_speed, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = VAR.benchmark,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == OPTIONS['LEADERBOARDS']:\n","                self.view_leaderboards()\n","            elif opt == OPTIONS['MENU']:\n","                opt = self.menu()\n","            if opt == OPTIONS['ADD_TO_LEADERBOARDS']:\n","                self.add_to_leaderboards(score, None) # Gotta improve this logic.\n","                self.view_leaderboards()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        score_option = None\n","\n","        if len(score) == VAR.benchmark:\n","            score_option = TextBlock(' ADD TO LEADERBOARDS ',\n","                                     (self.screen_rect.centerx,\n","                                      8 * self.screen_rect.centery / 10),\n","                                     self.window, (1 / 15), \"menu\")\n","\n","        text_score = 'SCORE: ' + str(int(np.mean(score)))\n","        list_menu = ['PLAY', 'MENU', 'ADD_TO_LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                   4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                   6 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        score_option,\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        TextBlock(text_score, (self.screen_rect.centerx,\n","                                               15 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"text\")]\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        LOGGER.info('EVENT: GAME OVER | FINAL %s', text_score)\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS)\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        list_menu = ['EASY', 'MEDIUM', 'HARD', 'MEGA_HARDCORE']\n","        menu_options = [TextBlock(LEVELS[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\")]\n","\n","        speed = self.cycle_menu(menu_options, list_menu, SPEEDS)\n","        mega_hardcore = False\n","\n","        if speed == SPEEDS['MEGA_HARDCORE']:\n","            mega_hardcore = True\n","\n","        return speed, mega_hardcore\n","\n","    def single_player(self, mega_hardcore = False):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                   previous_size)\n","\n","        # Main loop, where snakes moves after elapsed time is bigger than the\n","        # move_wait time. The last_key pressed is recorded to make the game more\n","        # smooth for human players.\n","        elapsed = 0\n","        last_key = self.snake.previous_action\n","        move_wait = VAR.game_speed\n","\n","        while not self.game_over:\n","            elapsed += self.fps.get_time()  # Get elapsed time since last call.\n","\n","            if mega_hardcore:  # Progressive speed increments, the hardest.\n","                move_wait = VAR.game_speed - (2 * (self.snake.length - 3))\n","\n","            key_input = self.handle_input()  # Receive inputs with tick.\n","            invalid_key = self.snake.is_movement_invalid(key_input)\n","\n","            if key_input is not None and not invalid_key:\n","                last_key = key_input\n","\n","            if elapsed >= move_wait:  # Move and redraw\n","                elapsed = 0\n","                self.play(last_key)\n","                current_size = self.snake.length  # Update the body size\n","\n","                if current_size > previous_size:\n","                    color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                               current_size)\n","\n","                    previous_size = current_size\n","\n","                self.draw(color_list)\n","\n","            pygame.display.update()\n","            self.fps.tick(GAME_FPS)  # Limit FPS to 100\n","\n","        score = current_size - 3  # After the game is over, record score\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (VAR.board_size - 1) or self.snake.head[0] < 0:\n","            LOGGER.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (VAR.board_size - 1) or self.snake.head[1] < 0:\n","            LOGGER.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            LOGGER.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = None\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            LOGGER.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            LOGGER.info('ACTION: KEY PRESSED: LEFT')\n","            action = ABSOLUTE_ACTIONS['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            LOGGER.info('ACTION: KEY PRESSED: RIGHT')\n","            action = ABSOLUTE_ACTIONS['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            LOGGER.info('ACTION: KEY PRESSED: UP')\n","            action = ABSOLUTE_ACTIONS['UP']\n","        elif keys[pygame.K_DOWN]:\n","            LOGGER.info('ACTION: KEY PRESSED: DOWN')\n","            action = ABSOLUTE_ACTIONS['DOWN']\n","\n","        return action\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size board_size**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((VAR.board_size, VAR.board_size))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = POINT_TYPE['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = POINT_TYPE['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = POINT_TYPE['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == RELATIVE_ACTIONS['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == RELATIVE_ACTIONS['LEFT']:\n","            if self.snake.previous_action == ABSOLUTE_ACTIONS['LEFT']:\n","                action = ABSOLUTE_ACTIONS['DOWN']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['RIGHT']:\n","                action = ABSOLUTE_ACTIONS['UP']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['UP']:\n","                action = ABSOLUTE_ACTIONS['LEFT']\n","            else:\n","                action = ABSOLUTE_ACTIONS['RIGHT']\n","        else:\n","            if self.snake.previous_action == ABSOLUTE_ACTIONS['LEFT']:\n","                action = ABSOLUTE_ACTIONS['UP']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['RIGHT']:\n","                action = ABSOLUTE_ACTIONS['DOWN']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['UP']:\n","                action = ABSOLUTE_ACTIONS['RIGHT']\n","            else:\n","                action = ABSOLUTE_ACTIONS['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                self.game_over = True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current reward. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = REWARDS['MOVE']\n","\n","        if self.game_over:\n","            reward = REWARDS['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect((part[0] *\n","                        VAR.block_size), part[1] * VAR.block_size,\n","                        VAR.block_size, VAR.block_size))\n","\n","        pygame.draw.rect(self.window, VAR.food_color,\n","                         pygame.Rect(self.food_pos[0] * VAR.block_size,\n","                         self.food_pos[1] * VAR.block_size, VAR.block_size,\n","                         VAR.block_size))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                   + str(self.snake.length - 3))\n","\n","    def get_name(self):\n","        \"\"\"See test.py in my desktop, for a textbox input in pygame\"\"\"\n","        return None\n","\n","    def add_to_leaderboards(self, score, step):\n","        file_path = resource_path(\"resources/scores.json\")\n","\n","        name = self.get_name()\n","        new_score = {'name': 'test',\n","                     'ranking_data': {'score': score,\n","                                      'step': step}}\n","\n","        with open(file_path, 'w') as leaderboards_file:\n","            json.dump(new_score, leaderboards_file)\n","\n","    def view_leaderboards(self):\n","        list_menu = ['MENU']\n","        menu_options = [TextBlock('LEADERBOARDS',\n","                                  (self.screen_rect.centerx,\n","                                   2 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"text\")]\n","\n","        file_path = resource_path(\"resources/scores.json\")\n","\n","        with open(file_path, 'r') as leaderboards_file:\n","            scores_data = json.loads(leaderboards_file.read())\n","\n","        scores_data.sort(key = operator.itemgetter('score'))\n","\n","#        for score in formatted_scores:\n","#            menu_options.append(TextBlock(person_ranked,\n","#                                (self.screen_rect.centerx,\n","#                                10 * self.screen_rect.centery / 10),\n","#                                self.window, (1 / 12), \"text\"))\n","\n","        menu_options.append(TextBlock('MENU',\n","                            (self.screen_rect.centerx,\n","                            10 * self.screen_rect.centery / 10),\n","                            self.window, (1 / 12), \"menu\"))\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS)\n","\n","    @staticmethod\n","    def format_scores(scores, ammount):\n","        scores = scores[-ammount:]\n","\n","\n","\n","    @staticmethod\n","    def eval_local_safety(canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size board_size**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if ((body[0][0] + 1) > (VAR.board_size - 1)\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]):\n","            canvas[VAR.board_size - 1, 0] = POINT_TYPE['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[VAR.board_size - 1, 1] = POINT_TYPE['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[VAR.board_size - 1, 2] = POINT_TYPE['DANGEROUS']\n","        if ((body[0][1] + 1) > (VAR.board_size - 1)\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]):\n","            canvas[VAR.board_size - 1, 3] = POINT_TYPE['DANGEROUS']\n","\n","        return canvas\n","\n","    @staticmethod\n","    def gradient(colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for substep in range(1, substeps):\n","                yield tuple([(start[component]\n","                              + (float(substep) / (substeps - 1))\n","                              * (finish[component] - start[component]))\n","                             for component in range(components)])\n","\n","        def pairs(seq):\n","            first_color, second_color = tee(seq)\n","            next(second_color, None)\n","\n","            return zip(first_color, second_color)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for first_color, second_color in pairs(colors):\n","            for gradient_color in linear_gradient(first_color, second_color,\n","                                                  substeps):\n","                result.append(gradient_color)\n","\n","        return result\n","\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EyzmgPBlpC_L"},"source":["## 2. Experience Replay (memory)"]},{"cell_type":"markdown","metadata":{"id":"GyA5BM9psnPh"},"source":["### 2.1 Utilities <a name=\"experience-replay-utilities\"></a>"]},{"cell_type":"markdown","metadata":{"id":"UpNnJYNUtAyG"},"source":["#### 2.1.1 Sum Tree"]},{"cell_type":"code","metadata":{"id":"ivP835vDtEnG","executionInfo":{"status":"ok","timestamp":1614171092049,"user_tz":180,"elapsed":13056,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["import numpy as np\n","import sys\n","import time\n","import operator\n","from datetime import timedelta\n","import collections\n","\n","class SegmentTree(object):\n","    def __init__(self, capacity, operation, neutral_element):\n","        \"\"\"Build a Segment Tree data structure.\n","        https://en.wikipedia.org/wiki/Segment_tree\n","        Can be used as regular array, but with two\n","        important differences:\n","            a) setting item's value is slightly slower.\n","               It is O(lg capacity) instead of O(1).\n","            b) user has access to an efficient `reduce`\n","               operation which reduces `operation` over\n","               a contiguous subsequence of items in the\n","               array.\n","        Paramters\n","        ---------\n","        capacity: int\n","            Total size of the array - must be a power of two.\n","        operation: lambda obj, obj -> obj\n","            and operation for combining elements (eg. sum, max)\n","            must for a mathematical group together with the set of\n","            possible values for array elements.\n","        neutral_element: obj\n","            neutral element for the operation above. eg. float('-inf')\n","            for max and 0 for sum.\n","        \"\"\"\n","        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n","        self._capacity = capacity\n","        self._value = [neutral_element for _ in range(2 * capacity)]\n","        self._operation = operation\n","\n","    def _reduce_helper(self, start, end, node, node_start, node_end):\n","        if start == node_start and end == node_end:\n","            return self._value[node]\n","        mid = (node_start + node_end) // 2\n","        if end <= mid:\n","            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n","        else:\n","            if mid + 1 <= start:\n","                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n","            else:\n","                return self._operation(\n","                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n","                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n","                )\n","\n","    def reduce(self, start=0, end=None):\n","        \"\"\"Returns result of applying `self.operation`\n","        to a contiguous subsequence of the array.\n","            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n","        Parameters\n","        ----------\n","        start: int\n","            beginning of the subsequence\n","        end: int\n","            end of the subsequences\n","        Returns\n","        -------\n","        reduced: obj\n","            result of reducing self.operation over the specified range of array elements.\n","        \"\"\"\n","        if end is None:\n","            end = self._capacity\n","        if end < 0:\n","            end += self._capacity\n","        end -= 1\n","        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n","\n","    def __setitem__(self, idx, val):\n","        # index of the leaf\n","        idx += self._capacity\n","        self._value[idx] = val\n","        idx //= 2\n","        while idx >= 1:\n","            self._value[idx] = self._operation(\n","                self._value[2 * idx],\n","                self._value[2 * idx + 1]\n","            )\n","            idx //= 2\n","\n","    def __getitem__(self, idx):\n","        assert 0 <= idx < self._capacity\n","        return self._value[self._capacity + idx]\n","\n","\n","class SumSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(SumSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=operator.add,\n","            neutral_element=0.0\n","        )\n","\n","    def sum(self, start=0, end=None):\n","        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n","        return super(SumSegmentTree, self).reduce(start, end)\n","\n","    def find_prefixsum_idx(self, prefixsum):\n","        \"\"\"Find the highest index `i` in the array such that\n","            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n","        if array values are probabilities, this function\n","        allows to sample indexes according to the discrete\n","        probability efficiently.\n","        Parameters\n","        ----------\n","        perfixsum: float\n","            upperbound on the sum of array prefix\n","        Returns\n","        -------\n","        idx: int\n","            highest index satisfying the prefixsum constraint\n","        \"\"\"\n","        assert 0 <= prefixsum <= self.sum() + 1e-5\n","        idx = 1\n","        while idx < self._capacity:  # while non-leaf\n","            if self._value[2 * idx] > prefixsum:\n","                idx = 2 * idx\n","            else:\n","                prefixsum -= self._value[2 * idx]\n","                idx = 2 * idx + 1\n","        return idx - self._capacity\n","\n","\n","class MinSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(MinSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=min,\n","            neutral_element=float('inf')\n","        )\n","\n","    def min(self, start=0, end=None):\n","        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n","\n","        return super(MinSegmentTree, self).reduce(start, end)\n","\n","\n","class SumTree:\n","    def __init__(self, capacity):\n","        self._capacity = capacity\n","        self._tree = np.zeros(2 * self._capacity - 1)\n","        self._data = np.zeros(self._capacity, dtype = object)\n","        self._data_idx = 0\n","\n","    @property\n","    def capacity(self):\n","        return self._capacity\n","\n","    @property\n","    def tree(self):\n","        return self._tree\n","\n","    @property\n","    def data(self):\n","        return self._data\n","\n","    def sum(self):\n","        return self._tree[0]\n","\n","    def insert(self, data, priority):\n","#        print(\"Data shape: {}\".format(data.shape))\n","#        print(\"Stored data shape: {}\".format(self._data.shape))\n","        self._data[self._data_idx] = data\n","        tree_idx = self._data_idx + self._capacity - 1\n","        self.update(tree_idx, priority)\n","        self._data_idx += 1\n","        if self._data_idx >= self._capacity:\n","            self._data_idx = 0\n","\n","    def update(self, tree_idx, priority):\n","        delta = priority - self._tree[tree_idx]\n","        self._tree[tree_idx] = priority\n","        while tree_idx != 0:\n","            tree_idx = (tree_idx - 1) // 2  # Get parent\n","            self._tree[tree_idx] += delta\n","\n","    def retrieve(self, val):\n","        tree_idx, parent = None, 0\n","        while True:\n","            left = 2 * parent + 1\n","            right = left + 1\n","            if left >= len(self._tree):  # Leaf\n","                tree_idx = parent\n","                break\n","            else:\n","                if val <= self._tree[left]:\n","                    parent = left\n","                else:\n","                    val -= self._tree[left]\n","                    parent = right\n","\n","        priority = self._tree[tree_idx]\n","        data = self._data[tree_idx - self._capacity + 1]\n","\n","        return tree_idx, priority, data\n","\n","    def max_leaf(self):\n","        return np.max(self.leaves())\n","\n","    def min_leaf(self):\n","        return np.min(self.leaves())\n","\n","    def leaves(self):\n","        return self._tree[-self._capacity:]\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tgIFwsvbtLbl"},"source":["### 2.1.2 Linear Schedule"]},{"cell_type":"code","metadata":{"id":"eJp_3r3nsy0b","executionInfo":{"status":"ok","timestamp":1614171092051,"user_tz":180,"elapsed":13053,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPTOzs8asNKS"},"source":["### 2.2 Imports <a name=\"experience-replay-imports\"></a>"]},{"cell_type":"code","metadata":{"id":"12HjK_vPsMrm","executionInfo":{"status":"ok","timestamp":1614171092052,"user_tz":180,"elapsed":13044,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["import numpy as np\n","from random import sample, uniform, random\n","from array import array  # Efficient numeric arrays"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"-uyxpIitpAnt","executionInfo":{"status":"ok","timestamp":1614171093094,"user_tz":180,"elapsed":14080,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes\n","    ----------\n","    memory: list of experiences\n","        Memory list to insert experiences.\n","    memory_size: int, optional, default = 150000\n","        The ammount of experiences to be stored in the memory.\n","    input_shape: tuple of 3 * int\n","        The shape of the input which will be stored.\n","    \"\"\"\n","    def __init__(self, memory_size = 150000):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        self.memory.append(experience)\n","\n","        if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","            self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\n","\n","        Return\n","        ----------\n","        batch: np.array of batch_size experiences\n","            The batched experiences from memory.\n","        IS_weights: np.array of batch_size of the weights\n","            As it's used only in PER, is an array of ones in this case.\n","        Indexes: list of batch_size * int\n","            As it's used only in PER, return None.\n","        \"\"\"\n","        IS_weights = np.ones((batch_size, ))\n","        batch = np.array(sample(self.memory, batch_size))\n","\n","        return batch, IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            memory_size = 150000\n","\n","        self.memory = []\n","\n","\n","class PrioritizedExperienceReplayNaive:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 150000, alpha = 0.6, epsilon = 0.001,\n","                 beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return self.exp\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        max_priority = self.memory.max_leaf()\n","\n","        if max_priority == 0:\n","            max_priority = self.get_priority(0)\n","\n","        self.memory.insert(experience, max_priority)\n","        self.exp += 1\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        batch = [None] * batch_size\n","        IS_weights = np.zeros((batch_size, ))\n","        tree_indices = [0] * batch_size\n","\n","        memory_sum = self.memory.sum()\n","        len_seg = memory_sum / batch_size\n","        min_prob = self.memory.min_leaf() / memory_sum\n","\n","        for i in range(batch_size):\n","            val = uniform(len_seg * i, len_seg * (i + 1))\n","            tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","            prob = priority / self.memory.sum()\n","            IS_weights[i] = np.power(prob / min_prob, -self.beta)\n","\n","        return np.array(batch), IS_weights, tree_indices\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = SumTree(self.memory_size)\n","        self.exp = 0\n","\n","\n","class PrioritizedExperienceReplay:\n","    def __init__(self, memory_size, nb_epoch = 10000, epsilon = 0.001,\n","                 alpha = 0.6, beta = 0.4, decay = 0.5):\n","        self.memory_size = memory_size\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.max_priority = 1.0\n","        self.reset_memory()\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","        if self.exp_size() < self.memory_size:\n","            self.memory.append(experience)\n","            self.pos += 1\n","        else:\n","            self.memory[self.pos] = experience\n","            self.pos = (self.pos + 1) % self.memory_size\n","\n","        self._it_sum[self.pos] = self.max_priority ** self.alpha\n","        self._it_min[self.pos] = self.max_priority ** self.alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = array('i')\n","\n","        for _ in range(batch_size):\n","            mass = random() * self._it_sum.sum(0, self.exp_size() - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","\n","        return res\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def get_samples(self, batch_size):\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = array('f')\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * self.exp_size()) ** (-self.beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * self.exp_size()) ** (-self.beta)\n","            weights.append(weight / max_weight)\n","\n","        weights = np.array(weights, dtype=np.float32)\n","        samples = [self.memory[idx] for idx in idxes]\n","\n","        return np.array(samples), weights, idxes\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def update_priorities(self, idxes, errors):\n","        priorities = self.get_priority(errors)\n","\n","        for idx, priority in zip(idxes, priorities):\n","            self._it_sum[idx] = priority ** self.alpha\n","            self._it_min[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = []\n","        self.pos = 0\n","\n","        it_capacity = 1\n","\n","        while it_capacity < self.memory_size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hDWB9sUZtf9C"},"source":["### 2.2 Experience Replay (ER)"]},{"cell_type":"code","metadata":{"id":"xxiGVK4std7v","executionInfo":{"status":"ok","timestamp":1614171093099,"user_tz":180,"elapsed":14080,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes\n","    ----------\n","    memory: list of experiences\n","        Memory list to insert experiences.\n","    memory_size: int, optional, default = 150000\n","        The ammount of experiences to be stored in the memory.\n","    input_shape: tuple of 3 * int\n","        The shape of the input which will be stored.\n","    \"\"\"\n","    def __init__(self, memory_size = 150000):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        self.memory.append(experience)\n","\n","        if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","            self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\n","\n","        Return\n","        ----------\n","        batch: np.array of batch_size experiences\n","            The batched experiences from memory.\n","        IS_weights: np.array of batch_size of the weights\n","            As it's used only in PER, is an array of ones in this case.\n","        Indexes: list of batch_size * int\n","            As it's used only in PER, return None.\n","        \"\"\"\n","        IS_weights = np.ones((batch_size, ))\n","        batch = np.array(sample(self.memory, batch_size))\n","\n","        return batch, IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            memory_size = 150000\n","\n","        self.memory = []\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVhJ1taVtk4a"},"source":["### 2.3 Naive Prioritized Experience Replay (NPER)"]},{"cell_type":"code","metadata":{"id":"glOfwZ04tfzC","executionInfo":{"status":"ok","timestamp":1614171093100,"user_tz":180,"elapsed":14074,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["class PrioritizedExperienceReplayNaive:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 150000, alpha = 0.6, epsilon = 0.001,\n","                 beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return self.exp\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        max_priority = self.memory.max_leaf()\n","\n","        if max_priority == 0:\n","            max_priority = self.get_priority(0)\n","\n","        self.memory.insert(experience, max_priority)\n","        self.exp += 1\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        batch = [None] * batch_size\n","        IS_weights = np.zeros((batch_size, ))\n","        tree_indices = [0] * batch_size\n","\n","        memory_sum = self.memory.sum()\n","        len_seg = memory_sum / batch_size\n","        min_prob = self.memory.min_leaf() / memory_sum\n","\n","        for i in range(batch_size):\n","            val = uniform(len_seg * i, len_seg * (i + 1))\n","            tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","            prob = priority / self.memory.sum()\n","            IS_weights[i] = np.power(prob / min_prob, -self.beta)\n","\n","        return np.array(batch), IS_weights, tree_indices\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = SumTree(self.memory_size)\n","        self.exp = 0\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SboDvSwptq0Y"},"source":["### 2.4 Prioritized Experience Replay"]},{"cell_type":"code","metadata":{"id":"c4jOQ046tXv-","executionInfo":{"status":"ok","timestamp":1614171093413,"user_tz":180,"elapsed":14383,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["class PrioritizedExperienceReplay:\n","    def __init__(self, memory_size, nb_epoch = 10000, epsilon = 0.001,\n","                 alpha = 0.6, beta = 0.4, decay = 0.5):\n","        self.memory_size = memory_size\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.max_priority = 1.0\n","        self.reset_memory()\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","        if self.exp_size() < self.memory_size:\n","            self.memory.append(experience)\n","            self.pos += 1\n","        else:\n","            self.memory[self.pos] = experience\n","            self.pos = (self.pos + 1) % self.memory_size\n","\n","        self._it_sum[self.pos] = self.max_priority ** self.alpha\n","        self._it_min[self.pos] = self.max_priority ** self.alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = array('i')\n","\n","        for _ in range(batch_size):\n","            mass = random() * self._it_sum.sum(0, self.exp_size() - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","\n","        return res\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def get_samples(self, batch_size):\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = array('f')\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * self.exp_size()) ** (-self.beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * self.exp_size()) ** (-self.beta)\n","            weights.append(weight / max_weight)\n","\n","        weights = np.array(weights, dtype=np.float32)\n","        samples = [self.memory[idx] for idx in idxes]\n","\n","        return np.array(samples), weights, idxes\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def update_priorities(self, idxes, errors):\n","        priorities = self.get_priority(errors)\n","\n","        for idx, priority in zip(idxes, priorities):\n","            self._it_sum[idx] = priority ** self.alpha\n","            self._it_min[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = []\n","        self.pos = 0\n","\n","        it_capacity = 1\n","\n","        while it_capacity < self.memory_size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uPSd7lKRuD74"},"source":["## 3. DQN Agent"]},{"cell_type":"markdown","metadata":{"id":"B4a4ukvGuRtv"},"source":["### 3.1 Exploration Policies"]},{"cell_type":"code","metadata":{"id":"SgjZMhgAuhms","executionInfo":{"status":"ok","timestamp":1614171093415,"user_tz":180,"elapsed":14369,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["import random\n","import numpy as np\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVT025fgusjo"},"source":["### 3.2 Networks"]},{"cell_type":"markdown","metadata":{"id":"HImaaq9ovNv3"},"source":["#### 3.2.1 Utilities\n"]},{"cell_type":"code","metadata":{"id":"n_XXrFzJvSR-","executionInfo":{"status":"ok","timestamp":1614171093416,"user_tz":180,"elapsed":14362,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\n","\tif hasattr(tf, 'select'):\n","\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\telse:\n","\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaPcM8lLvaYT"},"source":["#### 3.2.2 Noisy layers"]},{"cell_type":"code","metadata":{"id":"2T-q75K2vaJj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614171093875,"user_tz":180,"elapsed":14538,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}},"outputId":"c04c47db-9db3-43b3-e65a-94dba9420f14"},"source":["\"\"\"THIS\"\"\"\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.python.framework import tensor_shape\n","from tensorflow.python.layers import base\n","from tensorflow.python.ops.init_ops import Constant\n","\n","import keras\n","from keras.layers import Dense\n","from keras import backend as K\n","from keras.engine.topology import Layer\n","from keras import activations, initializers, regularizers, constraints\n","\n","\n","class SimplifiedNoisyDense(Layer):\n","    \"\"\"From OctThe16th repo.\"\"\"\n","    def __init__(self, units,\n","                 sigma_init=0.02,\n","                 activation=None,\n","                 use_bias=True,\n","                 kernel_initializer='glorot_uniform',\n","                 bias_initializer='zeros',\n","                 kernel_regularizer=None,\n","                 bias_regularizer=None,\n","                 activity_regularizer=None,\n","                 kernel_constraint=None,\n","                 bias_constraint=None,\n","                 **kwargs):\n","        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n","            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n","        super(SimplifiedNoisyDense, self).__init__(**kwargs)\n","        self.units = units\n","        self.sigma_init = sigma_init\n","        self.activation = activations.get(activation)\n","        self.use_bias = use_bias\n","        self.kernel_initializer = initializers.get(kernel_initializer)\n","        self.bias_initializer = initializers.get(bias_initializer)\n","        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n","        self.bias_regularizer = regularizers.get(bias_regularizer)\n","        self.activity_regularizer = regularizers.get(activity_regularizer)\n","        self.kernel_constraint = constraints.get(kernel_constraint)\n","        self.bias_constraint = constraints.get(bias_constraint)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) >= 2\n","        self.input_dim = input_shape[-1]\n","\n","        self.kernel = self.add_weight(shape=(self.input_dim, self.units),\n","                                      initializer=self.kernel_initializer,\n","                                      name='kernel',\n","                                      regularizer=self.kernel_regularizer,\n","                                      constraint=self.kernel_constraint)\n","\n","        self.sigma_kernel = self.add_weight(shape=(self.input_dim, self.units),\n","                                      initializer=initializers.Constant(value=self.sigma_init),\n","                                      name='sigma_kernel'\n","                                      )\n","\n","\n","        if self.use_bias:\n","            self.bias = self.add_weight(shape=(self.units,),\n","                                        initializer=self.bias_initializer,\n","                                        name='bias',\n","                                        regularizer=self.bias_regularizer,\n","                                        constraint=self.bias_constraint)\n","            self.sigma_bias = self.add_weight(shape=(self.units,),\n","                                        initializer=initializers.Constant(value=self.sigma_init),\n","                                        name='sigma_bias')\n","        else:\n","            self.bias = None\n","            self.epsilon_bias = None\n","\n","        self.epsilon_kernel = K.zeros(shape=(self.input_dim, self.units))\n","        self.epsilon_bias = K.zeros(shape=(self.units,))\n","\n","        self.sample_noise()\n","        super(SimplifiedNoisyDense, self).build(input_shape)\n","\n","\n","    def call(self, X):\n","        perturbation = self.sigma_kernel * self.epsilon_kernel\n","        perturbed_kernel = self.kernel + perturbation\n","        output = K.dot(X, perturbed_kernel)\n","        if self.use_bias:\n","            bias_perturbation = self.sigma_bias * self.epsilon_bias\n","            perturbed_bias = self.bias + bias_perturbation\n","            output = K.bias_add(output, perturbed_bias)\n","        if self.activation is not None:\n","            output = self.activation(output)\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        assert input_shape and len(input_shape) >= 2\n","        assert input_shape[-1]\n","        output_shape = list(input_shape)\n","        output_shape[-1] = self.units\n","        return tuple(output_shape)\n","\n","    def sample_noise(self):\n","        K.set_value(self.epsilon_kernel, np.random.normal(0, 1, (self.input_dim, self.units)))\n","        K.set_value(self.epsilon_bias, np.random.normal(0, 1, (self.units,)))\n","\n","    def remove_noise(self):\n","        K.set_value(self.epsilon_kernel, np.zeros(shape=(self.input_dim, self.units)))\n","        K.set_value(self.epsilon_bias, np.zeros(shape=self.units,))\n","\n","class NoisyDense(Dense):\n","    \"\"\"From spring01's drlbox repo.\"\"\"\n","    def build(self, input_shape):\n","        input_shape = tensor_shape.TensorShape(input_shape)\n","        if input_shape[-1].value is None:\n","            raise ValueError('The last dimension of the inputs to `Dense` '\n","                             'should be defined. Found `None`.')\n","        self.input_spec = base.InputSpec(min_ndim=2,\n","                                         axes={-1: input_shape[-1].value})\n","        kernel_shape = [input_shape[-1].value, self.units]\n","        kernel_quiet = self.add_weight('kernel_quiet',\n","                                         shape=kernel_shape,\n","                                         initializer=self.kernel_initializer,\n","                                         regularizer=self.kernel_regularizer,\n","                                         constraint=self.kernel_constraint,\n","                                         trainable=True)\n","        scale_init = Constant(value=(0.5 / np.sqrt(kernel_shape[0])))\n","        kernel_noise_scale = self.add_weight('kernel_noise_scale',\n","                                               shape=kernel_shape,\n","                                               initializer=scale_init,\n","                                               trainable=True)\n","        kernel_noise = self.make_kernel_noise(shape=kernel_shape)\n","        self.kernel = kernel_quiet + kernel_noise_scale * kernel_noise\n","        if self.use_bias:\n","            bias_shape = [self.units,]\n","            bias_quiet = self.add_weight('bias_quiet',\n","                                           shape=bias_shape,\n","                                           initializer=self.bias_initializer,\n","                                           regularizer=self.bias_regularizer,\n","                                           constraint=self.bias_constraint,\n","                                           trainable=True)\n","            bias_noise_scale = self.add_weight(name='bias_noise_scale',\n","                                                 shape=bias_shape,\n","                                                 initializer=scale_init,\n","                                                 trainable=True)\n","            bias_noise = self.make_bias_noise(shape=bias_shape)\n","            self.bias = bias_quiet + bias_noise_scale * bias_noise\n","        else:\n","            self.bias = None\n","        self.built = True\n","\n","    def make_kernel_noise(self, shape):\n","        raise NotImplementedError\n","\n","    def make_bias_noise(self, shape):\n","        raise NotImplementedError\n","\n","\n","class NoisyDenseIG(NoisyDense):\n","    '''\n","    Noisy dense layer with independent Gaussian noise\n","    '''\n","    def make_kernel_noise(self, shape):\n","        noise = tf.random_normal(shape)\n","        kernel_noise = tf.Variable(noise, trainable=False)\n","        self.noise_list = [kernel_noise]\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        noise = tf.random_normal(shape)\n","        bias_noise = tf.Variable(noise, trainable=False)\n","        self.noise_list.append(bias_noise)\n","        return bias_noise\n","\n","\n","class NoisyDenseFG(NoisyDense):\n","    '''\n","    Noisy dense layer with factorized Gaussian noise\n","    '''\n","    def make_kernel_noise(self, shape):\n","        kernel_noise_input = self.make_fg_noise(shape=[shape[0]])\n","        kernel_noise_output = self.make_fg_noise(shape=[shape[1]])\n","        self.noise_list = [kernel_noise_input, kernel_noise_output]\n","        kernel_noise = kernel_noise_input[:, tf.newaxis] * kernel_noise_output\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        return self.noise_list[1] # kernel_noise_output\n","\n","    def make_fg_noise(self, shape):\n","        noise = tf.random_normal(shape)\n","        trans_noise = tf.sign(noise) * tf.sqrt(tf.abs(noise))\n","        return tf.Variable(trans_noise, trainable=False)\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"4U6wkiWV1Y3k"},"source":["#### 3.2.3 Custom optimizers"]},{"cell_type":"code","metadata":{"id":"BPP-T0p11bwQ","executionInfo":{"status":"ok","timestamp":1614171095691,"user_tz":180,"elapsed":16350,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["from __future__ import absolute_import\n","\n","from keras import backend as K\n","from keras.optimizers import Optimizer\n","#from keras.legacy import interfaces\n","\n","from tensorflow.python.ops import state_ops\n","from tensorflow.python.ops import math_ops\n","from tensorflow.python.util.tf_export import tf_export\n","\n","if K.backend() == 'tensorflow':\n","    import tensorflow as tf\n","\n","class COCOB(Optimizer):\n","    \"\"\"COCOB-Backprop optimizer.\n","    It is recommended to leave the parameters of this optimizer\n","    at their default values\n","    (except the learning rate, which can be freely tuned).\n","    This optimizer, unlike other stochastic gradient based optimizers, optimize the function by\n","    finding individual learning rates in a coin-betting way.\n","    # Arguments\n","        alphs: float >= 0. Multiples of the largest absolute magtitude of gradients.\n","        epsilon: float >= 0. Fuzz factor.\n","    # References\n","        - [Training Deep Networks without Learning Rates Through Coin Betting](http://https://arxiv.org/pdf/1705.07795.pdf)\n","    \"\"\"\n","\n","    def __init__(self, alpha=100, epsilon=1e-8, **kwargs):\n","        super(COCOB, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.alpha = K.variable(alpha, name='alpha')\n","            self.iterations = K.variable(0., name='iterations')\n","        self.epsilon = epsilon\n","\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [K.update_add(self.iterations, 1)]\n","\n","        L = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n","        M = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n","        Reward = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n","        grad_sum = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n","\n","        if K.eval(self.iterations) == 0:\n","            old_params = [K.constant(K.eval(p)) for p in params]\n","            # [K.eval(p) for p in params]\n","\n","        self.weights = [self.iterations] + L + M + Reward + grad_sum\n","\n","        for old_p, p, g, gs, l, m, r in zip(old_params, params, grads, grad_sum, L, M, Reward):\n","            # update accumulator\n","            # old_p = K.variable(old_p)\n","\n","            new_l = K.maximum(l, K.abs(g))\n","            self.updates.append(K.update(l, new_l))\n","\n","            new_m = m + K.abs(g)\n","            self.updates.append(K.update(m, new_m))\n","\n","            new_r = K.maximum(r - (p - old_p)*g, 0)\n","            self.updates.append(K.update(r, new_r))\n","\n","            new_gs = gs + g\n","            self.updates.append(K.update(gs, new_gs))\n","\n","            new_p = old_p - (new_gs/(self.epsilon + new_l*K.maximum(new_m+new_l, self.alpha*new_l)))*(new_l + new_r)\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'alpha': float(K.get_value(self.alpha)),\n","                  'epsilon': self.epsilon}\n","        base_config = super(COCOB, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class SMORMS3(Optimizer):\n","    '''SMORMS3 optimizer.\n","    Implemented based on http://sifter.org/~simon/journal/20150420.html\n","    # Arguments\n","        lr: float >= 0. Learning rate.\n","        epsilon: float >= 0. Fuzz factor.\n","        decay: float >= 0. Learning rate decay over each update.\n","    '''\n","\n","    def __init__(self, lr=0.001, epsilon=1e-16, decay=0.,\n","                 **kwargs):\n","        super(SMORMS3, self).__init__(**kwargs)\n","        self.__dict__.update(locals())\n","        with K.name_scope(self.__class__.__name__):\n","            self.lr = K.variable(lr)\n","            # self.rho = K.variable(rho)\n","            self.decay = K.variable(decay)\n","            self.inital_decay = decay\n","            self.iterations = K.variable(0.)\n","        self.epsilon = epsilon\n","        self.initial_decay = decay\n","\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        shapes = [K.get_variable_shape(p) for p in params]\n","        self.updates.append(K.update_add(self.iterations, 1))\n","\n","        g1s = [K.zeros(shape) for shape in shapes]\n","        g2s = [K.zeros(shape) for shape in shapes]\n","        mems = [K.ones(shape) for shape in shapes]\n","\n","        lr = self.lr\n","        if self.inital_decay > 0:\n","            lr *= (1. / (1. + self.decay * self.iterations))\n","\n","        self.weights = [self.iterations] + g1s + g2s + mems\n","\n","        for p, g, g1, g2, m in zip(params, grads, g1s, g2s, mems):\n","            r = 1. / (m + 1)\n","            new_g1 = (1. - r) * g1 + r * g\n","            new_g2 = (1. - r) * g2 + r * K.square(g)\n","            # update accumulators\n","            self.updates.append(K.update(g1, new_g1))\n","            self.updates.append(K.update(g2, new_g2))\n","            new_p = p - g * K.minimum(lr, K.square(new_g1) / (new_g2 + self.epsilon)) / (\n","            K.sqrt(new_g2) + self.epsilon)\n","            new_m = 1 + m * (1 - K.square(new_g1) / (new_g2 + self.epsilon))\n","            # update rho\n","            self.updates.append(K.update(m, new_m))\n","            # apply constraints\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'lr': float(K.get_value(self.lr)),\n","                  'decay': float(K.get_value(self.decay)),\n","                  'epsilon': self.epsilon}\n","        base_config = super(SMORMS3, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","class Yogi(Optimizer):\n","    \"\"\"Yogi optimizer.\n","    Default parameters follow those provided in the original paper.\n","    Arguments:\n","      lr: float >= 0. Learning rate.\n","      beta_1: float, 0 < beta < 1. Generally close to 1.\n","      beta_2: float, 0 < beta < 1. Generally close to 1.\n","      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n","      decay: float >= 0. Learning rate decay over each update.\n","      amsgrad: boolean. Whether to apply the AMSGrad variant of this\n","          algorithm from the paper \"On the Convergence of Adam and\n","          Beyond\".\n","    \"\"\"\n","\n","    def __init__(self,\n","               lr=0.001,\n","               beta_1=0.9,\n","               beta_2=0.999,\n","               epsilon=None,\n","               decay=0.00000001,\n","               amsgrad=False,\n","               **kwargs):\n","        super(Yogi, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.iterations = K.variable(0, dtype='int64', name='iterations')\n","            self.lr = K.variable(lr, name='lr')\n","            self.beta_1 = K.variable(beta_1, name='beta_1')\n","            self.beta_2 = K.variable(beta_2, name='beta_2')\n","            self.decay = K.variable(decay, name='decay')\n","        if epsilon is None:\n","            epsilon = K.epsilon()\n","        self.epsilon = epsilon\n","        self.initial_decay = decay\n","        self.amsgrad = amsgrad\n","\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [state_ops.assign_add(self.iterations, 1)]\n","\n","        lr = self.lr\n","        if self.initial_decay > 0:\n","            lr = lr * (  # pylint: disable=g-no-augmented-assignment\n","                1. / (1. + self.decay * math_ops.cast(self.iterations,\n","                                                    K.dtype(self.decay))))\n","\n","        t = math_ops.cast(self.iterations, K.floatx()) + 1\n","        lr_t = lr * (\n","            K.sqrt(1. - math_ops.pow(self.beta_2, t)) /\n","            (1. - math_ops.pow(self.beta_1, t)))\n","\n","        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        if self.amsgrad:\n","            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        else:\n","            vhats = [K.zeros(1) for _ in params]\n","        self.weights = [self.iterations] + ms + vs + vhats\n","\n","        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n","            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n","            #v_t = (self.beta_2 * v) + (1. - self.beta_2) * math_ops.square(g) # from amsgrad\n","            v_t = v - (1-self.beta_2)*K.sign(v-math_ops.square(g))*math_ops.square(g)\n","            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n","\n","            self.updates.append(state_ops.assign(m, m_t))\n","            self.updates.append(state_ops.assign(v, v_t))\n","            new_p = p_t\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(state_ops.assign(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {\n","            'lr': float(K.get_value(self.lr)),\n","            'beta_1': float(K.get_value(self.beta_1)),\n","            'beta_2': float(K.get_value(self.beta_2)),\n","            'decay': float(K.get_value(self.decay)),\n","            'epsilon': self.epsilon,\n","            'amsgrad': self.amsgrad\n","            }\n","        base_config = super(Yogi, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","class Nadamax(Optimizer):\n","    \"\"\"Nesterov Adam optimizer with infinity norm.\n","    Much like Adam is essentially RMSprop with momentum,\n","    Nadam is Adam RMSprop with Nesterov momentum.\n","    Default parameters follow those provided in the paper.\n","    It is recommended to leave the parameters of this optimizer\n","    at their default values.\n","    # Arguments\n","        lr: float >= 0. Learning rate.\n","        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n","        epsilon: float >= 0. Fuzz factor.\n","    # References\n","        - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n","        - [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n","    \"\"\"\n","\n","    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n","                 epsilon=1e-8, schedule_decay=0.004, **kwargs):\n","        super(Nadamax, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.iterations = K.variable(0, dtype='int64', name='iterations')\n","            self.m_schedule = K.variable(1., name='m_schedule')\n","            self.lr = K.variable(lr, name='lr')\n","            self.beta_1 = K.variable(beta_1, name='beta_1')\n","            self.beta_2 = K.variable(beta_2, name='beta_2')\n","        self.epsilon = epsilon\n","        self.schedule_decay = schedule_decay\n","\n","    #@interfaces.legacy_get_updates_support\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [K.update_add(self.iterations, 1)]\n","\n","        t = K.cast(self.iterations, K.floatx()) + 1\n","\n","        # Due to the recommendations in [2], i.e. warming momentum schedule\n","        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n","        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n","        m_schedule_new = self.m_schedule * momentum_cache_t\n","        m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n","        self.updates.append((self.m_schedule, m_schedule_new))\n","\n","        shapes = [K.int_shape(p) for p in params]\n","        ms = [K.zeros(shape) for shape in shapes]\n","        vs = [K.zeros(shape) for shape in shapes]\n","\n","        self.weights = [self.iterations] + ms + vs\n","\n","        for p, g, m, v in zip(params, grads, ms, vs):\n","            # the following equations given in [1]\n","            g_prime = g / (1. - m_schedule_new)\n","            m_t = self.beta_1 * m + (1. - self.beta_1) * g\n","            m_t_prime = m_t / (1. - m_schedule_next)\n","            v_t = K.maximum(self.beta_2 * v, K.abs(g))\n","            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n","            m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n","\n","            self.updates.append(K.update(m, m_t))\n","            self.updates.append(K.update(v, v_t))\n","\n","            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n","            new_p = p_t\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'lr': float(K.get_value(self.lr)),\n","                  'beta_1': float(K.get_value(self.beta_1)),\n","                  'beta_2': float(K.get_value(self.beta_2)),\n","                  'epsilon': self.epsilon,\n","                  'schedule_decay': self.schedule_decay}\n","        base_config = super(Nadamax, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class Radamax(Optimizer):\n","    \"\"\"Nesterov Adam optimizer with infinity norm.\n","    Much like Adam is essentially RMSprop with momentum,\n","    Nadam is Adam RMSprop with Nesterov momentum.\n","    Default parameters follow those provided in the paper.\n","    It is recommended to leave the parameters of this optimizer\n","    at their default values.\n","    # Arguments\n","        lr: float >= 0. Learning rate.\n","        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n","        epsilon: float >= 0. Fuzz factor.\n","    # References\n","        - [Nadam report](http://cs229.stanford.edu/proj2015/054_report.pdf)\n","        - [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)\n","    \"\"\"\n","\n","    def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999,\n","                 epsilon=1e-8, schedule_decay=0.004, **kwargs):\n","        super(Radamax, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.iterations = K.variable(0, dtype='int64', name='iterations')\n","            self.m_schedule = K.variable(1., name='m_schedule')\n","            self.lr = K.variable(lr, name='lr')\n","            self.beta_1 = K.variable(beta_1, name='beta_1')\n","            self.beta_2 = K.variable(beta_2, name='beta_2')\n","        self.epsilon = epsilon\n","        self.schedule_decay = schedule_decay\n","\n","    #@interfaces.legacy_get_updates_support\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [K.update_add(self.iterations, 1)]\n","\n","        t = K.cast(self.iterations, K.floatx()) + 1\n","\n","        # Due to the recommendations in [2], i.e. warming momentum schedule\n","        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n","        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n","        m_schedule_new = self.m_schedule * momentum_cache_t\n","        m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n","        self.updates.append((self.m_schedule, m_schedule_new))\n","\n","        shapes = [K.int_shape(p) for p in params]\n","        ms = [K.zeros(shape) for shape in shapes]\n","        vs = [K.zeros(shape) for shape in shapes]\n","\n","        self.weights = [self.iterations] + ms + vs\n","\n","        for p, g, m, v in zip(params, grads, ms, vs):\n","            # the following equations given in [1]\n","            g_prime = g / (1. - m_schedule_new)\n","            m_t = self.beta_1 * m + (1. - self.beta_1) * g\n","            m_t_prime = m_t / (1. - m_schedule_next)\n","            if np.random.choice([1, -1]) == 1:\n","                v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n","            else:\n","                v_t = K.maximum(self.beta_2 * v, K.abs(g))\n","            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n","            m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n","\n","            self.updates.append(K.update(m, m_t))\n","            self.updates.append(K.update(v, v_t))\n","\n","            p_t = p - self.lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n","            new_p = p_t\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'lr': float(K.get_value(self.lr)),\n","                  'beta_1': float(K.get_value(self.beta_1)),\n","                  'beta_2': float(K.get_value(self.beta_2)),\n","                  'epsilon': self.epsilon,\n","                  'schedule_decay': self.schedule_decay}\n","        base_config = super(Radamax, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class AdamDelta(Optimizer):\n","    \"\"\"AdamDelta optimizer.\n","    Default parameters follow those provided in the original paper.\n","    # Arguments\n","        lr: float >= 0. Learning rate.\n","        beta_1: float, 0 < beta < 1. Generally close to 1.\n","        beta_2: float, 0 < beta < 1. Generally close to 1.\n","        epsilon: float >= 0. Fuzz factor.\n","        decay: float >= 0. Learning rate decay over each update.\n","    # References\n","        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n","    \"\"\"\n","\n","    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, rho=0.95,\n","                 epsilon=1e-8, decay=0., **kwargs):\n","        super(AdamDelta, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.iterations = K.variable(0, dtype='int64', name='iterations')\n","            self.lr = K.variable(lr, name='lr')\n","            self.beta_1 = K.variable(beta_1, name='beta_1')\n","            self.beta_2 = K.variable(beta_2, name='beta_2')\n","            self.rho = rho\n","            self.decay = K.variable(decay, name='decay')\n","        self.epsilon = epsilon\n","        self.initial_decay = decay\n","\n","    #@interfaces.legacy_get_updates_support\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        shapes = [K.int_shape(p) for p in params]\n","        accumulators = [K.zeros(shape) for shape in shapes]\n","        delta_accumulators = [K.zeros(shape) for shape in shapes]\n","\n","        self.updates = [K.update_add(self.iterations, 1)]\n","\n","        lr = self.lr\n","        if self.initial_decay > 0:\n","            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n","                                                  K.dtype(self.decay))))\n","\n","        t = K.cast(self.iterations, K.floatx()) + 1\n","        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n","                     (1. - K.pow(self.beta_1, t)))\n","\n","        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","\n","        self.weights = [self.iterations] + ms + vs\n","\n","        for p, g, m, v, a, d_a in zip(params, grads, ms, vs, accumulators, delta_accumulators):\n","            # update accumulator\n","            new_a = self.rho * a + (1. - self.rho) * K.square(g)\n","            self.updates.append(K.update(a, new_a))\n","\n","            # use the new accumulator and the *old* delta_accumulator\n","            update = g * K.sqrt(d_a + self.epsilon) / K.sqrt(new_a + self.epsilon)\n","\n","            m_t = (self.beta_1 * m) + (1. - self.beta_1) * update\n","            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(update)\n","            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n","\n","            self.updates.append(K.update(m, m_t))\n","            self.updates.append(K.update(v, v_t))\n","            new_p = p_t\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'lr': float(K.get_value(self.lr)),\n","                  'beta_1': float(K.get_value(self.beta_1)),\n","                  'beta_2': float(K.get_value(self.beta_2)),\n","                  'decay': float(K.get_value(self.decay)),\n","                  'epsilon': self.epsilon}\n","        base_config = super(AdamDelta, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mTEP4sBb06FZ"},"source":["#### 3.2.4 Model creation"]},{"cell_type":"code","metadata":{"id":"C5lCM8Kwu3v_","executionInfo":{"status":"ok","timestamp":1614171095696,"user_tz":180,"elapsed":16347,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["\"\"\"THIS\"\"\"\n","#!/usr/bin/env python\n","\n","\"\"\" Needs update!\n","\"\"\"\n","\n","import numpy as np\n","import tensorflow as tf\n","try:\n","    from keras.optimizers import *\n","    from keras.models import Sequential, load_model, Model\n","    from keras.layers import *\n","    from keras.regularizers import *\n","    from keras import backend as K\n","\n","    K.set_image_data_format('channels_first')\n","except ImportError:\n","    from tensorflow.keras.optimizers import RMSprop, Nadam, Adadelta, Adam, Ftrl\n","    from tensorflow.keras.models import Sequential, load_model, Model\n","    from tensorflow.keras.layers import *\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","DENSES = {'dense': Dense,\n","          'noisy_dense_fg': NoisyDenseFG,\n","          'noisy_dense_ig': NoisyDenseIG,\n","          'simplified_noisy_dense': SimplifiedNoisyDense}\n","\n","def select_optimizer(optimizer):\n","    assert optimizer in {'Nadam',\n","                         'Adam-AMS',\n","                         'RMSprop',\n","                         'Adadelta',\n","                         'Adam',\n","                         'Ftrl'}, \"Optimizer should be RMSprop, Nadam, Adadelta, Adam or Ftrl.\"\n","\n","    if optimizer == 'Nadam':\n","        optimizer = Nadam()     \n","    elif optimizer == 'Adadelta':\n","        optimizer = Adadelta()\n","    elif optimizer == 'Adam':\n","        optimizer = Adam()\n","    elif optimizer == 'Adam-AMS':\n","        optimizer = Adam(amsgrad = True)           \n","    elif optimizer == 'Ftrl':\n","        optimizer = Ftrl()\n","    else:\n","        optimizer = RMSprop()\n","\n","    return optimizer\n","\n","def select_error(error):\n","    assert type(error) is str, \"Should use string to select error.\"\n","\n","    if error == 'huber_loss':\n","        error = tf.losses.huber_loss()\n","\n","    return error\n","\n","def CNN1(inputs):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","\n","    return model\n","\n","def CNN2(inputs):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = Flatten()(net)\n","\n","    return model\n","\n","def CNN3(inputs):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    net = Conv2D(32, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(64, (2, 2), activation = 'relu')(net)\n","    net = Conv2D(64, (1, 1), activation = 'relu')(net)\n","    net = Flatten()(net)\n","\n","    return net\n","\n","def CNN4(inputs):\n","    \"\"\"From @aqtq314 implementation's of the AlphaZero.\"\"\"\n","    l2const = 1e-4\n","    \n","    net = Conv2D(32, (3, 3), kernel_regularizer = l2(l2const))(inputs)\n","    net = Activation(\"relu\")(net)    \n","    net = Conv2D(64, (2, 2), kernel_regularizer = l2(l2const))(net)\n","    net = Activation(\"relu\")(net)    \n","    net = Conv2D(64, (1, 1), kernel_regularizer = l2(l2const))(net)\n","    net = Activation(\"relu\")(net)\n","    net = Flatten()(net)   \n","\n","    return net\n","\n","def create_cnn(cnn, inputs):\n","    if cnn == \"CNN1\":\n","        net = CNN1(inputs)\n","    elif cnn == \"CNN2\":\n","        net = CNN2(inputs)\n","    elif cnn == \"CNN4\":\n","        net = CNN4(inputs)        \n","    else:\n","        net = CNN3(inputs)\n","\n","    return net\n","\n","def create_model(optimizer, loss, stack, input_size, output_size,\n","                 dueling = False, cnn = \"CNN3\", dense_type = \"dense\"):\n","    optimizer = select_optimizer(optimizer)\n","#    loss = select_error(loss)\n","    inputs = Input(shape = (stack, input_size, input_size))\n","    net = create_cnn(cnn, inputs)\n","\n","    if dueling:\n","        advt = DENSES[dense_type](3136, activation = 'relu')(net)\n","        advt = DENSES[dense_type](output_size)(advt)\n","        value = DENSES[dense_type](3136, activation = 'relu')(net)\n","        value = DENSES[dense_type](1)(value)\n","\n","        # now to combine the two streams\n","        advt = Lambda(lambda advt: advt - tf.reduce_mean(advt, axis = -1,\n","                                                         keepdims = True))(advt)\n","        value = Lambda(lambda value: tf.tile(value, [1, output_size]))(value)\n","        final = Add()([value, advt])\n","    else:\n","        final = DENSES[dense_type](3136, activation = 'relu')(net)\n","        final = DENSES[dense_type](output_size)(final)\n","\n","    model = Model(inputs = inputs, outputs = final)\n","    model.compile(optimizer = optimizer, loss = loss)\n","    model.summary()\n","\n","    return model\n","\n","\n","class Networks(object):\n","\n","    @staticmethod\n","    def actor_network(input_shape, action_size, learning_rate):\n","        \"\"\"Actor Network for A2C\n","        \"\"\"\n","\n","        model = Sequential()\n","        model.add(Conv2D(32, (4, 4), input_shape=(input_shape), activation = 'relu'))\n","        model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","        model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","        model.add(Flatten())\n","        model.add(Dense(3136, activation = 'relu'))\n","        model.add(Dense(action_size))\n","\n","        optimizer = RMSprop()\n","        model.compile(loss = tf.losses.huber_loss(), optimizer = optimizer)\n","\n","        return model\n","\n","    @staticmethod\n","    def critic_network(input_shape, value_size, learning_rate):\n","        \"\"\"Critic Network for A2C\n","        \"\"\"\n","\n","        model = Sequential()\n","        model.add(Conv2D(32, (4, 4), input_shape=(input_shape), activation = 'relu'))\n","        model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","        model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","        model.add(Flatten())\n","        model.add(Dense(3136, activation = 'relu'))\n","        model.add(Dense(value_size, activation = 'linear'))\n","\n","        optimizer = RMSprop()\n","        model.compile(loss = tf.losses.huber_loss(), optimizer = optimizer)\n","\n","        return model\n"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wve-DuiS1xxW"},"source":["### 3.3 The Agent"]},{"cell_type":"code","metadata":{"id":"4NeLppYW1wqz","executionInfo":{"status":"ok","timestamp":1614171096719,"user_tz":180,"elapsed":17366,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms\n","----------\n","    * Simple Deep Q-network (DQN with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double Deep Q-network (Double DQN);\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling Deep Q-network (Dueling DQN);\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * Prioritized Experience Replay (PER);\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns (n-steps);\n","        Paper: https://arxiv.org/pdf/1703.01327\n","    * Noisy nets.\n","        Paper: https://arxiv.org/abs/1706.10295\n","\n","Arguments\n","----------\n","--load: 'file.h5'\n","    Load a previously trained model in '.h5' format.\n","--board_size: int, optional, default = 10\n","    Assign the size of the board.\n","--nb_frames: int, optional, default = 4\n","    Assign the number of frames per stack, default = 4.\n","--nb_actions: int, optional, default = 5\n","    Assign the number of actions possible.\n","--update_freq: int, optional, default = 0.001\n","    Whether to soft or hard update the target. Epochs or ammount of the update.\n","--visual: boolean, optional, default = False\n","    Select wheter or not to draw the game in pygame.\n","--double: boolean, optional, default = False\n","    Use a target network with double DQN logic.\n","--dueling: boolean, optional, default = False\n","    Whether to use dueling network logic, Q(s,a) = A + V.\n","--per: boolean, optional, default = False\n","    Use Prioritized Experience Replay (based on Sum Trees).\n","--local_state: boolean, optional, default = True\n","    Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from array import array\n","import random\n","\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes\n","    ----------\n","    memory: object\n","        Memory used in training. ExperienceReplay or PrioritizedExperienceReplay\n","    memory_size: int, optional, default = -1\n","        Capacity of the memory used.\n","    model: keras model\n","        The input model in Keras.\n","    target: keras model, optional, default = None\n","        The target model, used to calculade the fixed Q-targets.\n","    nb_frames: int, optional, default = 4\n","        Ammount of frames for each experience (sars).\n","    board_size: int, optional, default = 10\n","        Size of the board used.\n","    frames: list of experiences\n","        The buffer of frames, store sars experiences.\n","    per: boolean, optional, default = False\n","        Flag for PER usage.\n","    update_target_freq: int or float, default = 0.001\n","        Whether soft or hard updates occur. If < 1, soft updated target model.\n","    n_steps: int, optional, default = 1\n","        Size of the rewards buffer, to use Multi-step returns.\n","    \"\"\"\n","    def __init__(self, model, target = None, memory_size = -1, nb_frames = 4,\n","                 board_size = 10, per = False, update_target_freq = 0.01):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if per == 'per':\n","            self.memory = PrioritizedExperienceReplay(memory_size = memory_size)\n","        elif per == 'per_naive':\n","            self.memory = PrioritizedExperienceReplayNaive(memory_size = memory_size)\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.update_target_freq = update_target_freq\n","        #self.sess = sess # Needs fix, tf v2\n","        self.clear_frames()\n","        self.set_noise_list()\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def set_noise_list(self):\n","        \"\"\"Set a list of noise variables if NoisyNet is involved.\"\"\"\n","        self.noise_list = []\n","        self.simplified_noise_layers = []\n","\n","        for layer in self.model.layers:\n","            if type(layer) in {NoisyDenseFG, NoisyDenseIG}:\n","                self.noise_list.extend(layer.noise_list)\n","            if type(layer) in {SimplifiedNoisyDense}:\n","                self.simplified_noise_layers.append(layer)\n","\n","    def sample_noise(self):\n","        \"\"\"Resample noise variables in NoisyNet.\"\"\"\n","        # for noise in self.noise_list:\n","            # self.sess.run(noise.initializer) # Needs fix, tf v2\n","        for simplified_noise in self.simplified_noise_layers:\n","            simplified_noise.sample_noise()\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\n","\n","        Return\n","        ----------\n","        expanded_frames: list of experiences\n","            The buffer of frames, shape = (nb_frames, board_size, board_size)\n","        \"\"\"\n","        frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        expanded_frames = np.expand_dims(self.frames, 0)\n","        # expanded_frames = np.transpose(expanded_frames, [0, 3, 2, 1]) # NCHW to NHWC\n","\n","        return expanded_frames\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model_hard(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def transfer_weights(self):\n","        \"\"\"Transfer Weights from Model to Target at rate update_target_freq.\"\"\"\n","        model_weights = self.model.get_weights()\n","        target_weights = self.target.get_weights()\n","\n","        for i in range(len(model_weights)):\n","            target_weights[i] = (self.update_target_freq * model_weights[i]\n","                                 + ((1 - self.update_target_freq)\n","                                    * target_weights[i]))\n","\n","        self.target.set_weights(target_weights)\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, policy, value,\n","                      win_count, history_step, history_reward,\n","                      history_loss = None, verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    np.mean(history_size[-10:]),\n","                                    max(history_size[-10:]),\n","                                    np.mean(history_step[-10:]),\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}'  # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            if loss is not None:  # Print training performance\n","                text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                              + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                              + 'Mean loss - 100 episodes: {:.4f}')\n","                print(text_perf.format(history_loss[-1],\n","                                       history_loss[-1] / history_step[-1],\n","                                       np.mean(history_loss[-100:])))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   np.mean(history_step[-100:])))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + '{:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + '{:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\n","\n","        Return\n","        ----------\n","        loss: float\n","            Training loss of given batch.\n","        \"\"\"\n","        loss = 0.\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = (batch_size),\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions,\n","                                        n_steps = self.n_steps)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, optim_rounds = 1, policy = \"EpsGreedyQPolicy\",\n","              verbose = 1, n_steps = 1):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        if not hasattr(self, 'n_steps'):\n","            self.n_steps = n_steps  # Set attribute only once\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_loss = array('f')  # Holds all the losses\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        q_policy = self.select_policy(policy, eps, temp, nb_epoch, learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        # If optim_rounds is bigger than one, the model will keep optimizing\n","        # after the exploration, in turns of nb_epoch size.\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                    text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                                  + '| Loss: {:.4f}')\n","                    print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:  # Exploration and training\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    game.reset_game()\n","                    self.clear_frames()\n","\n","                    S = self.get_game_data(game)\n","\n","                    if n_steps > 1:  # Create multi-step returns buffer.\n","                        n_step_buffer = array('f')\n","\n","                    while not game.game_over:  # Main loop, until game_over\n","                        game.food_pos = game.generate_food()\n","                        self.sample_noise()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","                        game.play(action)\n","                        r = game.get_reward()\n","                        total_reward += r\n","\n","                        if n_steps > 1:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","\n","                                n_step_buffer.pop(0)\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience)  # Add to the memory\n","                        S = S_prime  # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe:  # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1  # Counter of wins for metrics\n","\n","                    if self.per:  # Advance beta, used in PER\n","                        self.memory.beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None:  # Update the target model\n","                        if self.update_target_freq >= 1: # Hard updates\n","                            if epoch % self.update_target_freq == 0:\n","                                self.update_target_model_hard()\n","                        elif self.update_target_freq < 1.:  # Soft updates\n","                            self.transfer_weights()\n","                    \n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch = epoch, nb_epoch = nb_epoch,\n","                                           history_size = history_size,\n","                                           history_loss = history_loss,\n","                                           history_step = history_step,\n","                                           history_reward = history_reward,\n","                                           policy = policy, value = value,\n","                                           win_count = win_count,\n","                                           verbose = verbose)\n","\n","    def test(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        q_policy = self.select_policy(policy, eps, temp, nb_epoch,\n","                                      learning_rate = 0.5)\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","\n","            if visual:\n","                game.create_window()\n","                previous_size = game.snake.length  # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","                elapsed = 0\n","\n","            while not game.game_over:\n","                if visual:\n","                    elapsed += game.fps.get_time()  # Get elapsed time since last call.\n","\n","                    if elapsed >= 60:\n","                        elapsed = 0\n","                        S = self.get_game_data(game)\n","                        action, value = q_policy.select_action(self.model, S,\n","                                                               epoch,\n","                                                               game.nb_actions)\n","                        game.play(action)\n","                        current_size = game.snake.length  # Update the body size\n","\n","                        if current_size > previous_size:\n","                            color_list = game.gradient([(42, 42, 42), (152, 152,\n","                                                                       152)],\n","                                                       current_size)\n","\n","                            previous_size = current_size\n","\n","                        game.draw(color_list)\n","\n","                    pygame.display.update()\n","                    game.fps.tick(120)  # Limit FPS to 100\n","                else:\n","                    S = self.get_game_data(game)\n","                    action, value = q_policy.select_action(self.model, S, epoch,\n","                                                           game.nb_actions)\n","                    game.play(action)\n","                    current_size = game.snake.length  # Update the body size\n","\n","                if game.game_over:\n","                    history_size.append(current_size)\n","                    history_step.append(game.step)\n","                    history_reward.append(game.get_reward())\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(history_size), np.max(history_size),\n","                      np.min(history_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(history_step), np.max(history_step),\n","                      np.min(history_step)))\n","        print(\"Mean rewards: {} | Biggest reward: {} | Smallest reward: {}\"\\\n","              .format(np.mean(history_reward), np.max(history_reward),\n","                      np.min(history_reward)))\n","\n","    @staticmethod\n","    def select_policy(policy, eps, temp, nb_epoch, learning_rate):\n","        # Select exploration policy. EpsGreedyQPolicy runs faster, but takes\n","        # longer to converge. BoltzmannGumbelQPolicy is the slowest, but\n","        # converge really fast (0.1 * nb_epoch used in EpsGreedyQPolicy).\n","        # BoltzmannQPolicy is in the middle.\n","        if policy == 'BoltzmannQPolicy':\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch\n","                                                          * learning_rate)\n","        elif policy == 'BoltzmannGumbelQPolicy':\n","            q_policy = BoltzmannGumbelQPolicy()\n","        elif policy == 'GreedyQPolicy':\n","            q_policy = GreedyQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch\n","                                                        * learning_rate)\n","\n","        return q_policy\n"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6Dm1Nvn14oq"},"source":["## 4. Time to train/test"]},{"cell_type":"code","metadata":{"id":"NKk1Avv8HsNU","executionInfo":{"status":"ok","timestamp":1614171096725,"user_tz":180,"elapsed":17354,"user":{"displayName":"DamÃ¡sio Palmas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi3h0z8B42F1MC7aher_B8a4iAYiDGbOLgWqHvQ=s64","userId":"15339542767479210112"}}},"source":["from keras.optimizers import RMSprop, Nadam\n","from keras.models import Sequential, load_model, Model\n","from keras import backend as K\n","\n","K.set_image_data_format('channels_first') \n","\n","# Parameters\n","PLAYER = 'ROBOT'\n","BOARD_SIZE = 10\n","NB_FRAMES = 10\n","LOCAL_STATE = True\n","RELATIVE_POS = True\n","OPTIMIZER = 'RMSprop'\n","LOSS = 'kld'\n","DUELING = False\n","CNN = 'CNN3'\n","DENSE_TYPE = 'dense'\n","MEMORY_SIZE = -1\n","PER = False\n","UPDATE_TARGET_FREQ = 0.01\n","BATCH_SIZE = 64\n","NB_EPOCH = 10000\n","GAMMA = 0.95\n","N_STEPS = 3\n","POLICY = 'EpsGreedyQPolicy'"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1_GNLvk1-rC","outputId":"31384cf8-27e7-4bf1-f077-234eca2a283e"},"source":["# Train the model\n","GAME = Game(player = PLAYER, board_size = BOARD_SIZE,\n","                        local_state = LOCAL_STATE, relative_pos = RELATIVE_POS)\n","\n","\n","MODEL = create_model(optimizer = OPTIMIZER, loss = LOSS,\n","                     stack = NB_FRAMES, input_size = BOARD_SIZE,\n","                     output_size = GAME.nb_actions, dueling = DUELING, \n","                     cnn = CNN, dense_type = DENSE_TYPE)\n","                \n","TARGET = None\n","AGENT = Agent(model = MODEL, target = TARGET, memory_size = MEMORY_SIZE,\n","              nb_frames = NB_FRAMES, board_size = BOARD_SIZE,\n","              per = PER, update_target_freq = UPDATE_TARGET_FREQ)\n","AGENT.train(GAME, batch_size = BATCH_SIZE, nb_epoch = NB_EPOCH,\n","             gamma = GAMMA, n_steps = N_STEPS, policy = POLICY)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 10, 10, 10)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 32, 8, 8)          2912      \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 64, 7, 7)          8256      \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 64, 7, 7)          4160      \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 3136)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3136)              9837632   \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 9411      \n","=================================================================\n","Total params: 9,862,371\n","Trainable params: 9,862,371\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Epoch: 010/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.3 | Wins: 0 | 0.0%\n","Epoch: 020/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.5 | Wins: 0 | 0.0%\n","Epoch: 030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.7 | Wins: 0 | 0.0%\n","Epoch: 040/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 15.9 | Wins: 2 | 5.0%\n","Epoch: 050/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 3 | 6.0%\n","Epoch: 060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.1 | Wins: 4 | 6.7%\n","Epoch: 070/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 21.0 | Wins: 5 | 7.1%\n","Epoch: 080/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.6 | Wins: 7 | 8.8%\n","Epoch: 090/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.5 | Wins: 8 | 8.9%\n","Epoch: 100/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 9 | 9.0%\n","Epoch: 110/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.8 | Wins: 10 | 9.1%\n","Epoch: 120/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.8 | Wins: 11 | 9.2%\n","Epoch: 130/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.0 | Wins: 11 | 8.5%\n","Epoch: 140/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 22.7 | Wins: 12 | 8.6%\n","Epoch: 150/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.8 | Wins: 12 | 8.0%\n","Epoch: 160/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 19.0 | Wins: 13 | 8.1%\n","Epoch: 170/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 16 | 9.4%\n","Epoch: 180/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.1 | Wins: 18 | 10.0%\n","Epoch: 190/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 20.4 | Wins: 18 | 9.5%\n","Epoch: 200/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 21 | 10.5%\n","Epoch: 210/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.1 | Wins: 23 | 11.0%\n","Epoch: 220/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.8 | Wins: 23 | 10.5%\n","Epoch: 230/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.3 | Wins: 24 | 10.4%\n","Epoch: 240/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 11.5 | Wins: 25 | 10.4%\n","Epoch: 250/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.1 | Wins: 25 | 10.0%\n","Epoch: 260/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.8 | Wins: 25 | 9.6%\n","Epoch: 270/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 17.5 | Wins: 26 | 9.6%\n","Epoch: 280/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.8 | Wins: 28 | 10.0%\n","Epoch: 290/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 19.5 | Wins: 31 | 10.7%\n","Epoch: 300/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.1 | Wins: 31 | 10.3%\n","Epoch: 310/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 33 | 10.6%\n","Epoch: 320/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 23.4 | Wins: 35 | 10.9%\n","Epoch: 330/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.1 | Wins: 35 | 10.6%\n","Epoch: 340/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.8 | Wins: 37 | 10.9%\n","Epoch: 350/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 38 | 10.9%\n","Epoch: 360/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.8 | Wins: 38 | 10.6%\n","Epoch: 370/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 41 | 11.1%\n","Epoch: 380/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 45 | 11.8%\n","Epoch: 390/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.7 | Wins: 47 | 12.1%\n","Epoch: 400/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 49 | 12.2%\n","Epoch: 410/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.3 | Wins: 49 | 12.0%\n","Epoch: 420/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.2 | Wins: 50 | 11.9%\n","Epoch: 430/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 19.8 | Wins: 51 | 11.9%\n","Epoch: 440/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 12.0 | Wins: 53 | 12.0%\n","Epoch: 450/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 54 | 12.0%\n","Epoch: 460/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.3 | Wins: 56 | 12.2%\n","Epoch: 470/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 23.5 | Wins: 57 | 12.1%\n","Epoch: 480/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 18.1 | Wins: 61 | 12.7%\n","Epoch: 490/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 26.1 | Wins: 62 | 12.7%\n","Epoch: 500/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.8 | Wins: 63 | 12.6%\n","Epoch: 510/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 27.5 | Wins: 67 | 13.1%\n","Epoch: 520/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.4 | Wins: 68 | 13.1%\n","Epoch: 530/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.5 | Wins: 68 | 12.8%\n","Epoch: 540/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 21.6 | Wins: 69 | 12.8%\n","Epoch: 550/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.1 | Wins: 69 | 12.5%\n","Epoch: 560/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.4 | Wins: 69 | 12.3%\n","Epoch: 570/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 72 | 12.6%\n","Epoch: 580/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.5 | Wins: 72 | 12.4%\n","Epoch: 590/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 73 | 12.4%\n","Epoch: 600/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.3 | Wins: 75 | 12.5%\n","Epoch: 610/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 22.4 | Wins: 77 | 12.6%\n","Epoch: 620/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 12.6 | Wins: 78 | 12.6%\n","Epoch: 630/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 13.6 | Wins: 79 | 12.5%\n","Epoch: 640/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.7 | Wins: 79 | 12.3%\n","Epoch: 650/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.6 | Wins: 81 | 12.5%\n","Epoch: 660/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.8 | Wins: 81 | 12.3%\n","Epoch: 670/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 22.8 | Wins: 85 | 12.7%\n","Epoch: 680/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.6 | Wins: 85 | 12.5%\n","Epoch: 690/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 16.6 | Wins: 88 | 12.8%\n","Epoch: 700/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.8 | Wins: 88 | 12.6%\n","Epoch: 710/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 89 | 12.5%\n","Epoch: 720/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 9.4 | Wins: 90 | 12.5%\n","Epoch: 730/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.4 | Wins: 91 | 12.5%\n","Epoch: 740/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.6 | Wins: 92 | 12.4%\n","Epoch: 750/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.2 | Wins: 92 | 12.3%\n","Epoch: 760/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 23.0 | Wins: 93 | 12.2%\n","Epoch: 770/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 95 | 12.3%\n","Epoch: 780/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.1 | Wins: 95 | 12.2%\n","Epoch: 790/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.5 | Wins: 95 | 12.0%\n","Epoch: 800/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.0 | Wins: 97 | 12.1%\n","Epoch: 810/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.3 | Wins: 99 | 12.2%\n","Epoch: 820/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 20.1 | Wins: 100 | 12.2%\n","Epoch: 830/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 24.8 | Wins: 104 | 12.5%\n","Epoch: 840/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.0 | Wins: 105 | 12.5%\n","Epoch: 850/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.2 | Wins: 106 | 12.5%\n","Epoch: 860/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 27.5 | Wins: 108 | 12.6%\n","Epoch: 870/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.3 | Wins: 109 | 12.5%\n","Epoch: 880/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.1 | Wins: 111 | 12.6%\n","Epoch: 890/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 19.9 | Wins: 112 | 12.6%\n","Epoch: 900/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 18.2 | Wins: 114 | 12.7%\n","Epoch: 910/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 116 | 12.7%\n","Epoch: 920/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 19.5 | Wins: 119 | 12.9%\n","Epoch: 930/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 122 | 13.1%\n","Epoch: 940/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.3 | Wins: 122 | 13.0%\n","Epoch: 950/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 13.9 | Wins: 126 | 13.3%\n","Epoch: 960/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 22.1 | Wins: 129 | 13.4%\n","Epoch: 970/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.2 | Wins: 129 | 13.3%\n","Epoch: 980/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 130 | 13.3%\n","Epoch: 990/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 12.5 | Wins: 132 | 13.3%\n","Epoch: 1000/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 133 | 13.3%\n","Epoch: 1010/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.2 | Wins: 133 | 13.2%\n","Epoch: 1020/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.1 | Wins: 133 | 13.0%\n","Epoch: 1030/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 15.2 | Wins: 135 | 13.1%\n","Epoch: 1040/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.2 | Wins: 135 | 13.0%\n","Epoch: 1050/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.9 | Wins: 135 | 12.9%\n","Epoch: 1060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.7 | Wins: 136 | 12.8%\n","Epoch: 1070/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.9 | Wins: 137 | 12.8%\n","Epoch: 1080/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.9 | Wins: 138 | 12.8%\n","Epoch: 1090/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 140 | 12.8%\n","Epoch: 1100/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 17.5 | Wins: 141 | 12.8%\n","Epoch: 1110/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.9 | Wins: 141 | 12.7%\n","Epoch: 1120/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.4 | Wins: 141 | 12.6%\n","Epoch: 1130/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 14.4 | Wins: 145 | 12.8%\n","Epoch: 1140/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 17.8 | Wins: 149 | 13.1%\n","Epoch: 1150/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.6 | Wins: 149 | 13.0%\n","Epoch: 1160/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.0 | Wins: 150 | 12.9%\n","Epoch: 1170/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.8 | Wins: 151 | 12.9%\n","Epoch: 1180/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.7 | Wins: 153 | 13.0%\n","Epoch: 1190/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.2 | Wins: 155 | 13.0%\n","Epoch: 1200/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.9 | Wins: 155 | 12.9%\n","Epoch: 1210/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.8 | Wins: 155 | 12.8%\n","Epoch: 1220/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 156 | 12.8%\n","Epoch: 1230/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 20.9 | Wins: 157 | 12.8%\n","Epoch: 1240/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.8 | Wins: 158 | 12.7%\n","Epoch: 1250/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 160 | 12.8%\n","Epoch: 1260/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.5 | Wins: 160 | 12.7%\n","Epoch: 1270/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 19.0 | Wins: 161 | 12.7%\n","Epoch: 1280/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.3 | Wins: 163 | 12.7%\n","Epoch: 1290/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 16.4 | Wins: 166 | 12.9%\n","Epoch: 1300/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.3 | Wins: 166 | 12.8%\n","Epoch: 1310/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 19.1 | Wins: 167 | 12.7%\n","Epoch: 1320/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 168 | 12.7%\n","Epoch: 1330/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.8 | Wins: 170 | 12.8%\n","Epoch: 1340/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.4 | Wins: 170 | 12.7%\n","Epoch: 1350/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 171 | 12.7%\n","Epoch: 1360/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.9 | Wins: 171 | 12.6%\n","Epoch: 1370/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.2 | Wins: 171 | 12.5%\n","Epoch: 1380/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.5 | Wins: 173 | 12.5%\n","Epoch: 1390/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 20.3 | Wins: 175 | 12.6%\n","Epoch: 1400/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.8 | Wins: 177 | 12.6%\n","Epoch: 1410/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.9 | Wins: 180 | 12.8%\n","Epoch: 1420/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 10.2 | Wins: 183 | 12.9%\n","Epoch: 1430/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.0 | Wins: 183 | 12.8%\n","Epoch: 1440/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 184 | 12.8%\n","Epoch: 1450/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.6 | Wins: 185 | 12.8%\n","Epoch: 1460/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.8 | Wins: 186 | 12.7%\n","Epoch: 1470/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 21.3 | Wins: 188 | 12.8%\n","Epoch: 1480/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.3 | Wins: 189 | 12.8%\n","Epoch: 1490/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 14.7 | Wins: 190 | 12.8%\n","Epoch: 1500/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 21.4 | Wins: 190 | 12.7%\n","Epoch: 1510/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 19.8 | Wins: 190 | 12.6%\n","Epoch: 1520/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.8 | Wins: 190 | 12.5%\n","Epoch: 1530/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.7 | Wins: 191 | 12.5%\n","Epoch: 1540/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 30.3 | Wins: 194 | 12.6%\n","Epoch: 1550/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 19.2 | Wins: 197 | 12.7%\n","Epoch: 1560/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 198 | 12.7%\n","Epoch: 1570/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.1 | Wins: 200 | 12.7%\n","Epoch: 1580/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.5 | Wins: 200 | 12.7%\n","Epoch: 1590/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 202 | 12.7%\n","Epoch: 1600/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 13.7 | Wins: 204 | 12.8%\n","Epoch: 1610/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.5 | Wins: 205 | 12.7%\n","Epoch: 1620/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.6 | Wins: 207 | 12.8%\n","Epoch: 1630/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 27.4 | Wins: 211 | 12.9%\n","Epoch: 1640/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.0 | Wins: 212 | 12.9%\n","Epoch: 1650/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 14.9 | Wins: 213 | 12.9%\n","Epoch: 1660/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 8.3 | Wins: 215 | 13.0%\n","Epoch: 1670/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.9 | Wins: 218 | 13.1%\n","Epoch: 1680/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.7 | Wins: 219 | 13.0%\n","Epoch: 1690/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 220 | 13.0%\n","Epoch: 1700/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.3 | Wins: 220 | 12.9%\n","Epoch: 1710/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.1 | Wins: 221 | 12.9%\n","Epoch: 1720/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 19.3 | Wins: 222 | 12.9%\n","Epoch: 1730/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.2 | Wins: 223 | 12.9%\n","Epoch: 1740/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 224 | 12.9%\n","Epoch: 1750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.5 | Wins: 225 | 12.9%\n","Epoch: 1760/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.4 | Wins: 225 | 12.8%\n","Epoch: 1770/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 10.3 | Wins: 227 | 12.8%\n","Epoch: 1780/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.5 | Wins: 228 | 12.8%\n","Epoch: 1790/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 25.1 | Wins: 230 | 12.8%\n","Epoch: 1800/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.4 | Wins: 231 | 12.8%\n","Epoch: 1810/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.4 | Wins: 233 | 12.9%\n","Epoch: 1820/10000 | Mean size 10: 3.5 | Longest 10: 006 | Mean steps 10: 11.0 | Wins: 236 | 13.0%\n","Epoch: 1830/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.3 | Wins: 237 | 13.0%\n","Epoch: 1840/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.0 | Wins: 237 | 12.9%\n","Epoch: 1850/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 240 | 13.0%\n","Epoch: 1860/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 19.6 | Wins: 243 | 13.1%\n","Epoch: 1870/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 9.2 | Wins: 245 | 13.1%\n","Epoch: 1880/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 246 | 13.1%\n","Epoch: 1890/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.9 | Wins: 248 | 13.1%\n","Epoch: 1900/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.2 | Wins: 250 | 13.2%\n","Epoch: 1910/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.9 | Wins: 250 | 13.1%\n","Epoch: 1920/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 20.1 | Wins: 253 | 13.2%\n","Epoch: 1930/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.7 | Wins: 253 | 13.1%\n","Epoch: 1940/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 256 | 13.2%\n","Epoch: 1950/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.8 | Wins: 257 | 13.2%\n","Epoch: 1960/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 16.0 | Wins: 260 | 13.3%\n","Epoch: 1970/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.9 | Wins: 262 | 13.3%\n","Epoch: 1980/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 25.0 | Wins: 263 | 13.3%\n","Epoch: 1990/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.4 | Wins: 263 | 13.2%\n","Epoch: 2000/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 13.9 | Wins: 265 | 13.2%\n","Epoch: 2010/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.8 | Wins: 265 | 13.2%\n","Epoch: 2020/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 19.0 | Wins: 265 | 13.1%\n","Epoch: 2030/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.1 | Wins: 266 | 13.1%\n","Epoch: 2040/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.2 | Wins: 266 | 13.0%\n","Epoch: 2050/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.4 | Wins: 266 | 13.0%\n","Epoch: 2060/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 19.0 | Wins: 266 | 12.9%\n","Epoch: 2070/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 17.2 | Wins: 269 | 13.0%\n","Epoch: 2080/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 270 | 13.0%\n","Epoch: 2090/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 12.8 | Wins: 273 | 13.1%\n","Epoch: 2100/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 21.8 | Wins: 275 | 13.1%\n","Epoch: 2110/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 22.8 | Wins: 276 | 13.1%\n","Epoch: 2120/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 18.1 | Wins: 278 | 13.1%\n","Epoch: 2130/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 279 | 13.1%\n","Epoch: 2140/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 14.2 | Wins: 281 | 13.1%\n","Epoch: 2150/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.3 | Wins: 284 | 13.2%\n","Epoch: 2160/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.4 | Wins: 284 | 13.1%\n","Epoch: 2170/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.0 | Wins: 286 | 13.2%\n","Epoch: 2180/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.3 | Wins: 286 | 13.1%\n","Epoch: 2190/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 287 | 13.1%\n","Epoch: 2200/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.7 | Wins: 288 | 13.1%\n","Epoch: 2210/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.9 | Wins: 288 | 13.0%\n","Epoch: 2220/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 24.3 | Wins: 290 | 13.1%\n","Epoch: 2230/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.3 | Wins: 290 | 13.0%\n","Epoch: 2240/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 292 | 13.0%\n","Epoch: 2250/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 17.7 | Wins: 294 | 13.1%\n","Epoch: 2260/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 295 | 13.1%\n","Epoch: 2270/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.5 | Wins: 298 | 13.1%\n","Epoch: 2280/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 300 | 13.2%\n","Epoch: 2290/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 20.5 | Wins: 300 | 13.1%\n","Epoch: 2300/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 17.7 | Wins: 303 | 13.2%\n","Epoch: 2310/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.6 | Wins: 304 | 13.2%\n","Epoch: 2320/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.3 | Wins: 304 | 13.1%\n","Epoch: 2330/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.2 | Wins: 307 | 13.2%\n","Epoch: 2340/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.3 | Wins: 307 | 13.1%\n","Epoch: 2350/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.7 | Wins: 309 | 13.1%\n","Epoch: 2360/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.7 | Wins: 310 | 13.1%\n","Epoch: 2370/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 311 | 13.1%\n","Epoch: 2380/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.7 | Wins: 311 | 13.1%\n","Epoch: 2390/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 19.2 | Wins: 314 | 13.1%\n","Epoch: 2400/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 22.7 | Wins: 315 | 13.1%\n","Epoch: 2410/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 21.4 | Wins: 316 | 13.1%\n","Epoch: 2420/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 19.6 | Wins: 319 | 13.2%\n","Epoch: 2430/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 23.6 | Wins: 322 | 13.3%\n","Epoch: 2440/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 323 | 13.2%\n","Epoch: 2450/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.6 | Wins: 325 | 13.3%\n","Epoch: 2460/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 328 | 13.3%\n","Epoch: 2470/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 329 | 13.3%\n","Epoch: 2480/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.5 | Wins: 332 | 13.4%\n","Epoch: 2490/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 334 | 13.4%\n","Epoch: 2500/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 21.1 | Wins: 336 | 13.4%\n","Epoch: 2510/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 23.4 | Wins: 338 | 13.5%\n","Epoch: 2520/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 20.5 | Wins: 338 | 13.4%\n","Epoch: 2530/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.1 | Wins: 340 | 13.4%\n","Epoch: 2540/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.1 | Wins: 342 | 13.5%\n","Epoch: 2550/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.7 | Wins: 343 | 13.5%\n","Epoch: 2560/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 344 | 13.4%\n","Epoch: 2570/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 17.6 | Wins: 347 | 13.5%\n","Epoch: 2580/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.1 | Wins: 347 | 13.4%\n","Epoch: 2590/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.5 | Wins: 348 | 13.4%\n","Epoch: 2600/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.7 | Wins: 350 | 13.5%\n","Epoch: 2610/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.5 | Wins: 352 | 13.5%\n","Epoch: 2620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.8 | Wins: 353 | 13.5%\n","Epoch: 2630/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.0 | Wins: 353 | 13.4%\n","Epoch: 2640/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 356 | 13.5%\n","Epoch: 2650/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.3 | Wins: 356 | 13.4%\n","Epoch: 2660/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 19.8 | Wins: 356 | 13.4%\n","Epoch: 2670/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 29.2 | Wins: 358 | 13.4%\n","Epoch: 2680/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 359 | 13.4%\n","Epoch: 2690/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 18.7 | Wins: 362 | 13.5%\n","Epoch: 2700/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 16.9 | Wins: 364 | 13.5%\n","Epoch: 2710/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 368 | 13.6%\n","Epoch: 2720/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.5 | Wins: 369 | 13.6%\n","Epoch: 2730/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 25.1 | Wins: 371 | 13.6%\n","Epoch: 2740/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 24.7 | Wins: 373 | 13.6%\n","Epoch: 2750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 23.5 | Wins: 374 | 13.6%\n","Epoch: 2760/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 26.1 | Wins: 375 | 13.6%\n","Epoch: 2770/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.4 | Wins: 376 | 13.6%\n","Epoch: 2780/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.6 | Wins: 377 | 13.6%\n","Epoch: 2790/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 27.2 | Wins: 377 | 13.5%\n","Epoch: 2800/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 378 | 13.5%\n","Epoch: 2810/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 24.4 | Wins: 380 | 13.5%\n","Epoch: 2820/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.8 | Wins: 380 | 13.5%\n","Epoch: 2830/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 23.8 | Wins: 385 | 13.6%\n","Epoch: 2840/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 24.3 | Wins: 387 | 13.6%\n","Epoch: 2850/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 18.1 | Wins: 387 | 13.6%\n","Epoch: 2860/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.3 | Wins: 389 | 13.6%\n","Epoch: 2870/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 19.4 | Wins: 389 | 13.6%\n","Epoch: 2880/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 18.5 | Wins: 389 | 13.5%\n","Epoch: 2890/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 21.2 | Wins: 392 | 13.6%\n","Epoch: 2900/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 17.4 | Wins: 395 | 13.6%\n","Epoch: 2910/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.3 | Wins: 397 | 13.6%\n","Epoch: 2920/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.9 | Wins: 399 | 13.7%\n","Epoch: 2930/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 401 | 13.7%\n","Epoch: 2940/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.9 | Wins: 401 | 13.6%\n","Epoch: 2950/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 27.5 | Wins: 402 | 13.6%\n","Epoch: 2960/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 20.4 | Wins: 402 | 13.6%\n","Epoch: 2970/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.6 | Wins: 403 | 13.6%\n","Epoch: 2980/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 19.4 | Wins: 403 | 13.5%\n","Epoch: 2990/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.6 | Wins: 405 | 13.5%\n","Epoch: 3000/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 21.8 | Wins: 408 | 13.6%\n","Epoch: 3010/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.5 | Wins: 410 | 13.6%\n","Epoch: 3020/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.8 | Wins: 412 | 13.6%\n","Epoch: 3030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.6 | Wins: 412 | 13.6%\n","Epoch: 3040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 30.4 | Wins: 413 | 13.6%\n","Epoch: 3050/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 22.0 | Wins: 414 | 13.6%\n","Epoch: 3060/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 17.4 | Wins: 417 | 13.6%\n","Epoch: 3070/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.4 | Wins: 419 | 13.6%\n","Epoch: 3080/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.6 | Wins: 421 | 13.7%\n","Epoch: 3090/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 9.1 | Wins: 424 | 13.7%\n","Epoch: 3100/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 25.9 | Wins: 428 | 13.8%\n","Epoch: 3110/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 25.9 | Wins: 431 | 13.9%\n","Epoch: 3120/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.5 | Wins: 432 | 13.8%\n","Epoch: 3130/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 31.8 | Wins: 434 | 13.9%\n","Epoch: 3140/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.7 | Wins: 436 | 13.9%\n","Epoch: 3150/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.4 | Wins: 436 | 13.8%\n","Epoch: 3160/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 27.6 | Wins: 439 | 13.9%\n","Epoch: 3170/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.7 | Wins: 441 | 13.9%\n","Epoch: 3180/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.9 | Wins: 441 | 13.9%\n","Epoch: 3190/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.4 | Wins: 443 | 13.9%\n","Epoch: 3200/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 29.9 | Wins: 446 | 13.9%\n","Epoch: 3210/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 26.3 | Wins: 449 | 14.0%\n","Epoch: 3220/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.2 | Wins: 450 | 14.0%\n","Epoch: 3230/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 25.2 | Wins: 453 | 14.0%\n","Epoch: 3240/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 23.6 | Wins: 453 | 14.0%\n","Epoch: 3250/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 29.0 | Wins: 454 | 14.0%\n","Epoch: 3260/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 31.1 | Wins: 456 | 14.0%\n","Epoch: 3270/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 20.9 | Wins: 460 | 14.1%\n","Epoch: 3280/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 18.7 | Wins: 460 | 14.0%\n","Epoch: 3290/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 35.4 | Wins: 462 | 14.0%\n","Epoch: 3300/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 20.9 | Wins: 464 | 14.1%\n","Epoch: 3310/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 23.6 | Wins: 466 | 14.1%\n","Epoch: 3320/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 25.8 | Wins: 471 | 14.2%\n","Epoch: 3330/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 22.0 | Wins: 472 | 14.2%\n","Epoch: 3340/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 27.5 | Wins: 475 | 14.2%\n","Epoch: 3350/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 29.9 | Wins: 477 | 14.2%\n","Epoch: 3360/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 22.9 | Wins: 478 | 14.2%\n","Epoch: 3370/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 23.4 | Wins: 480 | 14.2%\n","Epoch: 3380/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 16.2 | Wins: 484 | 14.3%\n","Epoch: 3390/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.7 | Wins: 485 | 14.3%\n","Epoch: 3400/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.9 | Wins: 485 | 14.3%\n","Epoch: 3410/10000 | Mean size 10: 3.5 | Longest 10: 006 | Mean steps 10: 18.1 | Wins: 488 | 14.3%\n","Epoch: 3420/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 30.5 | Wins: 489 | 14.3%\n","Epoch: 3430/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 33.7 | Wins: 490 | 14.3%\n","Epoch: 3440/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 26.0 | Wins: 491 | 14.3%\n","Epoch: 3450/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 26.3 | Wins: 492 | 14.3%\n","Epoch: 3460/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 20.2 | Wins: 492 | 14.2%\n","Epoch: 3470/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.8 | Wins: 492 | 14.2%\n","Epoch: 3480/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 15.1 | Wins: 494 | 14.2%\n","Epoch: 3490/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 21.1 | Wins: 496 | 14.2%\n","Epoch: 3500/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 21.6 | Wins: 497 | 14.2%\n","Epoch: 3510/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 22.6 | Wins: 499 | 14.2%\n","Epoch: 3520/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 24.9 | Wins: 502 | 14.3%\n","Epoch: 3530/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.4 | Wins: 504 | 14.3%\n","Epoch: 3540/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 23.7 | Wins: 505 | 14.3%\n","Epoch: 3550/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 21.1 | Wins: 507 | 14.3%\n","Epoch: 3560/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 18.1 | Wins: 507 | 14.2%\n","Epoch: 3570/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.4 | Wins: 507 | 14.2%\n","Epoch: 3580/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 26.3 | Wins: 511 | 14.3%\n","Epoch: 3590/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.7 | Wins: 513 | 14.3%\n","Epoch: 3600/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.8 | Wins: 514 | 14.3%\n","Epoch: 3610/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 15.0 | Wins: 517 | 14.3%\n","Epoch: 3620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 28.5 | Wins: 518 | 14.3%\n","Epoch: 3630/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 19.2 | Wins: 519 | 14.3%\n","Epoch: 3640/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 27.1 | Wins: 520 | 14.3%\n","Epoch: 3650/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 29.0 | Wins: 521 | 14.3%\n","Epoch: 3660/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 22.3 | Wins: 525 | 14.3%\n","Epoch: 3670/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 528 | 14.4%\n","Epoch: 3680/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 37.2 | Wins: 529 | 14.4%\n","Epoch: 3690/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 33.4 | Wins: 530 | 14.4%\n","Epoch: 3700/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 19.8 | Wins: 533 | 14.4%\n","Epoch: 3710/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.0 | Wins: 534 | 14.4%\n","Epoch: 3720/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.3 | Wins: 536 | 14.4%\n","Epoch: 3730/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 25.1 | Wins: 540 | 14.5%\n","Epoch: 3740/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 26.3 | Wins: 542 | 14.5%\n","Epoch: 3750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 24.1 | Wins: 543 | 14.5%\n","Epoch: 3760/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 26.0 | Wins: 544 | 14.5%\n","Epoch: 3770/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 26.1 | Wins: 547 | 14.5%\n","Epoch: 3780/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 26.6 | Wins: 549 | 14.5%\n","Epoch: 3790/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.9 | Wins: 549 | 14.5%\n","Epoch: 3800/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 34.0 | Wins: 550 | 14.5%\n","Epoch: 3810/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 27.8 | Wins: 552 | 14.5%\n","Epoch: 3820/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 22.4 | Wins: 552 | 14.5%\n","Epoch: 3830/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 28.0 | Wins: 553 | 14.4%\n","Epoch: 3840/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 35.3 | Wins: 555 | 14.5%\n","Epoch: 3850/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 35.8 | Wins: 559 | 14.5%\n","Epoch: 3860/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 560 | 14.5%\n","Epoch: 3870/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 18.2 | Wins: 562 | 14.5%\n","Epoch: 3880/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 33.3 | Wins: 562 | 14.5%\n","Epoch: 3890/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 43.6 | Wins: 564 | 14.5%\n","Epoch: 3900/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 25.8 | Wins: 566 | 14.5%\n","Epoch: 3910/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 23.9 | Wins: 567 | 14.5%\n","Epoch: 3920/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 35.4 | Wins: 569 | 14.5%\n","Epoch: 3930/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 21.3 | Wins: 571 | 14.5%\n","Epoch: 3940/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 21.2 | Wins: 574 | 14.6%\n","Epoch: 3950/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 25.2 | Wins: 575 | 14.6%\n","Epoch: 3960/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 37.0 | Wins: 576 | 14.5%\n","Epoch: 3970/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 49.3 | Wins: 579 | 14.6%\n","Epoch: 3980/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 24.4 | Wins: 579 | 14.5%\n","Epoch: 3990/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 32.4 | Wins: 581 | 14.6%\n","Epoch: 4000/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 28.9 | Wins: 584 | 14.6%\n","Epoch: 4010/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.1 | Wins: 585 | 14.6%\n","Epoch: 4020/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.0 | Wins: 585 | 14.6%\n","Epoch: 4030/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 28.8 | Wins: 586 | 14.5%\n","Epoch: 4040/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 30.2 | Wins: 589 | 14.6%\n","Epoch: 4050/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 19.9 | Wins: 589 | 14.5%\n","Epoch: 4060/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.1 | Wins: 591 | 14.6%\n","Epoch: 4070/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 23.7 | Wins: 591 | 14.5%\n","Epoch: 4080/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 29.8 | Wins: 593 | 14.5%\n","Epoch: 4090/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 29.8 | Wins: 596 | 14.6%\n","Epoch: 4100/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 30.7 | Wins: 597 | 14.6%\n","Epoch: 4110/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 42.7 | Wins: 598 | 14.5%\n","Epoch: 4120/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 32.7 | Wins: 600 | 14.6%\n","Epoch: 4130/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 21.9 | Wins: 600 | 14.5%\n","Epoch: 4140/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 37.3 | Wins: 602 | 14.5%\n","Epoch: 4150/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 46.0 | Wins: 605 | 14.6%\n","Epoch: 4160/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.0 | Wins: 605 | 14.5%\n","Epoch: 4170/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 27.9 | Wins: 607 | 14.6%\n","Epoch: 4180/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 19.2 | Wins: 608 | 14.5%\n","Epoch: 4190/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 22.5 | Wins: 608 | 14.5%\n","Epoch: 4200/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 33.1 | Wins: 608 | 14.5%\n","Epoch: 4210/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 23.2 | Wins: 610 | 14.5%\n","Epoch: 4220/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 22.8 | Wins: 610 | 14.5%\n","Epoch: 4230/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 34.2 | Wins: 610 | 14.4%\n","Epoch: 4240/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 30.0 | Wins: 611 | 14.4%\n","Epoch: 4250/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 41.9 | Wins: 617 | 14.5%\n","Epoch: 4260/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 46.0 | Wins: 618 | 14.5%\n","Epoch: 4270/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 28.6 | Wins: 619 | 14.5%\n","Epoch: 4280/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 26.1 | Wins: 620 | 14.5%\n","Epoch: 4290/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 52.5 | Wins: 623 | 14.5%\n","Epoch: 4300/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 28.7 | Wins: 623 | 14.5%\n","Epoch: 4310/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 30.3 | Wins: 623 | 14.5%\n","Epoch: 4320/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 33.4 | Wins: 623 | 14.4%\n","Epoch: 4330/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 45.8 | Wins: 625 | 14.4%\n","Epoch: 4340/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 52.6 | Wins: 626 | 14.4%\n","Epoch: 4350/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 26.8 | Wins: 627 | 14.4%\n","Epoch: 4360/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 34.3 | Wins: 627 | 14.4%\n","Epoch: 4370/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 32.5 | Wins: 628 | 14.4%\n","Epoch: 4380/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 33.3 | Wins: 630 | 14.4%\n","Epoch: 4390/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 40.5 | Wins: 631 | 14.4%\n","Epoch: 4400/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 32.9 | Wins: 634 | 14.4%\n","Epoch: 4410/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 35.4 | Wins: 635 | 14.4%\n","Epoch: 4420/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 53.7 | Wins: 638 | 14.4%\n","Epoch: 4430/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 41.0 | Wins: 638 | 14.4%\n","Epoch: 4440/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 55.3 | Wins: 639 | 14.4%\n","Epoch: 4450/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 38.4 | Wins: 639 | 14.4%\n","Epoch: 4460/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 29.1 | Wins: 640 | 14.3%\n","Epoch: 4470/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 53.2 | Wins: 641 | 14.3%\n","Epoch: 4480/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 33.5 | Wins: 641 | 14.3%\n","Epoch: 4490/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 41.9 | Wins: 642 | 14.3%\n","Epoch: 4500/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 54.1 | Wins: 647 | 14.4%\n","Epoch: 4510/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 51.6 | Wins: 648 | 14.4%\n","Epoch: 4520/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 61.9 | Wins: 650 | 14.4%\n","Epoch: 4530/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 36.4 | Wins: 651 | 14.4%\n","Epoch: 4540/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 48.3 | Wins: 652 | 14.4%\n","Epoch: 4550/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 53.6 | Wins: 653 | 14.4%\n","Epoch: 4560/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 56.5 | Wins: 656 | 14.4%\n","Epoch: 4570/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 56.5 | Wins: 658 | 14.4%\n","Epoch: 4580/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 39.0 | Wins: 660 | 14.4%\n","Epoch: 4590/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 41.4 | Wins: 662 | 14.4%\n","Epoch: 4600/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 59.8 | Wins: 663 | 14.4%\n","Epoch: 4610/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 38.6 | Wins: 664 | 14.4%\n","Epoch: 4620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 45.5 | Wins: 665 | 14.4%\n","Epoch: 4630/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 69.4 | Wins: 666 | 14.4%\n","Epoch: 4640/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 45.0 | Wins: 670 | 14.4%\n","Epoch: 4650/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 91.1 | Wins: 671 | 14.4%\n","Epoch: 4660/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 39.7 | Wins: 672 | 14.4%\n","Epoch: 4670/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 40.0 | Wins: 672 | 14.4%\n","Epoch: 4680/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 36.8 | Wins: 673 | 14.4%\n","Epoch: 4690/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 64.3 | Wins: 673 | 14.3%\n","Epoch: 4700/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 62.8 | Wins: 674 | 14.3%\n","Epoch: 4710/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 80.6 | Wins: 675 | 14.3%\n","Epoch: 4720/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 57.6 | Wins: 676 | 14.3%\n","Epoch: 4730/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 65.3 | Wins: 676 | 14.3%\n","Epoch: 4740/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 73.9 | Wins: 676 | 14.3%\n","Epoch: 4750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 57.3 | Wins: 677 | 14.3%\n","Epoch: 4760/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 86.7 | Wins: 679 | 14.3%\n","Epoch: 4770/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 83.5 | Wins: 681 | 14.3%\n","Epoch: 4780/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 62.4 | Wins: 682 | 14.3%\n","Epoch: 4790/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 99.5 | Wins: 684 | 14.3%\n","Epoch: 4800/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 82.5 | Wins: 684 | 14.2%\n","Epoch: 4810/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 97.7 | Wins: 687 | 14.3%\n","Epoch: 4820/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 56.6 | Wins: 689 | 14.3%\n","Epoch: 4830/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 66.7 | Wins: 689 | 14.3%\n","Epoch: 4840/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 91.6 | Wins: 689 | 14.2%\n","Epoch: 4850/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 57.7 | Wins: 691 | 14.2%\n","Epoch: 4860/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 86.9 | Wins: 694 | 14.3%\n","Epoch: 4870/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 58.7 | Wins: 696 | 14.3%\n","Epoch: 4880/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 101.6 | Wins: 698 | 14.3%\n","Epoch: 4890/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 103.1 | Wins: 699 | 14.3%\n","Epoch: 4900/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 81.3 | Wins: 700 | 14.3%\n","Epoch: 4910/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 72.6 | Wins: 700 | 14.3%\n","Epoch: 4920/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 100.8 | Wins: 702 | 14.3%\n","Epoch: 4930/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 117.7 | Wins: 702 | 14.2%\n","Epoch: 4940/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 117.0 | Wins: 703 | 14.2%\n","Epoch: 4950/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 124.9 | Wins: 703 | 14.2%\n","Epoch: 4960/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 124.4 | Wins: 703 | 14.2%\n","Epoch: 4970/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 140.6 | Wins: 705 | 14.2%\n","Epoch: 4980/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 148.3 | Wins: 706 | 14.2%\n","Epoch: 4990/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 124.7 | Wins: 707 | 14.2%\n","Epoch: 5000/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 135.1 | Wins: 708 | 14.2%\n","Epoch: 5010/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 131.0 | Wins: 710 | 14.2%\n","Epoch: 5020/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 132.9 | Wins: 710 | 14.1%\n","Epoch: 5030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 116.6 | Wins: 710 | 14.1%\n","Epoch: 5040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 136.1 | Wins: 711 | 14.1%\n","Epoch: 5050/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 146.3 | Wins: 712 | 14.1%\n","Epoch: 5060/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 141.3 | Wins: 715 | 14.1%\n","Epoch: 5070/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 132.9 | Wins: 716 | 14.1%\n","Epoch: 5080/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 143.7 | Wins: 716 | 14.1%\n","Epoch: 5090/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 153.3 | Wins: 717 | 14.1%\n","Epoch: 5100/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 146.2 | Wins: 718 | 14.1%\n","Epoch: 5110/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 138.7 | Wins: 718 | 14.1%\n","Epoch: 5120/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 142.2 | Wins: 722 | 14.1%\n","Epoch: 5130/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 120.2 | Wins: 722 | 14.1%\n","Epoch: 5140/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 138.0 | Wins: 723 | 14.1%\n","Epoch: 5150/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 143.8 | Wins: 724 | 14.1%\n","Epoch: 5160/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 130.0 | Wins: 724 | 14.0%\n","Epoch: 5170/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 134.8 | Wins: 725 | 14.0%\n","Epoch: 5180/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 154.5 | Wins: 726 | 14.0%\n","Epoch: 5190/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 133.9 | Wins: 727 | 14.0%\n","Epoch: 5200/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 98.1 | Wins: 728 | 14.0%\n","Epoch: 5210/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 135.5 | Wins: 728 | 14.0%\n","Epoch: 5220/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 131.7 | Wins: 731 | 14.0%\n","Epoch: 5230/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 156.0 | Wins: 732 | 14.0%\n","Epoch: 5240/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 137.9 | Wins: 732 | 14.0%\n","Epoch: 5250/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 127.6 | Wins: 732 | 13.9%\n","Epoch: 5260/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 152.9 | Wins: 734 | 14.0%\n","Epoch: 5270/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 135.2 | Wins: 735 | 13.9%\n","Epoch: 5280/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 98.2 | Wins: 735 | 13.9%\n","Epoch: 5290/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 128.7 | Wins: 736 | 13.9%\n","Epoch: 5300/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 143.1 | Wins: 736 | 13.9%\n","Epoch: 5310/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 131.7 | Wins: 737 | 13.9%\n","Epoch: 5320/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 151.0 | Wins: 737 | 13.9%\n","Epoch: 5330/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 107.4 | Wins: 737 | 13.8%\n","Epoch: 5340/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 142.7 | Wins: 741 | 13.9%\n","Epoch: 5350/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 143.7 | Wins: 741 | 13.9%\n","Epoch: 5360/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 127.3 | Wins: 741 | 13.8%\n","Epoch: 5370/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 138.7 | Wins: 744 | 13.9%\n","Epoch: 5380/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 141.8 | Wins: 745 | 13.8%\n","Epoch: 5390/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 137.7 | Wins: 747 | 13.9%\n","Epoch: 5400/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 111.0 | Wins: 747 | 13.8%\n","Epoch: 5410/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 115.1 | Wins: 747 | 13.8%\n","Epoch: 5420/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 144.4 | Wins: 748 | 13.8%\n","Epoch: 5430/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 122.9 | Wins: 748 | 13.8%\n","Epoch: 5440/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 147.7 | Wins: 748 | 13.8%\n","Epoch: 5450/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 131.9 | Wins: 749 | 13.7%\n","Epoch: 5460/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 118.1 | Wins: 750 | 13.7%\n","Epoch: 5470/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 156.0 | Wins: 751 | 13.7%\n","Epoch: 5480/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 124.1 | Wins: 752 | 13.7%\n","Epoch: 5490/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 144.2 | Wins: 754 | 13.7%\n","Epoch: 5500/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 126.0 | Wins: 755 | 13.7%\n","Epoch: 5510/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 110.3 | Wins: 755 | 13.7%\n","Epoch: 5520/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 159.5 | Wins: 757 | 13.7%\n","Epoch: 5530/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 141.8 | Wins: 758 | 13.7%\n","Epoch: 5540/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 119.3 | Wins: 758 | 13.7%\n","Epoch: 5550/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 151.0 | Wins: 758 | 13.7%\n","Epoch: 5560/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 149.5 | Wins: 759 | 13.7%\n","Epoch: 5570/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 143.4 | Wins: 761 | 13.7%\n","Epoch: 5580/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 146.9 | Wins: 763 | 13.7%\n","Epoch: 5590/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 143.6 | Wins: 763 | 13.6%\n","Epoch: 5600/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 137.6 | Wins: 763 | 13.6%\n","Epoch: 5610/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 151.7 | Wins: 764 | 13.6%\n","Epoch: 5620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 143.7 | Wins: 765 | 13.6%\n","Epoch: 5630/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 140.8 | Wins: 766 | 13.6%\n","Epoch: 5640/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 151.0 | Wins: 766 | 13.6%\n","Epoch: 5650/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 126.3 | Wins: 766 | 13.6%\n","Epoch: 5660/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 137.8 | Wins: 767 | 13.6%\n","Epoch: 5670/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 129.7 | Wins: 768 | 13.5%\n","Epoch: 5680/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 120.1 | Wins: 769 | 13.5%\n","Epoch: 5690/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 126.5 | Wins: 769 | 13.5%\n","Epoch: 5700/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 130.0 | Wins: 771 | 13.5%\n","Epoch: 5710/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 117.3 | Wins: 772 | 13.5%\n","Epoch: 5720/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 145.3 | Wins: 772 | 13.5%\n","Epoch: 5730/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 135.8 | Wins: 772 | 13.5%\n","Epoch: 5740/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 140.4 | Wins: 774 | 13.5%\n","Epoch: 5750/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 147.4 | Wins: 776 | 13.5%\n","Epoch: 5760/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 133.7 | Wins: 777 | 13.5%\n","Epoch: 5770/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 116.5 | Wins: 779 | 13.5%\n","Epoch: 5780/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 155.9 | Wins: 780 | 13.5%\n","Epoch: 5790/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 141.6 | Wins: 781 | 13.5%\n","Epoch: 5800/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 137.9 | Wins: 781 | 13.5%\n","Epoch: 5810/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 145.6 | Wins: 783 | 13.5%\n","Epoch: 5820/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 125.9 | Wins: 783 | 13.5%\n","Epoch: 5830/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 140.0 | Wins: 783 | 13.4%\n","Epoch: 5840/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 79.9 | Wins: 783 | 13.4%\n","Epoch: 5850/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 129.0 | Wins: 784 | 13.4%\n","Epoch: 5860/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 142.2 | Wins: 785 | 13.4%\n","Epoch: 5870/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 128.6 | Wins: 786 | 13.4%\n","Epoch: 5880/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 149.2 | Wins: 786 | 13.4%\n","Epoch: 5890/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 122.9 | Wins: 786 | 13.3%\n","Epoch: 5900/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 122.2 | Wins: 786 | 13.3%\n","Epoch: 5910/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 147.3 | Wins: 786 | 13.3%\n","Epoch: 5920/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 128.9 | Wins: 787 | 13.3%\n","Epoch: 5930/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 98.1 | Wins: 789 | 13.3%\n","Epoch: 5940/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 116.7 | Wins: 789 | 13.3%\n","Epoch: 5950/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 125.2 | Wins: 790 | 13.3%\n","Epoch: 5960/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 148.9 | Wins: 791 | 13.3%\n","Epoch: 5970/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 127.2 | Wins: 791 | 13.2%\n","Epoch: 5980/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 151.0 | Wins: 791 | 13.2%\n","Epoch: 5990/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 135.3 | Wins: 792 | 13.2%\n","Epoch: 6000/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 138.4 | Wins: 794 | 13.2%\n","Epoch: 6010/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 140.0 | Wins: 794 | 13.2%\n","Epoch: 6020/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 131.8 | Wins: 795 | 13.2%\n","Epoch: 6030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 151.0 | Wins: 795 | 13.2%\n","Epoch: 6040/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 121.3 | Wins: 795 | 13.2%\n","Epoch: 6050/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 141.2 | Wins: 795 | 13.1%\n","Epoch: 6060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 113.0 | Wins: 796 | 13.1%\n","Epoch: 6070/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 161.0 | Wins: 798 | 13.1%\n","Epoch: 6080/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 138.8 | Wins: 798 | 13.1%\n","Epoch: 6090/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 146.4 | Wins: 798 | 13.1%\n","Epoch: 6100/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 115.8 | Wins: 798 | 13.1%\n","Epoch: 6110/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 129.0 | Wins: 799 | 13.1%\n","Epoch: 6120/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 128.7 | Wins: 799 | 13.1%\n","Epoch: 6130/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 137.7 | Wins: 800 | 13.1%\n","Epoch: 6140/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 130.7 | Wins: 801 | 13.0%\n","Epoch: 6150/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 118.5 | Wins: 801 | 13.0%\n","Epoch: 6160/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 120.4 | Wins: 802 | 13.0%\n","Epoch: 6170/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 150.8 | Wins: 804 | 13.0%\n","Epoch: 6180/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 141.7 | Wins: 805 | 13.0%\n","Epoch: 6190/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 118.2 | Wins: 805 | 13.0%\n","Epoch: 6200/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 141.2 | Wins: 805 | 13.0%\n","Epoch: 6210/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 126.9 | Wins: 806 | 13.0%\n","Epoch: 6220/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 145.3 | Wins: 806 | 13.0%\n","Epoch: 6230/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 169.4 | Wins: 811 | 13.0%\n","Epoch: 6240/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 152.7 | Wins: 812 | 13.0%\n","Epoch: 6250/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 161.0 | Wins: 814 | 13.0%\n","Epoch: 6260/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 135.1 | Wins: 815 | 13.0%\n","Epoch: 6270/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 121.3 | Wins: 815 | 13.0%\n","Epoch: 6280/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 124.9 | Wins: 815 | 13.0%\n","Epoch: 6290/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 123.2 | Wins: 815 | 13.0%\n","Epoch: 6300/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 139.1 | Wins: 816 | 13.0%\n","Epoch: 6310/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 140.1 | Wins: 817 | 12.9%\n","Epoch: 6320/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 133.0 | Wins: 817 | 12.9%\n","Epoch: 6330/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 136.0 | Wins: 817 | 12.9%\n","Epoch: 6340/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 136.7 | Wins: 817 | 12.9%\n","Epoch: 6350/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 138.2 | Wins: 819 | 12.9%\n","Epoch: 6360/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 135.8 | Wins: 820 | 12.9%\n","Epoch: 6370/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 145.1 | Wins: 820 | 12.9%\n","Epoch: 6380/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 146.2 | Wins: 820 | 12.9%\n","Epoch: 6390/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 114.2 | Wins: 821 | 12.8%\n","Epoch: 6400/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 132.6 | Wins: 821 | 12.8%\n","Epoch: 6410/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 133.8 | Wins: 822 | 12.8%\n","Epoch: 6420/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 129.3 | Wins: 822 | 12.8%\n","Epoch: 6430/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 103.8 | Wins: 822 | 12.8%\n","Epoch: 6440/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 133.1 | Wins: 822 | 12.8%\n","Epoch: 6450/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 128.6 | Wins: 822 | 12.7%\n","Epoch: 6460/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 133.3 | Wins: 823 | 12.7%\n","Epoch: 6470/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 102.4 | Wins: 823 | 12.7%\n","Epoch: 6480/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 133.8 | Wins: 825 | 12.7%\n","Epoch: 6490/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 156.0 | Wins: 826 | 12.7%\n","Epoch: 6500/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 139.5 | Wins: 826 | 12.7%\n","Epoch: 6510/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 145.5 | Wins: 827 | 12.7%\n","Epoch: 6520/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 99.9 | Wins: 827 | 12.7%\n","Epoch: 6530/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 147.5 | Wins: 828 | 12.7%\n","Epoch: 6540/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 129.6 | Wins: 829 | 12.7%\n","Epoch: 6550/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 141.3 | Wins: 829 | 12.7%\n","Epoch: 6560/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 122.2 | Wins: 829 | 12.6%\n","Epoch: 6570/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 154.1 | Wins: 832 | 12.7%\n","Epoch: 6580/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 143.8 | Wins: 833 | 12.7%\n","Epoch: 6590/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 133.0 | Wins: 835 | 12.7%\n","Epoch: 6600/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 114.3 | Wins: 835 | 12.7%\n","Epoch: 6610/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 135.0 | Wins: 835 | 12.6%\n","Epoch: 6620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 147.1 | Wins: 836 | 12.6%\n","Epoch: 6630/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 144.8 | Wins: 838 | 12.6%\n","Epoch: 6640/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 146.8 | Wins: 838 | 12.6%\n","Epoch: 6650/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 141.5 | Wins: 840 | 12.6%\n","Epoch: 6660/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 111.1 | Wins: 842 | 12.6%\n","Epoch: 6670/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 132.7 | Wins: 843 | 12.6%\n","Epoch: 6680/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 151.0 | Wins: 843 | 12.6%\n","Epoch: 6690/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 123.0 | Wins: 845 | 12.6%\n","Epoch: 6700/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 148.8 | Wins: 846 | 12.6%\n","Epoch: 6710/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 145.4 | Wins: 846 | 12.6%\n","Epoch: 6720/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 120.9 | Wins: 846 | 12.6%\n","Epoch: 6730/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 144.2 | Wins: 846 | 12.6%\n","Epoch: 6740/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 143.6 | Wins: 847 | 12.6%\n","Epoch: 6750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 156.0 | Wins: 848 | 12.6%\n","Epoch: 6760/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 143.5 | Wins: 848 | 12.5%\n","Epoch: 6770/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 140.5 | Wins: 849 | 12.5%\n","Epoch: 6780/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 156.0 | Wins: 850 | 12.5%\n","Epoch: 6790/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 135.6 | Wins: 850 | 12.5%\n","Epoch: 6800/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 150.0 | Wins: 851 | 12.5%\n","Epoch: 6810/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 104.1 | Wins: 851 | 12.5%\n","Epoch: 6820/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 142.8 | Wins: 851 | 12.5%\n","Epoch: 6830/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 140.9 | Wins: 851 | 12.5%\n","Epoch: 6840/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 136.9 | Wins: 852 | 12.5%\n","Epoch: 6850/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 136.3 | Wins: 853 | 12.5%\n","Epoch: 6860/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 143.9 | Wins: 854 | 12.4%\n","Epoch: 6870/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 135.5 | Wins: 854 | 12.4%\n","Epoch: 6880/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 121.3 | Wins: 855 | 12.4%\n","Epoch: 6890/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 152.2 | Wins: 856 | 12.4%\n","Epoch: 6900/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 123.2 | Wins: 857 | 12.4%\n","Epoch: 6910/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 137.6 | Wins: 857 | 12.4%\n","Epoch: 6920/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 143.2 | Wins: 857 | 12.4%\n","Epoch: 6930/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 129.7 | Wins: 857 | 12.4%\n","Epoch: 6940/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 124.8 | Wins: 857 | 12.3%\n","Epoch: 6950/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 138.0 | Wins: 858 | 12.3%\n","Epoch: 6960/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 104.4 | Wins: 859 | 12.3%\n","Epoch: 6970/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 117.8 | Wins: 859 | 12.3%\n","Epoch: 6980/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 128.0 | Wins: 859 | 12.3%\n","Epoch: 6990/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 152.2 | Wins: 860 | 12.3%\n","Epoch: 7000/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 124.1 | Wins: 860 | 12.3%\n","Epoch: 7010/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 136.1 | Wins: 860 | 12.3%\n","Epoch: 7020/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 155.1 | Wins: 861 | 12.3%\n","Epoch: 7030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 140.3 | Wins: 861 | 12.2%\n","Epoch: 7040/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 135.0 | Wins: 864 | 12.3%\n","Epoch: 7050/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 134.0 | Wins: 864 | 12.3%\n","Epoch: 7060/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 158.2 | Wins: 866 | 12.3%\n","Epoch: 7070/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 151.0 | Wins: 866 | 12.2%\n","Epoch: 7080/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 129.1 | Wins: 867 | 12.2%\n","Epoch: 7090/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 139.2 | Wins: 867 | 12.2%\n","Epoch: 7100/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 137.6 | Wins: 868 | 12.2%\n","Epoch: 7110/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 133.0 | Wins: 868 | 12.2%\n","Epoch: 7120/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 137.6 | Wins: 868 | 12.2%\n","Epoch: 7130/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 140.5 | Wins: 868 | 12.2%\n","Epoch: 7140/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 138.9 | Wins: 869 | 12.2%\n","Epoch: 7150/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 156.0 | Wins: 870 | 12.2%\n","Epoch: 7160/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 126.9 | Wins: 871 | 12.2%\n","Epoch: 7170/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 144.6 | Wins: 872 | 12.2%\n","Epoch: 7180/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 156.0 | Wins: 873 | 12.2%\n","Epoch: 7190/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 126.3 | Wins: 873 | 12.1%\n","Epoch: 7200/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 129.8 | Wins: 873 | 12.1%\n","Epoch: 7210/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 142.2 | Wins: 874 | 12.1%\n","Epoch: 7220/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 152.3 | Wins: 878 | 12.2%\n","Epoch: 7230/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 128.1 | Wins: 880 | 12.2%\n","Epoch: 7240/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 141.0 | Wins: 880 | 12.2%\n","Epoch: 7250/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 111.4 | Wins: 880 | 12.1%\n","Epoch: 7260/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 127.9 | Wins: 881 | 12.1%\n","Epoch: 7270/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 151.0 | Wins: 881 | 12.1%\n","Epoch: 7280/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 130.7 | Wins: 881 | 12.1%\n","Epoch: 7290/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 139.1 | Wins: 881 | 12.1%\n","Epoch: 7300/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 119.5 | Wins: 881 | 12.1%\n","Epoch: 7310/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 106.2 | Wins: 882 | 12.1%\n","Epoch: 7320/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 138.9 | Wins: 882 | 12.0%\n","Epoch: 7330/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 147.5 | Wins: 884 | 12.1%\n","Epoch: 7340/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 130.2 | Wins: 884 | 12.0%\n","Epoch: 7350/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 107.3 | Wins: 884 | 12.0%\n","Epoch: 7360/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 148.9 | Wins: 884 | 12.0%\n","Epoch: 7370/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 140.6 | Wins: 884 | 12.0%\n","Epoch: 7380/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 138.8 | Wins: 884 | 12.0%\n","Epoch: 7390/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 141.8 | Wins: 884 | 12.0%\n","Epoch: 7400/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 144.5 | Wins: 885 | 12.0%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c_gQJ5FSvivj"},"source":["model = create_model(optimizer = RMSprop(), loss = tf.losses.huber_loss(),\n","                     stack = nb_frames, input_size = board_size,\n","                     output_size = game.nb_actions)\n","function = keras.models.load_model('keras.h5', \n","                                   custom_objects = {'huber_loss': tf.losses.huber_loss(),\n","                                                     'NoisyDenseFG': NoisyDenseFG})\n","\n","model.set_weights(function.get_weights())\n","    \n","board_size = 10\n","nb_frames = 4\n","nb_actions = 5\n","\n","agent = Agent(model = model, target = None, memory_size = -1,\n","              nb_frames = nb_frames, board_size = board_size, per = False)\n","\n","agent.test(game, visual = False, nb_epoch = 10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AuYPlAtY-kOA"},"source":["#@title save yo data to drive\n","from google.colab import files\n","from google.colab import auth\n","from googleapiclient.http import MediaFileUpload\n","from googleapiclient.discovery import build\n","\n","def save_file_to_drive(name, path):\n","    file_metadata = {\n","    'name': name,\n","    'mimeType': 'application/octet-stream'\n","    }\n","\n","    media = MediaFileUpload(path, \n","                        mimetype='application/octet-stream',\n","                        resumable=True)\n","\n","    created = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n","\n","    print('File ID: {}'.format(created.get('id')))\n","\n","    return created\n","\n","\n","# !rm -rf $zip_file\n","#!ls | grep keras_ | zip -@ zip_file # FOLDERS TO SAVE INTO ZIP FILE\n","!zip -r keras_epsgreedy_dueling_3steps_dqn.zip keras_model.h5 keras_training_data.csv\n","from google.colab import files\n","files.download('keras_epsgreedy_dueling_3steps_dqn.zip')\n","auth.authenticate_user()\n","drive_service = build('drive', 'v3')\n","\n","destination_name = 'keras_epsgreedy_dueling_3steps_dqn.zip'\n","path_to_file = 'keras_epsgreedy_dueling_3steps_dqn.zip'\n","save_file_to_drive(destination_name, path_to_file)"],"execution_count":null,"outputs":[]}]}