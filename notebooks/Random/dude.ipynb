{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dude.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"M-EV5W9gfM-q","colab_type":"code","outputId":"496ae46c-2c35-4f89-81dd-d3931f446991","executionInfo":{"status":"error","timestamp":1544959078730,"user_tz":120,"elapsed":30536,"user":{"displayName":"Victor Neves","photoUrl":"https://lh5.googleusercontent.com/-zfn1KWpcK60/AAAAAAAAAAI/AAAAAAAAZas/oOSqICtc8i0/s64/photo.jpg","userId":"17687780415293205160"}},"colab":{"base_uri":"https://localhost:8080/","height":14195}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI. - THIS\n","\"\"\"\n","\n","import sys  # To close the window when the game is over\n","from array import array  # Efficient numeric arrays\n","from os import environ, path  # To center the game window the best possible\n","import random  # Random numbers used for the food\n","import logging  # Logging function for movements and errors\n","import json # For file handling (leaderboards)\n","from itertools import tee  # For the color gradient on snake\n","\n","import numpy as np # Used in calculations and math\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","OPTIONS = {'QUIT': 0,\n","           'PLAY': 1,\n","           'BENCHMARK': 2,\n","           'LEADERBOARDS': 3,\n","           'MENU': 4,\n","           'ADD_TO_LEADERBOARDS': 5}\n","RELATIVE_ACTIONS = {'LEFT': 0,\n","                    'FORWARD': 1,\n","                    'RIGHT': 2}\n","ABSOLUTE_ACTIONS = {'LEFT': 0,\n","                    'RIGHT': 1,\n","                    'UP': 2,\n","                    'DOWN': 3,\n","                    'IDLE': 4}\n","FORBIDDEN_MOVES = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","REWARDS = {'MOVE': -0.005,\n","           'GAME_OVER': -1,\n","           'SCORED': 1}\n","\n","# Types of point in the board\n","POINT_TYPE = {'EMPTY': 0,\n","              'FOOD': 1,\n","              'BODY': 2,\n","              'HEAD': 3,\n","              'DANGEROUS': 4}\n","\n","# Speed levels possible to human players. MEGA HARDCORE starts with MEDIUM and\n","# increases with snake size\n","LEVELS = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","SPEEDS = {'EASY': 80,\n","          'MEDIUM': 60,\n","          'HARD': 40,\n","          'MEGA_HARDCORE': 65}\n","\n","# Set the constant FPS limit for the game. Smoothness depend on this.\n","GAME_FPS = 100\n","\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    block_size: int, optional, default = 20\n","        The size in pixels of a block.\n","    head_color: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    tail_color: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    food_color: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    game_speed: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    benchmark: int, optional, default = 10\n","        Ammount of matches to benchmark and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, board_size = 30, block_size = 20,\n","                 head_color = (42, 42, 42), tail_color = (152, 152, 152),\n","                 food_color = (200, 0, 0), game_speed = 80, benchmark = 10):\n","        \"\"\"Initialize all global variables. Updated with argument_handler.\"\"\"\n","        self.board_size = board_size\n","        self.block_size = block_size\n","        self.head_color = head_color\n","        self.tail_color = tail_color\n","        self.food_color = food_color\n","        self.game_speed = game_speed\n","        self.benchmark = benchmark\n","\n","        if self.board_size > 50: # Warn the user about performance\n","            LOGGER.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","    @property\n","    def canvas_size(self):\n","        \"\"\"Canvas size is updated with board_size and block_size.\"\"\"\n","        return self.board_size * self.block_size\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), block_type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.block_type = block_type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((VAR.canvas_size) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.block_type == \"menu\" and not self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.block_type == \"menu\" and self.hovered:\n","            color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [board_size / 4, board_size / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(VAR.board_size / 4), int(VAR.board_size / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def is_movement_invalid(self, action):\n","        \"\"\"Check if the movement is invalid, according to FORBIDDEN_MOVES.\"\"\"\n","        valid = False\n","\n","        if (action, self.previous_action) in FORBIDDEN_MOVES:\n","            valid = True\n","\n","        return valid\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if (action == ABSOLUTE_ACTIONS['IDLE'] or\n","            self.is_movement_invalid(action)):\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == ABSOLUTE_ACTIONS['LEFT']:\n","            self.head[0] -= 1\n","        elif action == ABSOLUTE_ACTIONS['RIGHT']:\n","            self.head[0] += 1\n","        elif action == ABSOLUTE_ACTIONS['UP']:\n","            self.head[1] -= 1\n","        elif action == ABSOLUTE_ACTIONS['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            LOGGER.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((VAR.board_size - 1) * random.random()),\n","                        int((VAR.board_size - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            LOGGER.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        VAR.board_size = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with board_size * block_size dimension.\"\"\"\n","        pygame.init()\n","        flags = pygame.DOUBLEBUF | pygame.HWSURFACE\n","        self.window = pygame.display.set_mode((VAR.canvas_size, VAR.canvas_size),\n","                                              flags)\n","        self.window.set_alpha(None)\n","\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def cycle_menu(self, menu_options, list_menu, dictionary, img = None,\n","                   img_rect = None):\n","        \"\"\"Cycle through a given menu, waiting for an option to be clicked.\"\"\"\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            events = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for i, option in enumerate(menu_options):\n","                if option is not None:\n","                    option.draw()\n","                    option.hovered = False\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        for event in events:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = dictionary[list_menu[i]]\n","\n","            if selected_option is not None:\n","                selected = True\n","            if img is not None:\n","                self.window.blit(img, img_rect.bottomleft)\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def cycle_matches(self, n_matches, mega_hardcore = False):\n","        \"\"\"Cycle through matches until the end.\"\"\"\n","        score = array('i')\n","\n","        for _ in range(n_matches):\n","            self.reset_game()\n","            self.start_match(wait = 3)\n","            score.append(self.single_player(mega_hardcore))\n","\n","        return score\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images\" +\n","                                              \"/snake_logo.png\")).convert()\n","        img = pygame.transform.scale(img, (VAR.canvas_size,\n","                                           int(VAR.canvas_size / 3)))\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","        list_menu = ['PLAY', 'BENCHMARK', 'LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY GAME ',\n","                                  (self.screen_rect.centerx,\n","                                   4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ',\n","                                  (self.screen_rect.centerx,\n","                                   6 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ',\n","                                  (self.screen_rect.centerx,\n","                                   8 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ',\n","                                  (self.screen_rect.centerx,\n","                                   10 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\")]\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS,\n","                                          img, img_rect)\n","\n","        return selected_option\n","\n","    def start_match(self, wait):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(wait):\n","            time = str(wait - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in',\n","                              (self.screen_rect.centerx,\n","                               4 * self.screen_rect.centery / 10),\n","                              self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                     12 * self.screen_rect.centery / 10),\n","                              self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","            pygame.time.wait(1000)\n","\n","        LOGGER.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","\n","        while True:\n","            if opt == OPTIONS['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == OPTIONS['PLAY']:\n","                VAR.game_speed, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = 1,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == OPTIONS['BENCHMARK']:\n","                VAR.game_speed, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = VAR.benchmark,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == OPTIONS['LEADERBOARDS']:\n","                self.view_leaderboards()\n","            elif opt == OPTIONS['MENU']:\n","                opt = self.menu()\n","            if opt == OPTIONS['ADD_TO_LEADERBOARDS']:\n","                self.add_to_leaderboards(score, None) # Gotta improve this logic.\n","                self.view_leaderboards()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        score_option = None\n","\n","        if len(score) == VAR.benchmark:\n","            score_option = TextBlock(' ADD TO LEADERBOARDS ',\n","                                     (self.screen_rect.centerx,\n","                                      8 * self.screen_rect.centery / 10),\n","                                     self.window, (1 / 15), \"menu\")\n","\n","        text_score = 'SCORE: ' + str(int(np.mean(score)))\n","        list_menu = ['PLAY', 'MENU', 'ADD_TO_LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                   4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                   6 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        score_option,\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        TextBlock(text_score, (self.screen_rect.centerx,\n","                                               15 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"text\")]\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        LOGGER.info('EVENT: GAME OVER | FINAL %s', text_score)\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS)\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        list_menu = ['EASY', 'MEDIUM', 'HARD', 'MEGA_HARDCORE']\n","        menu_options = [TextBlock(LEVELS[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\")]\n","\n","        speed = self.cycle_menu(menu_options, list_menu, SPEEDS)\n","        mega_hardcore = False\n","\n","        if speed == SPEEDS['MEGA_HARDCORE']:\n","            mega_hardcore = True\n","\n","        return speed, mega_hardcore\n","\n","    def single_player(self, mega_hardcore = False):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                   previous_size)\n","\n","        # Main loop, where snakes moves after elapsed time is bigger than the\n","        # move_wait time. The last_key pressed is recorded to make the game more\n","        # smooth for human players.\n","        elapsed = 0\n","        last_key = self.snake.previous_action\n","        move_wait = VAR.game_speed\n","\n","        while not self.game_over:\n","            elapsed += self.fps.get_time()  # Get elapsed time since last call.\n","\n","            if mega_hardcore:  # Progressive speed increments, the hardest.\n","                move_wait = VAR.game_speed - (2 * (self.snake.length - 3))\n","\n","            key_input = self.handle_input()  # Receive inputs with tick.\n","            invalid_key = self.snake.is_movement_invalid(key_input)\n","\n","            if key_input is not None and not invalid_key:\n","                last_key = key_input\n","\n","            if elapsed >= move_wait:  # Move and redraw\n","                elapsed = 0\n","                self.play(last_key)\n","                current_size = self.snake.length  # Update the body size\n","\n","                if current_size > previous_size:\n","                    color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                               current_size)\n","\n","                    previous_size = current_size\n","\n","                self.draw(color_list)\n","\n","            pygame.display.update()\n","            self.fps.tick(GAME_FPS)  # Limit FPS to 100\n","\n","        score = current_size - 3  # After the game is over, record score\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (VAR.board_size - 1) or self.snake.head[0] < 0:\n","            LOGGER.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (VAR.board_size - 1) or self.snake.head[1] < 0:\n","            LOGGER.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            LOGGER.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = None\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            LOGGER.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            LOGGER.info('ACTION: KEY PRESSED: LEFT')\n","            action = ABSOLUTE_ACTIONS['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            LOGGER.info('ACTION: KEY PRESSED: RIGHT')\n","            action = ABSOLUTE_ACTIONS['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            LOGGER.info('ACTION: KEY PRESSED: UP')\n","            action = ABSOLUTE_ACTIONS['UP']\n","        elif keys[pygame.K_DOWN]:\n","            LOGGER.info('ACTION: KEY PRESSED: DOWN')\n","            action = ABSOLUTE_ACTIONS['DOWN']\n","\n","        return action\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size board_size**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((VAR.board_size, VAR.board_size))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = POINT_TYPE['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = POINT_TYPE['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = POINT_TYPE['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == RELATIVE_ACTIONS['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == RELATIVE_ACTIONS['LEFT']:\n","            if self.snake.previous_action == ABSOLUTE_ACTIONS['LEFT']:\n","                action = ABSOLUTE_ACTIONS['DOWN']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['RIGHT']:\n","                action = ABSOLUTE_ACTIONS['UP']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['UP']:\n","                action = ABSOLUTE_ACTIONS['LEFT']\n","            else:\n","                action = ABSOLUTE_ACTIONS['RIGHT']\n","        else:\n","            if self.snake.previous_action == ABSOLUTE_ACTIONS['LEFT']:\n","                action = ABSOLUTE_ACTIONS['UP']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['RIGHT']:\n","                action = ABSOLUTE_ACTIONS['DOWN']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['UP']:\n","                action = ABSOLUTE_ACTIONS['RIGHT']\n","            else:\n","                action = ABSOLUTE_ACTIONS['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                self.game_over = True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current reward. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = REWARDS['MOVE']\n","\n","        if self.game_over:\n","            reward = REWARDS['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect((part[0] *\n","                        VAR.block_size), part[1] * VAR.block_size,\n","                        VAR.block_size, VAR.block_size))\n","\n","        pygame.draw.rect(self.window, VAR.food_color,\n","                         pygame.Rect(self.food_pos[0] * VAR.block_size,\n","                         self.food_pos[1] * VAR.block_size, VAR.block_size,\n","                         VAR.block_size))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                   + str(self.snake.length - 3))\n","\n","    def get_name(self):\n","        \"\"\"See test.py in my desktop, for a textbox input in pygame\"\"\"\n","        return None\n","\n","    def add_to_leaderboards(self, score, step):\n","        file_path = resource_path(\"resources/scores.json\")\n","\n","        name = self.get_name()\n","        new_score = {'name': 'test',\n","                     'ranking_data': {'score': score,\n","                                      'step': step}}\n","\n","        with open(file_path, 'w') as leaderboards_file:\n","            json.dump(new_score, leaderboards_file)\n","\n","    def view_leaderboards(self):\n","        list_menu = ['MENU']\n","        menu_options = [TextBlock('LEADERBOARDS',\n","                                  (self.screen_rect.centerx,\n","                                   2 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"text\")]\n","\n","        file_path = resource_path(\"resources/scores.json\")\n","\n","        with open(file_path, 'r') as leaderboards_file:\n","            scores_data = json.loads(leaderboards_file.read())\n","\n","        scores_data.sort(key = operator.itemgetter('score'))\n","\n","#        for score in formatted_scores:\n","#            menu_options.append(TextBlock(person_ranked,\n","#                                (self.screen_rect.centerx,\n","#                                10 * self.screen_rect.centery / 10),\n","#                                self.window, (1 / 12), \"text\"))\n","\n","        menu_options.append(TextBlock('MENU',\n","                            (self.screen_rect.centerx,\n","                            10 * self.screen_rect.centery / 10),\n","                            self.window, (1 / 12), \"menu\"))\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS)\n","\n","    @staticmethod\n","    def format_scores(scores, ammount):\n","        scores = scores[-ammount:]\n","\n","\n","\n","    @staticmethod\n","    def eval_local_safety(canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size board_size**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if ((body[0][0] + 1) > (VAR.board_size - 1)\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]):\n","            canvas[VAR.board_size - 1, 0] = POINT_TYPE['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[VAR.board_size - 1, 1] = POINT_TYPE['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[VAR.board_size - 1, 2] = POINT_TYPE['DANGEROUS']\n","        if ((body[0][1] + 1) > (VAR.board_size - 1)\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]):\n","            canvas[VAR.board_size - 1, 3] = POINT_TYPE['DANGEROUS']\n","\n","        return canvas\n","\n","    @staticmethod\n","    def gradient(colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for substep in range(1, substeps):\n","                yield tuple([(start[component]\n","                              + (float(substep) / (substeps - 1))\n","                              * (finish[component] - start[component]))\n","                             for component in range(components)])\n","\n","        def pairs(seq):\n","            first_color, second_color = tee(seq)\n","            next(second_color, None)\n","\n","            return zip(first_color, second_color)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for first_color, second_color in pairs(colors):\n","            for gradient_color in linear_gradient(first_color, second_color,\n","                                                  substeps):\n","                result.append(gradient_color)\n","\n","        return result\n","\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","VAR = GlobalVariables() # Initializing GlobalVariables\n","LOGGER = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","\n","\"\"\"THIS\"\"\"\n","\n","import numpy as np\n","\n","from random import sample, uniform\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 100, per = False, alpha = 0.6,\n","                 epsilon = 0.001, beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.per = per\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","        if self.per:\n","            self.per_epsilon = epsilon\n","            self.per_alpha = alpha\n","            self.per_beta = beta\n","            self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        if self.per:\n","            return self.exp\n","        else:\n","            return len(self.memory)\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.per_epsilon) ** self.per_alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        if self.per: # If using PER, insert in the max_priority.\n","            max_priority = self.memory.max_leaf()\n","\n","            if max_priority == 0:\n","                max_priority = self.get_priority(0)\n","\n","            self.memory.insert(experience, max_priority)\n","            self.exp += 1\n","        else: # Else, just append the experience to the list.\n","            self.memory.append(experience)\n","\n","            if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","                self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        if self.per:\n","            batch = [None] * batch_size\n","            IS_weights = np.zeros((batch_size, ))\n","            tree_indices = [0] * batch_size\n","\n","            memory_sum = self.memory.sum()\n","            len_seg = memory_sum / batch_size\n","            min_prob = self.memory.min_leaf() / memory_sum\n","\n","            for i in range(batch_size):\n","                val = uniform(len_seg * i, len_seg * (i + 1))\n","                tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","                prob = priority / self.memory.sum()\n","                IS_weights[i] = np.power(prob / min_prob, -self.per_beta)\n","\n","            return np.array(batch), IS_weights, tree_indices\n","\n","        else:\n","            IS_weights = np.ones((batch_size, ))\n","            batch = sample(self.memory, batch_size)\n","            return np.array(batch), IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        r = r.repeat(nb_actions).reshape((batch_size, nb_actions))\n","        game_over = game_over.repeat(nb_actions)\\\n","                             .reshape((batch_size, nb_actions))\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","            for i in range(batch_size):\n","                Qsa[i] = Y_target[i][actions[i]]\n","            Qsa = np.array(Qsa).repeat(nb_actions).reshape((batch_size, nb_actions))\n","\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1).repeat(nb_actions)\\\n","                                                .reshape((batch_size, nb_actions))\n","\n","        # The targets here already take into account\n","        delta = np.zeros((batch_size, nb_actions))\n","        a = np.cast['int'](a)\n","        delta[np.arange(batch_size), a] = 1\n","        targets = ((1 - delta) * Y[:batch_size]\n","                  + delta * (r + gamma * (1 - game_over) * Qsa))\n","\n","        if self.per: # Update the Sum Tree with the absolute error.\n","            errors = np.abs((targets - Y[:batch_size]).max(axis = 1)).clip(max = 1.)\n","            self.update(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.per:\n","            if self.memory_size <= 0:\n","                self.memory_size = 150000\n","\n","            self.memory = SumTree(self.memory_size)\n","            self.exp = 0\n","        else:\n","            self.memory = []\n","\n","\n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms\n","----------\n","    * Simple Deep Q-network (DQN with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double Deep Q-network (Double DQN);\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling Deep Q-network (Dueling DQN);\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * Prioritized Experience Replay (PER);\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns (n-steps);\n","        Paper: https://arxiv.org/pdf/1703.01327\n","    * Noisy nets.\n","        Paper: https://arxiv.org/abs/1706.10295\n","\n","Arguments\n","----------\n","--load: 'file.h5'\n","    Load a previously trained model in '.h5' format.\n","--board_size: int, optional, default = 10\n","    Assign the size of the board.\n","--nb_frames: int, optional, default = 4\n","    Assign the number of frames per stack, default = 4.\n","--nb_actions: int, optional, default = 5\n","    Assign the number of actions possible.\n","--update_freq: int, optional, default = 0.001\n","    Whether to soft or hard update the target. Epochs or ammount of the update.\n","--visual: boolean, optional, default = False\n","    Select wheter or not to draw the game in pygame.\n","--double: boolean, optional, default = False\n","    Use a target network with double DQN logic.\n","--dueling: boolean, optional, default = False\n","    Whether to use dueling network logic, Q(s,a) = A + V.\n","--per: boolean, optional, default = False\n","    Use Prioritized Experience Replay (based on Sum Trees).\n","--local_state: boolean, optional, default = True\n","    Verify is possible next moves are dangerous (field expertise)\n","    THIS\n","\"\"\"\n","\n","import numpy as np\n","from array import array\n","import random\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes\n","    ----------\n","    memory: object\n","        Memory used in training. ExperienceReplay or PrioritizedExperienceReplay\n","    memory_size: int, optional, default = -1\n","        Capacity of the memory used.\n","    model: keras model\n","        The input model in Keras.\n","    target: keras model, optional, default = None\n","        The target model, used to calculade the fixed Q-targets.\n","    nb_frames: int, optional, default = 4\n","        Ammount of frames for each experience (sars).\n","    board_size: int, optional, default = 10\n","        Size of the board used.\n","    frames: list of experiences\n","        The buffer of frames, store sars experiences.\n","    per: boolean, optional, default = False\n","        Flag for PER usage.\n","    update_target_freq: int or float, default = 0.001\n","        Whether soft or hard updates occur. If < 1, soft updated target model.\n","    n_steps: int, optional, default = 1\n","        Size of the rewards buffer, to use Multi-step returns.\n","    \"\"\"\n","    def __init__(self, model, sess, target = None, memory_size = -1, nb_frames = 4,\n","                 board_size = 10, per = False, update_target_freq = 0.001):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if per:\n","            self.memory = PrioritizedExperienceReplay(memory_size = memory_size)\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.update_target_freq = update_target_freq\n","        self.sess = sess\n","        self.set_noise_list()\n","        self.clear_frames()\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def set_noise_list(self):\n","        \"\"\"Set a list of noise variables if NoisyNet is involved.\"\"\"\n","        self.noise_list = []\n","        for layer in self.model.layers:\n","            if type(layer) in {NoisyDenseFG}:\n","                self.noise_list.extend(layer.noise_list)\n","\n","    def sample_noise(self):\n","        \"\"\"Resample noise variables in NoisyNet.\"\"\"\n","        for noise in self.noise_list:\n","            self.sess.run(noise.initializer)\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\n","\n","        Return\n","        ----------\n","        expanded_frames: list of experiences\n","            The buffer of frames, shape = (nb_frames, board_size, board_size)\n","        \"\"\"\n","        frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        expanded_frames = np.expand_dims(self.frames, 0)\n","        # expanded_frames = np.transpose(expanded_frames, [0, 3, 2, 1])\n","\n","        return expanded_frames\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model_hard(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def transfer_weights(self):\n","        \"\"\"Transfer Weights from Model to Target at rate update_target_freq.\"\"\"\n","        model_weights = self.model.get_weights()\n","        target_weights = self.target.get_weights()\n","\n","        for i in range(len(W)):\n","            target_weights[i] = (self.update_target_freq * model_weights[i]\n","                                 + ((1 - self.update_target_frequency)\n","                                    * target_weights[i]))\n","\n","        self.target.set_weights(target_weights)\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, policy, value,\n","                      win_count, history_step, history_reward,\n","                      history_loss = None, verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    np.mean(history_size[-10:]),\n","                                    max(history_size[-10:]),\n","                                    np.mean(history_step[-10:]),\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}'  # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            if loss is not None:  # Print training performance\n","                text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                              + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                              + 'Mean loss - 100 episodes: {:.4f}')\n","                print(text_perf.format(history_loss[-1],\n","                                       history_loss[-1] / history_step[-1],\n","                                       np.mean(history_loss[-100:])))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   np.mean(history_step[-100:])))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\n","\n","        Return\n","        ----------\n","        loss: float\n","            Training loss of given batch.\n","        \"\"\"\n","        loss = 0.\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, optim_rounds = 1, policy = \"EpsGreedyQPolicy\",\n","              verbose = 1, n_steps = 1):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        if not hasattr(self, 'n_steps'):\n","            self.n_steps = n_steps  # Set attribute only once\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_loss = array('f')  # Holds all the losses\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        # Select exploration policy. EpsGreedyQPolicy runs faster, but takes\n","        # longer to converge. BoltzmannGumbelQPolicy is the slowest, but\n","        # converge really fast (0.1 * nb_epoch used in EpsGreedyQPolicy).\n","        # BoltzmannQPolicy is in the middle.\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        elif policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        elif policy == 'GreedyQPolicy':\n","            q_policy = GreedyQPolicy()        \n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        # If optim_rounds is bigger than one, the model will keep optimizing\n","        # after the exploration, in turns of nb_epoch size.\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                    text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                                  + '| Loss: {:.4f}')\n","                    print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:  # Exploration and training\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    game.reset_game()\n","                    self.clear_frames()\n","                    S = self.get_game_data(game)\n","\n","                    if n_steps > 1:  # Create multi-step returns buffer.\n","                        n_step_buffer = array('f')\n","\n","                    while not game.game_over:  # Main loop, until game_over\n","                        game.food_pos = game.generate_food()\n","                        self.sample_noise()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","                        game.play(action)\n","                        r = game.get_reward()\n","                        total_reward += r\n","\n","                        if n_steps > 1:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","\n","                                n_step_buffer.pop(0)\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience)  # Add to the memory\n","                        S = S_prime  # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe:  # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1  # Counter of wins for metrics\n","\n","                    if self.per:  # Advance beta, used in PER\n","                        self.memory.beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None:  # Update the target model\n","                        if update_target_freq >= 1: # Hard updates\n","                            if epoch % self.update_target_freq == 0:\n","                                self.update_target_model_hard()\n","                        elif update_target_freq < 1.:  # Soft updates\n","                            self.transfer_weights()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch = epoch, nb_epoch = nb_epoch,\n","                                           history_size = history_size,\n","                                           history_loss = history_loss,\n","                                           history_step = history_step,\n","                                           history_reward = history_reward,\n","                                           policy = policy, value = value,\n","                                           win_count = win_count,\n","                                           verbose = verbose)\n","\n","    def test(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","\n","            if visual:\n","                game.create_window()\n","                previous_size = game.snake.length  # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","                elapsed = 0\n","\n","            while not game.game_over:\n","                if visual:\n","                    elapsed += game.fps.get_time()  # Get elapsed time since last call.\n","\n","                    if elapsed >= 60:\n","                        elapsed = 0\n","                        S = self.get_game_data(game)\n","                        action, value = q_policy.select_action(self.model, S, epoch, game.nb_actions)\n","                        game.play(action)\n","                        current_size = game.snake.length  # Update the body size\n","\n","                        if current_size > previous_size:\n","                            color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                       game.snake.length)\n","\n","                            previous_size = current_size\n","\n","                        game.draw(color_list)\n","\n","                    pygame.display.update()\n","                    game.fps.tick(120)  # Limit FPS to 100\n","                else:\n","                    S = self.get_game_data(game)\n","                    action, value = q_policy.select_action(self.model, S, epoch, game.nb_actions)\n","                    game.play(action)\n","                    current_size = game.snake.length  # Update the body size\n","\n","                if game.game_over:\n","                    history_size.append(current_size)\n","                    history_step.append(game.step)\n","                    history_reward.append(game.get_reward())\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(history_size), np.max(history_size),\n","                      np.min(history_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(history_step), np.max(history_step),\n","                      np.min(history_step)))\n","        print(\"Mean rewards: {} | Biggest reward: {} | Smallest reward: {}\"\\\n","              .format(np.mean(history_reward), np.max(history_reward),\n","                      np.min(history_reward)))\n","\n","\"\"\"THIS\"\"\"\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.framework import tensor_shape\n","from tensorflow.python.layers import base\n","from tensorflow.python.ops.init_ops import Constant\n","\n","class NoisyDense(tf.keras.layers.Dense):\n","\n","    def build(self, input_shape):\n","        input_shape = tensor_shape.TensorShape(input_shape)\n","        if input_shape[-1].value is None:\n","            raise ValueError('The last dimension of the inputs to `Dense` '\n","                             'should be defined. Found `None`.')\n","        self.input_spec = base.InputSpec(min_ndim=2,\n","                                         axes={-1: input_shape[-1].value})\n","        kernel_shape = [input_shape[-1].value, self.units]\n","        kernel_quiet = self.add_variable('kernel_quiet',\n","                                         shape=kernel_shape,\n","                                         initializer=self.kernel_initializer,\n","                                         regularizer=self.kernel_regularizer,\n","                                         constraint=self.kernel_constraint,\n","                                         dtype=self.dtype,\n","                                         trainable=True)\n","        scale_init = Constant(value=(0.5 / np.sqrt(kernel_shape[0])))\n","        kernel_noise_scale = self.add_variable('kernel_noise_scale',\n","                                               shape=kernel_shape,\n","                                               initializer=scale_init,\n","                                               dtype=self.dtype,\n","                                               trainable=True)\n","        kernel_noise = self.make_kernel_noise(shape=kernel_shape)\n","        self.kernel = kernel_quiet + kernel_noise_scale * kernel_noise\n","        if self.use_bias:\n","            bias_shape = [self.units,]\n","            bias_quiet = self.add_variable('bias_quiet',\n","                                           shape=bias_shape,\n","                                           initializer=self.bias_initializer,\n","                                           regularizer=self.bias_regularizer,\n","                                           constraint=self.bias_constraint,\n","                                           dtype=self.dtype,\n","                                           trainable=True)\n","            bias_noise_scale = self.add_variable(name='bias_noise_scale',\n","                                                 shape=bias_shape,\n","                                                 initializer=scale_init,\n","                                                 dtype=self.dtype,\n","                                                 trainable=True)\n","            bias_noise = self.make_bias_noise(shape=bias_shape)\n","            self.bias = bias_quiet + bias_noise_scale * bias_noise\n","        else:\n","            self.bias = None\n","        self.built = True\n","\n","    def make_kernel_noise(self, shape):\n","        raise NotImplementedError\n","\n","    def make_bias_noise(self, shape):\n","        raise NotImplementedError\n","\n","\n","class NoisyDenseIG(NoisyDense):\n","    '''\n","    Noisy dense layer with independent Gaussian noise\n","    '''\n","    def make_kernel_noise(self, shape):\n","        noise = tf.random_normal(shape, dtype=self.dtype)\n","        kernel_noise = tf.Variable(noise, trainable=False, dtype=self.dtype)\n","        self.noise_list = [kernel_noise]\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        noise = tf.random_normal(shape, dtype=self.dtype)\n","        bias_noise = tf.Variable(noise, trainable=False, dtype=self.dtype)\n","        self.noise_list.append(bias_noise)\n","        return bias_noise\n","\n","\n","class NoisyDenseFG(NoisyDense):\n","    '''\n","    Noisy dense layer with factorized Gaussian noise\n","    '''\n","    def make_kernel_noise(self, shape):\n","        kernel_noise_input = self.make_fg_noise(shape=[shape[0]])\n","        kernel_noise_output = self.make_fg_noise(shape=[shape[1]])\n","        self.noise_list = [kernel_noise_input, kernel_noise_output]\n","        kernel_noise = kernel_noise_input[:, tf.newaxis] * kernel_noise_output\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        return self.noise_list[1] # kernel_noise_output\n","\n","    def make_fg_noise(self, shape):\n","        noise = tf.random_normal(shape, dtype=self.dtype)\n","        trans_noise = tf.sign(noise) * tf.sqrt(tf.abs(noise))\n","        return tf.Variable(trans_noise, trainable=False, dtype=self.dtype)\n","        \n","        \n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","import random\n","import numpy as np\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","from keras.engine import Layer, InputSpec\n","from keras import initializers\n","from keras import regularizers\n","from keras import constraints\n","from keras import backend as K\n","\n","from keras.utils.generic_utils import get_custom_objects\n","\n","from keras.layers import BatchNormalization\n","\n","\n","class SwitchNormalization(Layer):\n","    \"\"\"Switchable Normalization layer\n","    Switch Normalization performs Instance Normalization, Layer Normalization and Batch\n","    Normalization using its parameters, and then weighs them using learned parameters to\n","    allow different levels of interaction of the 3 normalization schemes for each layer.\n","    Only supports the moving average variant from the paper, since the `batch average`\n","    scheme requires dynamic graph execution to compute the mean and variance of several\n","    batches at runtime.\n","    # Arguments\n","        axis: Integer, the axis that should be normalized\n","            (typically the features axis).\n","            For instance, after a `Conv2D` layer with\n","            `data_format=\"channels_first\"`,\n","            set `axis=1` in `BatchNormalization`.\n","        momentum: Momentum for the moving mean and the moving variance. The original\n","            implementation suggests a default momentum of `0.997`, however it is highly\n","            unstable and training can fail after a few epochs. To stabilise training, use\n","            lower values of momentum such as `0.99` or `0.98`.\n","        epsilon: Small float added to variance to avoid dividing by zero.\n","        final_gamma: Bool value to determine if this layer is the final\n","            normalization layer for the residual block.  Overrides the initialization\n","            of the scaling weights to be `zeros`. Only used for Residual Networks,\n","            to make the forward/backward signal initially propagated through an\n","            identity shortcut.\n","        center: If True, add offset of `beta` to normalized tensor.\n","            If False, `beta` is ignored.\n","        scale: If True, multiply by `gamma`.\n","            If False, `gamma` is not used.\n","            When the next layer is linear (also e.g. `nn.relu`),\n","            this can be disabled since the scaling\n","            will be done by the next layer.\n","        beta_initializer: Initializer for the beta weight.\n","        gamma_initializer: Initializer for the gamma weight.\n","        mean_weights_initializer: Initializer for the mean weights.\n","        variance_weights_initializer: Initializer for the variance weights.\n","        moving_mean_initializer: Initializer for the moving mean.\n","        moving_variance_initializer: Initializer for the moving variance.\n","        beta_regularizer: Optional regularizer for the beta weight.\n","        gamma_regularizer: Optional regularizer for the gamma weight.\n","        mean_weights_regularizer: Optional regularizer for the mean weights.\n","        variance_weights_regularizer: Optional regularizer for the variance weights.\n","        beta_constraint: Optional constraint for the beta weight.\n","        gamma_constraint: Optional constraint for the gamma weight.\n","        mean_weights_constraints: Optional constraint for the mean weights.\n","        variance_weights_constraints: Optional constraint for the variance weights.\n","    # Input shape\n","        Arbitrary. Use the keyword argument `input_shape`\n","        (tuple of integers, does not include the samples axis)\n","        when using this layer as the first layer in a model.\n","    # Output shape\n","        Same shape as input.\n","    # References\n","        - [Differentiable Learning-to-Normalize via Switchable Normalization](https://arxiv.org/abs/1806.10779)\n","    \"\"\"\n","\n","    def __init__(self,\n","                 axis=-1,\n","                 momentum=0.99,\n","                 epsilon=1e-3,\n","                 final_gamma=False,\n","                 center=True,\n","                 scale=True,\n","                 beta_initializer='zeros',\n","                 gamma_initializer='ones',\n","                 mean_weights_initializer='ones',\n","                 variance_weights_initializer='ones',\n","                 moving_mean_initializer='ones',\n","                 moving_variance_initializer='zeros',\n","                 beta_regularizer=None,\n","                 gamma_regularizer=None,\n","                 mean_weights_regularizer=None,\n","                 variance_weights_regularizer=None,\n","                 beta_constraint=None,\n","                 gamma_constraint=None,\n","                 mean_weights_constraints=None,\n","                 variance_weights_constraints=None,\n","                 **kwargs):\n","        super(SwitchNormalization, self).__init__(**kwargs)\n","        self.supports_masking = True\n","        self.axis = axis\n","        self.momentum = momentum\n","        self.epsilon = epsilon\n","        self.center = center\n","        self.scale = scale\n","\n","        self.beta_initializer = initializers.get(beta_initializer)\n","        if final_gamma:\n","            self.gamma_initializer = initializers.get('zeros')\n","        else:\n","            self.gamma_initializer = initializers.get(gamma_initializer)\n","        self.mean_weights_initializer = initializers.get(mean_weights_initializer)\n","        self.variance_weights_initializer = initializers.get(variance_weights_initializer)\n","        self.moving_mean_initializer = initializers.get(moving_mean_initializer)\n","        self.moving_variance_initializer = initializers.get(moving_variance_initializer)\n","        self.beta_regularizer = regularizers.get(beta_regularizer)\n","        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n","        self.mean_weights_regularizer = regularizers.get(mean_weights_regularizer)\n","        self.variance_weights_regularizer = regularizers.get(variance_weights_regularizer)\n","        self.beta_constraint = constraints.get(beta_constraint)\n","        self.gamma_constraint = constraints.get(gamma_constraint)\n","        self.mean_weights_constraints = constraints.get(mean_weights_constraints)\n","        self.variance_weights_constraints = constraints.get(variance_weights_constraints)\n","\n","    def build(self, input_shape):\n","        dim = input_shape[self.axis]\n","\n","        if dim is None:\n","            raise ValueError('Axis ' + str(self.axis) + ' of '\n","                             'input tensor should have a defined dimension '\n","                             'but the layer received an input with shape ' +\n","                             str(input_shape) + '.')\n","\n","        self.input_spec = InputSpec(ndim=len(input_shape),\n","                                    axes={self.axis: dim})\n","        shape = (dim,)\n","\n","        if self.scale:\n","            self.gamma = self.add_weight(\n","                shape=shape,\n","                name='gamma',\n","                initializer=self.gamma_initializer,\n","                regularizer=self.gamma_regularizer,\n","                constraint=self.gamma_constraint)\n","        else:\n","            self.gamma = None\n","        if self.center:\n","            self.beta = self.add_weight(\n","                shape=shape,\n","                name='beta',\n","                initializer=self.beta_initializer,\n","                regularizer=self.beta_regularizer,\n","                constraint=self.beta_constraint)\n","        else:\n","            self.beta = None\n","\n","        self.moving_mean = self.add_weight(\n","            shape=shape,\n","            name='moving_mean',\n","            initializer=self.moving_mean_initializer,\n","            trainable=False)\n","\n","        self.moving_variance = self.add_weight(\n","            shape=shape,\n","            name='moving_variance',\n","            initializer=self.moving_variance_initializer,\n","            trainable=False)\n","\n","        self.mean_weights = self.add_weight(\n","            shape=(3,),\n","            name='mean_weights',\n","            initializer=self.mean_weights_initializer,\n","            regularizer=self.mean_weights_regularizer,\n","            constraint=self.mean_weights_constraints)\n","\n","        self.variance_weights = self.add_weight(\n","            shape=(3,),\n","            name='variance_weights',\n","            initializer=self.variance_weights_initializer,\n","            regularizer=self.variance_weights_regularizer,\n","            constraint=self.variance_weights_constraints)\n","\n","        self.built = True\n","\n","    def call(self, inputs, training=None):\n","        input_shape = K.int_shape(inputs)\n","\n","        # Prepare broadcasting shape.\n","        reduction_axes = list(range(len(input_shape)))\n","        del reduction_axes[self.axis]\n","\n","        if self.axis != 0:\n","            del reduction_axes[0]\n","\n","        broadcast_shape = [1] * len(input_shape)\n","        broadcast_shape[self.axis] = input_shape[self.axis]\n","\n","        mean_instance = K.mean(inputs, reduction_axes, keepdims=True)\n","        variance_instance = K.var(inputs, reduction_axes, keepdims=True)\n","\n","        mean_layer = K.mean(mean_instance, self.axis, keepdims=True)\n","        temp = variance_instance + K.square(mean_instance)\n","        variance_layer = K.mean(temp, self.axis, keepdims=True) - K.square(mean_layer)\n","\n","        def training_phase():\n","            mean_batch = K.mean(mean_instance, axis=0, keepdims=True)\n","            variance_batch = K.mean(temp, axis=0, keepdims=True) - K.square(mean_batch)\n","\n","            mean_batch_reshaped = K.flatten(mean_batch)\n","            variance_batch_reshaped = K.flatten(variance_batch)\n","\n","            if K.backend() != 'cntk':\n","                sample_size = K.prod([K.shape(inputs)[axis]\n","                                      for axis in reduction_axes])\n","                sample_size = K.cast(sample_size, dtype=K.dtype(inputs))\n","\n","                # sample variance - unbiased estimator of population variance\n","                variance_batch_reshaped *= sample_size / (sample_size - (1.0 + self.epsilon))\n","\n","            self.add_update([K.moving_average_update(self.moving_mean,\n","                                                     mean_batch_reshaped,\n","                                                     self.momentum),\n","                             K.moving_average_update(self.moving_variance,\n","                                                     variance_batch_reshaped,\n","                                                     self.momentum)],\n","                            inputs)\n","\n","            return normalize_func(mean_batch, variance_batch)\n","\n","        def inference_phase():\n","            mean_batch = self.moving_mean\n","            variance_batch = self.moving_variance\n","\n","            return normalize_func(mean_batch, variance_batch)\n","\n","        def normalize_func(mean_batch, variance_batch):\n","            mean_batch = K.reshape(mean_batch, broadcast_shape)\n","            variance_batch = K.reshape(variance_batch, broadcast_shape)\n","\n","            mean_weights = K.softmax(self.mean_weights, axis=0)\n","            variance_weights = K.softmax(self.variance_weights, axis=0)\n","\n","            mean = (mean_weights[0] * mean_instance +\n","                    mean_weights[1] * mean_layer +\n","                    mean_weights[2] * mean_batch)\n","\n","            variance = (variance_weights[0] * variance_instance +\n","                        variance_weights[1] * variance_layer +\n","                        variance_weights[2] * variance_batch)\n","\n","            outputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n","\n","            if self.scale:\n","                broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n","                outputs = outputs * broadcast_gamma\n","\n","            if self.center:\n","                broadcast_beta = K.reshape(self.beta, broadcast_shape)\n","                outputs = outputs + broadcast_beta\n","\n","            return outputs\n","\n","        if training in {0, False}:\n","            return inference_phase()\n","\n","        return K.in_train_phase(training_phase,\n","                                inference_phase,\n","                                training=training)\n","\n","    def get_config(self):\n","        config = {\n","            'axis': self.axis,\n","            'epsilon': self.epsilon,\n","            'momentum': self.momentum,\n","            'center': self.center,\n","            'scale': self.scale,\n","            'beta_initializer': initializers.serialize(self.beta_initializer),\n","            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n","            'mean_weights_initializer': initializers.serialize(self.mean_weights_initializer),\n","            'variance_weights_initializer': initializers.serialize(self.variance_weights_initializer),\n","            'moving_mean_initializer': initializers.serialize(self.moving_mean_initializer),\n","            'moving_variance_initializer': initializers.serialize(self.moving_variance_initializer),\n","            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n","            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n","            'mean_weights_regularizer': regularizers.serialize(self.mean_weights_regularizer),\n","            'variance_weights_regularizer': regularizers.serialize(self.variance_weights_regularizer),\n","            'beta_constraint': constraints.serialize(self.beta_constraint),\n","            'gamma_constraint': constraints.serialize(self.gamma_constraint),\n","            'mean_weights_constraints': constraints.serialize(self.mean_weights_constraints),\n","            'variance_weights_constraints': constraints.serialize(self.variance_weights_constraints),\n","        }\n","        base_config = super(SwitchNormalization, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","    \n","get_custom_objects().update({'SwitchNormalization': SwitchNormalization})\n","    \n","\"\"\"THIS\"\"\"\n","#!/usr/bin/env python\n","\n","\"\"\" Needs update!\n","\"\"\"\n","\n","import numpy as np\n","import tensorflow as tf\n","try:\n","    from keras.optimizers import RMSprop, Nadam\n","    from keras.models import Sequential, load_model, Model\n","    from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Flatten,\\\n","                             Input, Lambda, Add\n","    from keras import backend as K\n","\n","    K.set_image_dim_ordering('th')\n","except ImportError:\n","    from tensorflow.keras.optimizers import RMSprop, Nadam\n","    from tensorflow.keras.models import Sequential, load_model, Model\n","    from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D,\\\n","                                        Flatten, Input, Lambda, Add\n","\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","DENSES = {'dense': Dense,\n","          'noisy_dense_fg': NoisyDenseFG,\n","          'noisy_dense_ig': NoisyDenseIG}\n","\n","def select_optimizer(optimizer):\n","    assert optimizer in {'Nadam', 'RMSprop'}, \"Optimizer should be RMSprop or Nadam.\"\n","\n","    if optimizer == 'Nadam':\n","        optimizer = Nadam()\n","    else:\n","        optimizer = RMSprop()\n","\n","    return optimizer\n","\n","def select_error(error):\n","    assert type(error) is str, \"Should use string to select error.\"\n","\n","    if error == 'clipped_error':\n","        error = clipped_error\n","\n","    return error\n","\n","def CNN1(inputs):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","\n","    return model\n","\n","def CNN2(inputs):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = Flatten()(net)\n","\n","    return model\n","\n","def CNN3(inputs):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    \n","    net = Conv2D(32, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(64, (3, 3), activation = 'relu')(net)\n","    net = Conv2D(64, (3, 3), activation = 'relu')(net)\n","    net = Flatten()(net)\n","\n","    return net\n","\n","def create_cnn(cnn, inputs):\n","    if cnn == \"CNN1\":\n","        net = CNN1(inputs)\n","    elif cnn == \"CNN2\":\n","        net = CNN2(inputs)\n","    else:\n","        net = CNN3(inputs)\n","\n","    return net\n","\n","def create_model(optimizer, loss, stack, input_size, output_size,\n","                 dueling = False, cnn = \"CNN3\", dense_type = \"dense\"):\n","    # optimizer = select_optimizer(optimizer)\n","    # loss = select_error(loss)\n","    inputs = Input(shape = (stack, input_size, input_size))\n","    net = create_cnn(cnn, inputs)\n","\n","    if dueling:\n","        advt = DENSES[dense_type](3136, activation = 'relu')(net)\n","        advt = DENSES[dense_type](output_size)(advt)\n","        value = DENSES[dense_type](3136, activation = 'relu')(net)\n","        value = DENSES[dense_type](1)(value)\n","\n","        # now to combine the two streams\n","        advt = Lambda(lambda advt: advt - tf.reduce_mean(advt, axis = -1,\n","                                                         keepdims = True))(advt)\n","        value = Lambda(lambda value: tf.tile(value, [1, output_size]))(value)\n","        final = Add()([value, advt])\n","    else:\n","        final = DENSES[dense_type](3136, activation = 'relu')(net)\n","        final = DENSES[dense_type](output_size)(final)\n","\n","    model = Model(inputs = inputs, outputs = final)\n","   \n","    model.compile(optimizer = optimizer, loss = loss)\n","\n","    return model\n","\n","\n","from __future__ import absolute_import\n","\n","from keras import backend as K\n","from keras.optimizers import Optimizer\n","\n","from tensorflow.python.ops import state_ops\n","from tensorflow.python.ops import math_ops\n","from tensorflow.python.util.tf_export import tf_export\n","\n","if K.backend() == 'tensorflow':\n","    import tensorflow as tf\n","\n","class COCOB(Optimizer):\n","    \"\"\"COCOB-Backprop optimizer.\n","    It is recommended to leave the parameters of this optimizer\n","    at their default values\n","    (except the learning rate, which can be freely tuned).\n","    This optimizer, unlike other stochastic gradient based optimizers, optimize the function by\n","    finding individual learning rates in a coin-betting way.\n","    # Arguments\n","        alphs: float >= 0. Multiples of the largest absolute magtitude of gradients.\n","        epsilon: float >= 0. Fuzz factor.\n","    # References\n","        - [Training Deep Networks without Learning Rates Through Coin Betting](http://https://arxiv.org/pdf/1705.07795.pdf)\n","    \"\"\"\n","\n","    def __init__(self, alpha=100, epsilon=1e-8, **kwargs):\n","        super(COCOB, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.alpha = K.variable(alpha, name='alpha')\n","            self.iterations = K.variable(0., name='iterations')\n","        self.epsilon = epsilon\n","\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [K.update_add(self.iterations, 1)]\n","\n","        L = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n","        M = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n","        Reward = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n","        grad_sum = [K.zeros(K.get_variable_shape(p), dtype=K.dtype(p)) for p in params]\n","\n","        if K.eval(self.iterations) == 0:\n","            old_params = [K.constant(K.eval(p)) for p in params]\n","            # [K.eval(p) for p in params]\n","\n","        self.weights = [self.iterations] + L + M + Reward + grad_sum\n","\n","        for old_p, p, g, gs, l, m, r in zip(old_params, params, grads, grad_sum, L, M, Reward):\n","            # update accumulator\n","            # old_p = K.variable(old_p)\n","\n","            new_l = K.maximum(l, K.abs(g))\n","            self.updates.append(K.update(l, new_l))\n","\n","            new_m = m + K.abs(g)\n","            self.updates.append(K.update(m, new_m))\n","\n","            new_r = K.maximum(r - (p - old_p)*g, 0)\n","            self.updates.append(K.update(r, new_r))\n","\n","            new_gs = gs + g\n","            self.updates.append(K.update(gs, new_gs))\n","\n","            new_p = old_p - (new_gs/(self.epsilon + new_l*K.maximum(new_m+new_l, self.alpha*new_l)))*(new_l + new_r)\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'alpha': float(K.get_value(self.alpha)),\n","                  'epsilon': self.epsilon}\n","        base_config = super(COCOB, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class SMORMS3(Optimizer):\n","    '''SMORMS3 optimizer.\n","    Implemented based on http://sifter.org/~simon/journal/20150420.html\n","    # Arguments\n","        lr: float >= 0. Learning rate.\n","        epsilon: float >= 0. Fuzz factor.\n","        decay: float >= 0. Learning rate decay over each update.\n","    '''\n","\n","    def __init__(self, lr=0.001, epsilon=1e-16, decay=0.,\n","                 **kwargs):\n","        super(SMORMS3, self).__init__(**kwargs)\n","        self.__dict__.update(locals())\n","        with K.name_scope(self.__class__.__name__):\n","            self.lr = K.variable(lr)\n","            # self.rho = K.variable(rho)\n","            self.decay = K.variable(decay)\n","            self.inital_decay = decay\n","            self.iterations = K.variable(0.)\n","        self.epsilon = epsilon\n","        self.initial_decay = decay\n","\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        shapes = [K.get_variable_shape(p) for p in params]\n","        self.updates.append(K.update_add(self.iterations, 1))\n","\n","        g1s = [K.zeros(shape) for shape in shapes]\n","        g2s = [K.zeros(shape) for shape in shapes]\n","        mems = [K.ones(shape) for shape in shapes]\n","\n","        lr = self.lr\n","        if self.inital_decay > 0:\n","            lr *= (1. / (1. + self.decay * self.iterations))\n","\n","        self.weights = [self.iterations] + g1s + g2s + mems\n","\n","        for p, g, g1, g2, m in zip(params, grads, g1s, g2s, mems):\n","            r = 1. / (m + 1)\n","            new_g1 = (1. - r) * g1 + r * g\n","            new_g2 = (1. - r) * g2 + r * K.square(g)\n","            # update accumulators\n","            self.updates.append(K.update(g1, new_g1))\n","            self.updates.append(K.update(g2, new_g2))\n","            new_p = p - g * K.minimum(lr, K.square(new_g1) / (new_g2 + self.epsilon)) / (\n","            K.sqrt(new_g2) + self.epsilon)\n","            new_m = 1 + m * (1 - K.square(new_g1) / (new_g2 + self.epsilon))\n","            # update rho\n","            self.updates.append(K.update(m, new_m))\n","            # apply constraints\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {'lr': float(K.get_value(self.lr)),\n","                  'decay': float(K.get_value(self.decay)),\n","                  'epsilon': self.epsilon}\n","        base_config = super(SMORMS3, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","class Yogi(Optimizer):\n","    \"\"\"Yogi optimizer.\n","    Default parameters follow those provided in the original paper.\n","    Arguments:\n","      lr: float >= 0. Learning rate.\n","      beta_1: float, 0 < beta < 1. Generally close to 1.\n","      beta_2: float, 0 < beta < 1. Generally close to 1.\n","      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n","      decay: float >= 0. Learning rate decay over each update.\n","      amsgrad: boolean. Whether to apply the AMSGrad variant of this\n","          algorithm from the paper \"On the Convergence of Adam and\n","          Beyond\".\n","    \"\"\"\n","\n","    def __init__(self,\n","               lr=0.001,\n","               beta_1=0.9,\n","               beta_2=0.999,\n","               epsilon=None,\n","               decay=0.00000001,\n","               amsgrad=False,\n","               **kwargs):\n","        super(Yogi, self).__init__(**kwargs)\n","        with K.name_scope(self.__class__.__name__):\n","            self.iterations = K.variable(0, dtype='int64', name='iterations')\n","            self.lr = K.variable(lr, name='lr')\n","            self.beta_1 = K.variable(beta_1, name='beta_1')\n","            self.beta_2 = K.variable(beta_2, name='beta_2')\n","            self.decay = K.variable(decay, name='decay')\n","        if epsilon is None:\n","            epsilon = K.epsilon()\n","        self.epsilon = epsilon\n","        self.initial_decay = decay\n","        self.amsgrad = amsgrad\n","\n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [state_ops.assign_add(self.iterations, 1)]\n","\n","        lr = self.lr\n","        if self.initial_decay > 0:\n","            lr = lr * (  # pylint: disable=g-no-augmented-assignment\n","                1. / (1. + self.decay * math_ops.cast(self.iterations,\n","                                                    K.dtype(self.decay))))\n","\n","        t = math_ops.cast(self.iterations, K.floatx()) + 1\n","        lr_t = lr * (\n","            K.sqrt(1. - math_ops.pow(self.beta_2, t)) /\n","            (1. - math_ops.pow(self.beta_1, t)))\n","\n","        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        if self.amsgrad:\n","            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n","        else:\n","            vhats = [K.zeros(1) for _ in params]\n","        self.weights = [self.iterations] + ms + vs + vhats\n","\n","        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n","            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n","            #v_t = (self.beta_2 * v) + (1. - self.beta_2) * math_ops.square(g) # from amsgrad\n","            v_t = v - (1-self.beta_2)*K.sign(v-math_ops.square(g))*math_ops.square(g)\n","            p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n","\n","            self.updates.append(state_ops.assign(m, m_t))\n","            self.updates.append(state_ops.assign(v, v_t))\n","            new_p = p_t\n","\n","            # Apply constraints.\n","            if getattr(p, 'constraint', None) is not None:\n","                new_p = p.constraint(new_p)\n","\n","            self.updates.append(state_ops.assign(p, new_p))\n","        return self.updates\n","\n","    def get_config(self):\n","        config = {\n","            'lr': float(K.get_value(self.lr)),\n","            'beta_1': float(K.get_value(self.beta_1)),\n","            'beta_2': float(K.get_value(self.beta_2)),\n","            'decay': float(K.get_value(self.decay)),\n","            'epsilon': self.epsilon,\n","            'amsgrad': self.amsgrad\n","            }\n","        base_config = super(Yogi, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","class SGD2(Optimizer):\n","    \"\"\"Stochastic gradient descent using second-order information.\n","   Ye, C., Yang, Y., Fermuller, C., & Aloimonos, Y. (2017).\n","   On the Importance of Consistency in Training Deep Neural Networks. arXiv pre\n","   arXiv:1708.00631.\n","     # Arguments\n","        lr: float >= 0. Learning rate.\n","        momentum: float >= 0. Parameter updates momentum.\n","        decay: float >= 0. Learning rate decay over each update.\n","        nesterov: boolean. Whether to apply Nesterov momentum.\n","    \"\"\"\n","    def __init__(self, lr=0.01, momentum=0., decay=0.,\n","                 nesterov=False, **kwargs):\n","        super(SGD2, self).__init__(**kwargs)\n","        \n","        with K.name_scope(self.__class__.__name__):\n","            self.iterations = K.variable(0, dtype='int64', name='iterations')\n","            self.lr = K.variable(lr, name='lr')\n","            self.momentum = K.variable(momentum, name='momentum')\n","            self.decay = K.variable(decay, name='decay')\n","        self.initial_decay = decay\n","    \n","    def get_updates(self, loss, params):\n","        grads = self.get_gradients(loss, params)\n","        self.updates = [K.update_add(self.iterations, 1)]\n","        lr = self.lr\n","        if self.initial_decay > 0:\n","            lr *= (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n","        layer_count = 0\n","        lambda_value = 0.01\n","        # momentum\n","        shapes = [K.int_shape(p) for p in params ]\n","        moments = [K.zeros(shape) for shape in shapes]\n","        self.weights = [self.iterations] + moments\n","\n","        for p, g, m in zip(params, grads, moments):\n","            # gradients correction by second order information\n","            if len(K.int_shape(g)) > 1:\n","                x = self.layer_inputs[layer_count]\n","                shape_g = K.int_shape(g)\n","                # First permute the x, then compute transpose of x\n","                layer_count = layer_count + 1\n","                # For 3 channel image\n","                if len(K.int_shape(x)) == 4:\n","                    x = tf.transpose(x, perm=[3, 0, 1, 2])\n","                    x = tf.reshape(x, [K.int_shape(x)[0], -1])\n","                    x = tf.matmul(x, tf.transpose(x))\n","                    g = tf.transpose(g, perm=[2, 0, 1, 3])\n","                    g = tf.reshape(g, [K.int_shape(g)[0], -1])\n","\n","                elif len(K.int_shape(x)) == 2:\n","                    xt = tf.reshape(x, [K.int_shape(x)[1], -1])\n","                    x = tf.matmul(xt, x)\n","                    x = tf.reshape(x, [K.int_shape(x)[0], K.int_shape(x)[0]])\n","\n","                lambda_eye =  tf.eye(K.int_shape(x)[0])\n","                corr_term = tf.matrix_inverse(tf.add(x, tf.multiply(lambda_value, lambda_eye)))\n","                g = tf.matmul(corr_term, g)\n","                g = tf.reshape(g, shape_g)\n","\n","            v = self.momentum * m - lr * g  # velocity\n","            self.updates.append(K.update(m, v))\n","            new_p = p + v\n","            self.updates.append(K.update(p, new_p))\n","        return self.updates\n","    def set_layerinput(self, layer_inputs):\n","        self.layer_inputs = layer_inputs\n","        \n","    def get_config(self):\n","        config = {'lr': float(K.get_value(self.lr)),\n","                  'momentum': float(K.get_value(self.momentum)),\n","                  'decay': float(K.get_value(self.decay))}\n","        base_config = super(SGD2, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","import inspect\n","import keras\n","from keras import backend as K\n","\n","\n","def _get_shape(x):\n","    if hasattr(x, 'dense_shape'):\n","        return x.dense_shape\n","\n","    return K.shape(x)\n","\n","\n","def add_gradient_noise(BaseOptimizer):\n","    \"\"\"\n","    Given a Keras-compatible optimizer class, returns a modified class that\n","    supports adding gradient noise as introduced in this paper:\n","    https://arxiv.org/abs/1511.06807\n","    The relevant parameters from equation 1 in the paper can be set via\n","    noise_eta and noise_gamma, set by default to 0.3 and 0.55 respectively.\n","    \"\"\"\n","    if not (\n","        inspect.isclass(BaseOptimizer) and\n","        issubclass(BaseOptimizer, keras.optimizers.Optimizer)\n","    ):\n","        raise ValueError(\n","            'add_gradient_noise() expects a valid Keras optimizer'\n","        )\n","\n","    class NoisyOptimizer(BaseOptimizer):\n","        def __init__(self, noise_eta=0.3, noise_gamma=0.55, **kwargs):\n","            super(NoisyOptimizer, self).__init__(**kwargs)\n","            with K.name_scope(self.__class__.__name__):\n","                self.noise_eta = K.variable(noise_eta, name='noise_eta')\n","                self.noise_gamma = K.variable(noise_gamma, name='noise_gamma')\n","\n","        def get_gradients(self, loss, params):\n","            grads = super(NoisyOptimizer, self).get_gradients(loss, params)\n","\n","            # Add decayed gaussian noise\n","            t = K.cast(self.iterations, K.dtype(grads[0]))\n","            variance = self.noise_eta / ((1 + t) ** self.noise_gamma)\n","\n","            grads = [\n","                grad + K.random_normal(\n","                    _get_shape(grad),\n","                    mean=0.0,\n","                    stddev=K.sqrt(variance),\n","                    dtype=K.dtype(grads[0])\n","                )\n","                for grad in grads\n","            ]\n","\n","            return grads\n","\n","        def get_config(self):\n","            config = {'noise_eta': float(K.get_value(self.noise_eta)),\n","                      'noise_gamma': float(K.get_value(self.noise_gamma))}\n","            base_config = super(NoisyOptimizer, self).get_config()\n","            return dict(list(base_config.items()) + list(config.items()))\n","\n","    NoisyOptimizer.__name__ = 'Noisy{}'.format(BaseOptimizer.__name__)\n","\n","    return NoisyOptimizer\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import Sequential, load_model, Model\n","from keras.layers import *\n","from keras.losses import *\n","\n","K.set_image_dim_ordering('th')  \n","  \n","board_size = 10\n","nb_frames = 4\n","  \n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","optimizer = add_gradient_noise(RMSprop)(noise_eta = 0.01)\n","error = tf.losses.huber_loss\n","\n","with tf.Session() as sess:\n","    model = create_model(optimizer = optimizer, loss = error,\n","                                 stack = nb_frames, input_size = board_size,\n","                                 output_size = game.nb_actions,\n","                                 dueling = False, cnn = 'CNN3',\n","                                 dense_type = 'dense')\n","    target = None\n","    sess.run(tf.global_variables_initializer())\n","    agent = Agent(model = model, sess = sess, target = target, memory_size = -1,\n","                      nb_frames = nb_frames, board_size = board_size,\n","                      per = False, update_target_freq = 500)\n","    agent.train(game, batch_size = 64, nb_epoch = 10000,\n","                    gamma = 0.95, n_steps = 1, policy = 'GreedyQPolicy')\n","    agent.test(game)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 010/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.2 | Wins: 0 | Win percentage: 0.0%\n","Epoch: 020/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 10.2 | Wins: 3 | Win percentage: 15.0%\n","Epoch: 030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.1 | Wins: 3 | Win percentage: 10.0%\n","Epoch: 040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 4 | Win percentage: 10.0%\n","Epoch: 050/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.0 | Wins: 5 | Win percentage: 10.0%\n","Epoch: 060/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 7 | Win percentage: 11.7%\n","Epoch: 070/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 8 | Win percentage: 11.4%\n","Epoch: 080/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.6 | Wins: 10 | Win percentage: 12.5%\n","Epoch: 090/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 23.3 | Wins: 11 | Win percentage: 12.2%\n","Epoch: 100/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 45.0 | Wins: 14 | Win percentage: 14.0%\n","Epoch: 110/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 82.8 | Wins: 18 | Win percentage: 16.4%\n","Epoch: 120/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 88.0 | Wins: 23 | Win percentage: 19.2%\n","Epoch: 130/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 25 | Win percentage: 19.2%\n","Epoch: 140/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 39.1 | Wins: 28 | Win percentage: 20.0%\n","Epoch: 150/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 114.1 | Wins: 34 | Win percentage: 22.7%\n","Epoch: 160/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 164.0 | Wins: 40 | Win percentage: 25.0%\n","Epoch: 170/10000 | Mean size 10: 4.4 | Longest 10: 007 | Mean steps 10: 163.0 | Wins: 46 | Win percentage: 27.1%\n","Epoch: 180/10000 | Mean size 10: 4.2 | Longest 10: 006 | Mean steps 10: 136.1 | Wins: 53 | Win percentage: 29.4%\n","Epoch: 190/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 120.8 | Wins: 59 | Win percentage: 31.1%\n","Epoch: 200/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 46.1 | Wins: 62 | Win percentage: 31.0%\n","Epoch: 210/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 23.0 | Wins: 64 | Win percentage: 30.5%\n","Epoch: 220/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 28.9 | Wins: 66 | Win percentage: 30.0%\n","Epoch: 230/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 32.8 | Wins: 69 | Win percentage: 30.0%\n","Epoch: 240/10000 | Mean size 10: 3.6 | Longest 10: 006 | Mean steps 10: 66.8 | Wins: 73 | Win percentage: 30.4%\n","Epoch: 250/10000 | Mean size 10: 3.7 | Longest 10: 008 | Mean steps 10: 66.5 | Wins: 76 | Win percentage: 30.4%\n","Epoch: 260/10000 | Mean size 10: 3.6 | Longest 10: 006 | Mean steps 10: 90.0 | Wins: 80 | Win percentage: 30.8%\n","Epoch: 270/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 62.0 | Wins: 83 | Win percentage: 30.7%\n","Epoch: 280/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 37.8 | Wins: 84 | Win percentage: 30.0%\n","Epoch: 290/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 71.6 | Wins: 90 | Win percentage: 31.0%\n","Epoch: 300/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 22.2 | Wins: 92 | Win percentage: 30.7%\n","Epoch: 310/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.0 | Wins: 92 | Win percentage: 29.7%\n","Epoch: 320/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 37.4 | Wins: 93 | Win percentage: 29.1%\n","Epoch: 330/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 37.7 | Wins: 97 | Win percentage: 29.4%\n","Epoch: 340/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 31.4 | Wins: 98 | Win percentage: 28.8%\n","Epoch: 350/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 66.0 | Wins: 102 | Win percentage: 29.1%\n","Epoch: 360/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 70.6 | Wins: 104 | Win percentage: 28.9%\n","Epoch: 370/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 35.0 | Wins: 106 | Win percentage: 28.6%\n","Epoch: 380/10000 | Mean size 10: 4.5 | Longest 10: 008 | Mean steps 10: 73.1 | Wins: 112 | Win percentage: 29.5%\n","Epoch: 390/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 73.6 | Wins: 116 | Win percentage: 29.7%\n","Epoch: 400/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 59.0 | Wins: 120 | Win percentage: 30.0%\n","Epoch: 410/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 59.7 | Wins: 122 | Win percentage: 29.8%\n","Epoch: 420/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 40.8 | Wins: 125 | Win percentage: 29.8%\n","Epoch: 430/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 67.3 | Wins: 127 | Win percentage: 29.5%\n","Epoch: 440/10000 | Mean size 10: 3.3 | Longest 10: 006 | Mean steps 10: 47.2 | Wins: 128 | Win percentage: 29.1%\n","Epoch: 450/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 60.5 | Wins: 132 | Win percentage: 29.3%\n","Epoch: 460/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 42.8 | Wins: 135 | Win percentage: 29.3%\n","Epoch: 470/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 68.0 | Wins: 138 | Win percentage: 29.4%\n","Epoch: 480/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 43.7 | Wins: 140 | Win percentage: 29.2%\n","Epoch: 490/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 63.8 | Wins: 144 | Win percentage: 29.4%\n","Epoch: 500/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 47.0 | Wins: 146 | Win percentage: 29.2%\n","Epoch: 510/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 41.3 | Wins: 150 | Win percentage: 29.4%\n","Epoch: 520/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 48.2 | Wins: 150 | Win percentage: 28.8%\n","Epoch: 530/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 68.1 | Wins: 153 | Win percentage: 28.9%\n","Epoch: 540/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 88.2 | Wins: 159 | Win percentage: 29.4%\n","Epoch: 550/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 53.2 | Wins: 160 | Win percentage: 29.1%\n","Epoch: 560/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 39.3 | Wins: 162 | Win percentage: 28.9%\n","Epoch: 570/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 62.1 | Wins: 167 | Win percentage: 29.3%\n","Epoch: 580/10000 | Mean size 10: 3.9 | Longest 10: 007 | Mean steps 10: 49.7 | Wins: 172 | Win percentage: 29.7%\n","Epoch: 590/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 69.7 | Wins: 174 | Win percentage: 29.5%\n","Epoch: 600/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 54.4 | Wins: 179 | Win percentage: 29.8%\n","Epoch: 610/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 45.9 | Wins: 182 | Win percentage: 29.8%\n","Epoch: 620/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 43.3 | Wins: 186 | Win percentage: 30.0%\n","Epoch: 630/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 36.3 | Wins: 192 | Win percentage: 30.5%\n","Epoch: 640/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 27.8 | Wins: 194 | Win percentage: 30.3%\n","Epoch: 650/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 49.2 | Wins: 198 | Win percentage: 30.5%\n","Epoch: 660/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 52.5 | Wins: 201 | Win percentage: 30.5%\n","Epoch: 670/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 35.9 | Wins: 202 | Win percentage: 30.1%\n","Epoch: 680/10000 | Mean size 10: 3.4 | Longest 10: 006 | Mean steps 10: 29.6 | Wins: 204 | Win percentage: 30.0%\n","Epoch: 690/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.9 | Wins: 204 | Win percentage: 29.6%\n","Epoch: 700/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 28.2 | Wins: 207 | Win percentage: 29.6%\n","Epoch: 710/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 41.1 | Wins: 209 | Win percentage: 29.4%\n","Epoch: 720/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 47.3 | Wins: 213 | Win percentage: 29.6%\n","Epoch: 730/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 29.5 | Wins: 216 | Win percentage: 29.6%\n","Epoch: 740/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 39.9 | Wins: 219 | Win percentage: 29.6%\n","Epoch: 750/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 38.4 | Wins: 222 | Win percentage: 29.6%\n","Epoch: 760/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 38.3 | Wins: 224 | Win percentage: 29.5%\n","Epoch: 770/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 55.7 | Wins: 227 | Win percentage: 29.5%\n","Epoch: 780/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 37.4 | Wins: 232 | Win percentage: 29.7%\n","Epoch: 790/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 24.8 | Wins: 233 | Win percentage: 29.5%\n","Epoch: 800/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 41.5 | Wins: 234 | Win percentage: 29.2%\n","Epoch: 810/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 43.8 | Wins: 238 | Win percentage: 29.4%\n","Epoch: 820/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 60.4 | Wins: 242 | Win percentage: 29.5%\n","Epoch: 830/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 46.5 | Wins: 248 | Win percentage: 29.9%\n","Epoch: 840/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 32.6 | Wins: 250 | Win percentage: 29.8%\n","Epoch: 850/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 32.7 | Wins: 253 | Win percentage: 29.8%\n","Epoch: 860/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.6 | Wins: 255 | Win percentage: 29.7%\n","Epoch: 870/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 54.0 | Wins: 259 | Win percentage: 29.8%\n","Epoch: 880/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 23.8 | Wins: 260 | Win percentage: 29.5%\n","Epoch: 890/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 30.6 | Wins: 263 | Win percentage: 29.6%\n","Epoch: 900/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 49.9 | Wins: 266 | Win percentage: 29.6%\n","Epoch: 910/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 32.2 | Wins: 268 | Win percentage: 29.5%\n","Epoch: 920/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 50.8 | Wins: 274 | Win percentage: 29.8%\n","Epoch: 930/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 53.0 | Wins: 279 | Win percentage: 30.0%\n","Epoch: 940/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 43.4 | Wins: 283 | Win percentage: 30.1%\n","Epoch: 950/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 37.3 | Wins: 287 | Win percentage: 30.2%\n","Epoch: 960/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 60.4 | Wins: 293 | Win percentage: 30.5%\n","Epoch: 970/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 21.0 | Wins: 295 | Win percentage: 30.4%\n","Epoch: 980/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 25.5 | Wins: 295 | Win percentage: 30.1%\n","Epoch: 990/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 21.4 | Wins: 295 | Win percentage: 29.8%\n","Epoch: 1000/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 35.3 | Wins: 298 | Win percentage: 29.8%\n","Epoch: 1010/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 35.2 | Wins: 300 | Win percentage: 29.7%\n","Epoch: 1020/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 56.9 | Wins: 304 | Win percentage: 29.8%\n","Epoch: 1030/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 66.8 | Wins: 306 | Win percentage: 29.7%\n","Epoch: 1040/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 33.0 | Wins: 308 | Win percentage: 29.6%\n","Epoch: 1050/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 45.2 | Wins: 309 | Win percentage: 29.4%\n","Epoch: 1060/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 52.4 | Wins: 316 | Win percentage: 29.8%\n","Epoch: 1070/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 61.4 | Wins: 318 | Win percentage: 29.7%\n","Epoch: 1080/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 77.0 | Wins: 319 | Win percentage: 29.5%\n","Epoch: 1090/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 65.5 | Wins: 325 | Win percentage: 29.8%\n","Epoch: 1100/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 55.0 | Wins: 328 | Win percentage: 29.8%\n","Epoch: 1110/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 34.5 | Wins: 329 | Win percentage: 29.6%\n","Epoch: 1120/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 57.3 | Wins: 335 | Win percentage: 29.9%\n","Epoch: 1130/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 41.1 | Wins: 337 | Win percentage: 29.8%\n","Epoch: 1140/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 54.0 | Wins: 340 | Win percentage: 29.8%\n","Epoch: 1150/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 42.8 | Wins: 343 | Win percentage: 29.8%\n","Epoch: 1160/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 45.6 | Wins: 346 | Win percentage: 29.8%\n","Epoch: 1170/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 63.7 | Wins: 350 | Win percentage: 29.9%\n","Epoch: 1180/10000 | Mean size 10: 3.6 | Longest 10: 006 | Mean steps 10: 77.0 | Wins: 354 | Win percentage: 30.0%\n","Epoch: 1190/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 77.0 | Wins: 357 | Win percentage: 30.0%\n","Epoch: 1200/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 87.0 | Wins: 363 | Win percentage: 30.2%\n","Epoch: 1210/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 52.3 | Wins: 366 | Win percentage: 30.2%\n","Epoch: 1220/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 40.9 | Wins: 369 | Win percentage: 30.2%\n","Epoch: 1230/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 36.0 | Wins: 371 | Win percentage: 30.2%\n","Epoch: 1240/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 44.8 | Wins: 372 | Win percentage: 30.0%\n","Epoch: 1250/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 68.9 | Wins: 374 | Win percentage: 29.9%\n","Epoch: 1260/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 63.1 | Wins: 377 | Win percentage: 29.9%\n","Epoch: 1270/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 36.3 | Wins: 380 | Win percentage: 29.9%\n","Epoch: 1280/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 29.9 | Wins: 380 | Win percentage: 29.7%\n","Epoch: 1290/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 41.4 | Wins: 383 | Win percentage: 29.7%\n","Epoch: 1300/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 33.0 | Wins: 389 | Win percentage: 29.9%\n","Epoch: 1310/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 96.9 | Wins: 395 | Win percentage: 30.2%\n","Epoch: 1320/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 56.7 | Wins: 399 | Win percentage: 30.2%\n","Epoch: 1330/10000 | Mean size 10: 3.4 | Longest 10: 006 | Mean steps 10: 33.7 | Wins: 401 | Win percentage: 30.2%\n","Epoch: 1340/10000 | Mean size 10: 3.9 | Longest 10: 005 | Mean steps 10: 66.9 | Wins: 408 | Win percentage: 30.4%\n","Epoch: 1350/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 54.3 | Wins: 412 | Win percentage: 30.5%\n","Epoch: 1360/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 38.1 | Wins: 412 | Win percentage: 30.3%\n","Epoch: 1370/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 39.0 | Wins: 415 | Win percentage: 30.3%\n","Epoch: 1380/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 43.0 | Wins: 417 | Win percentage: 30.2%\n","Epoch: 1390/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 47.1 | Wins: 421 | Win percentage: 30.3%\n","Epoch: 1400/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 44.8 | Wins: 425 | Win percentage: 30.4%\n","Epoch: 1410/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 50.6 | Wins: 428 | Win percentage: 30.4%\n","Epoch: 1420/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 32.5 | Wins: 430 | Win percentage: 30.3%\n","Epoch: 1430/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 26.8 | Wins: 433 | Win percentage: 30.3%\n","Epoch: 1440/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 36.5 | Wins: 438 | Win percentage: 30.4%\n","Epoch: 1450/10000 | Mean size 10: 3.5 | Longest 10: 006 | Mean steps 10: 48.6 | Wins: 441 | Win percentage: 30.4%\n","Epoch: 1460/10000 | Mean size 10: 3.5 | Longest 10: 006 | Mean steps 10: 68.8 | Wins: 444 | Win percentage: 30.4%\n","Epoch: 1470/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 91.2 | Wins: 450 | Win percentage: 30.6%\n","Epoch: 1480/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 34.0 | Wins: 450 | Win percentage: 30.4%\n","Epoch: 1490/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 47.0 | Wins: 452 | Win percentage: 30.3%\n","Epoch: 1500/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 52.3 | Wins: 454 | Win percentage: 30.3%\n","Epoch: 1510/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 37.7 | Wins: 457 | Win percentage: 30.3%\n","Epoch: 1520/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 64.5 | Wins: 461 | Win percentage: 30.3%\n","Epoch: 1530/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 33.4 | Wins: 465 | Win percentage: 30.4%\n","Epoch: 1540/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 27.0 | Wins: 467 | Win percentage: 30.3%\n","Epoch: 1550/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 78.1 | Wins: 471 | Win percentage: 30.4%\n","Epoch: 1560/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 54.3 | Wins: 474 | Win percentage: 30.4%\n","Epoch: 1570/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 32.4 | Wins: 478 | Win percentage: 30.4%\n","Epoch: 1580/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 63.2 | Wins: 482 | Win percentage: 30.5%\n","Epoch: 1590/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 58.0 | Wins: 485 | Win percentage: 30.5%\n","Epoch: 1600/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 53.3 | Wins: 489 | Win percentage: 30.6%\n","Epoch: 1610/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 46.0 | Wins: 493 | Win percentage: 30.6%\n","Epoch: 1620/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 42.1 | Wins: 498 | Win percentage: 30.7%\n","Epoch: 1630/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 55.7 | Wins: 500 | Win percentage: 30.7%\n","Epoch: 1640/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 40.9 | Wins: 503 | Win percentage: 30.7%\n","Epoch: 1650/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 42.9 | Wins: 507 | Win percentage: 30.7%\n","Epoch: 1660/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 33.7 | Wins: 510 | Win percentage: 30.7%\n","Epoch: 1670/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 34.4 | Wins: 511 | Win percentage: 30.6%\n","Epoch: 1680/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 43.7 | Wins: 512 | Win percentage: 30.5%\n","Epoch: 1690/10000 | Mean size 10: 3.3 | Longest 10: 006 | Mean steps 10: 31.1 | Wins: 513 | Win percentage: 30.4%\n","Epoch: 1700/10000 | Mean size 10: 3.9 | Longest 10: 007 | Mean steps 10: 34.0 | Wins: 517 | Win percentage: 30.4%\n","Epoch: 1710/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 33.8 | Wins: 520 | Win percentage: 30.4%\n","Epoch: 1720/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 46.8 | Wins: 526 | Win percentage: 30.6%\n","Epoch: 1730/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 50.7 | Wins: 529 | Win percentage: 30.6%\n","Epoch: 1740/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 35.2 | Wins: 531 | Win percentage: 30.5%\n","Epoch: 1750/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 33.4 | Wins: 535 | Win percentage: 30.6%\n","Epoch: 1760/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 78.7 | Wins: 540 | Win percentage: 30.7%\n","Epoch: 1770/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 54.5 | Wins: 543 | Win percentage: 30.7%\n","Epoch: 1780/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 76.0 | Wins: 549 | Win percentage: 30.8%\n","Epoch: 1790/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 44.2 | Wins: 553 | Win percentage: 30.9%\n","Epoch: 1800/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 36.4 | Wins: 556 | Win percentage: 30.9%\n","Epoch: 1810/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 557 | Win percentage: 30.8%\n","Epoch: 1820/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.6 | Wins: 557 | Win percentage: 30.6%\n","Epoch: 1830/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.3 | Wins: 558 | Win percentage: 30.5%\n","Epoch: 1840/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.5 | Wins: 559 | Win percentage: 30.4%\n","Epoch: 1850/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 18.9 | Wins: 563 | Win percentage: 30.4%\n","Epoch: 1860/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.8 | Wins: 565 | Win percentage: 30.4%\n","Epoch: 1870/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.5 | Wins: 565 | Win percentage: 30.2%\n","Epoch: 1880/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.7 | Wins: 565 | Win percentage: 30.1%\n","Epoch: 1890/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.0 | Wins: 565 | Win percentage: 29.9%\n","Epoch: 1900/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.6 | Wins: 565 | Win percentage: 29.7%\n","Epoch: 1910/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.0 | Wins: 566 | Win percentage: 29.6%\n","Epoch: 1920/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.4 | Wins: 567 | Win percentage: 29.5%\n","Epoch: 1930/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.6 | Wins: 569 | Win percentage: 29.5%\n","Epoch: 1940/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 7.7 | Wins: 570 | Win percentage: 29.4%\n","Epoch: 1950/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 17.8 | Wins: 573 | Win percentage: 29.4%\n","Epoch: 1960/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.3 | Wins: 576 | Win percentage: 29.4%\n","Epoch: 1970/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.2 | Wins: 577 | Win percentage: 29.3%\n","Epoch: 1980/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.4 | Wins: 577 | Win percentage: 29.1%\n","Epoch: 1990/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.3 | Wins: 577 | Win percentage: 29.0%\n","Epoch: 2000/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.5 | Wins: 577 | Win percentage: 28.9%\n","Epoch: 2010/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 7.8 | Wins: 578 | Win percentage: 28.8%\n","Epoch: 2020/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.7 | Wins: 578 | Win percentage: 28.6%\n","Epoch: 2030/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 579 | Win percentage: 28.5%\n","Epoch: 2040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 580 | Win percentage: 28.4%\n","Epoch: 2050/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.0 | Wins: 582 | Win percentage: 28.4%\n","Epoch: 2060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.9 | Wins: 583 | Win percentage: 28.3%\n","Epoch: 2070/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.4 | Wins: 583 | Win percentage: 28.2%\n","Epoch: 2080/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.9 | Wins: 585 | Win percentage: 28.1%\n","Epoch: 2090/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 586 | Win percentage: 28.0%\n","Epoch: 2100/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 15.2 | Wins: 588 | Win percentage: 28.0%\n","Epoch: 2110/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.9 | Wins: 590 | Win percentage: 28.0%\n","Epoch: 2120/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.9 | Wins: 590 | Win percentage: 27.8%\n","Epoch: 2130/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 591 | Win percentage: 27.7%\n","Epoch: 2140/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 23.7 | Wins: 596 | Win percentage: 27.9%\n","Epoch: 2150/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.8 | Wins: 596 | Win percentage: 27.7%\n","Epoch: 2160/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.8 | Wins: 596 | Win percentage: 27.6%\n","Epoch: 2170/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 598 | Win percentage: 27.6%\n","Epoch: 2180/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 8.6 | Wins: 600 | Win percentage: 27.5%\n","Epoch: 2190/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 601 | Win percentage: 27.4%\n","Epoch: 2200/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 17.6 | Wins: 603 | Win percentage: 27.4%\n","Epoch: 2210/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.1 | Wins: 604 | Win percentage: 27.3%\n","Epoch: 2220/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.5 | Wins: 606 | Win percentage: 27.3%\n","Epoch: 2230/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 607 | Win percentage: 27.2%\n","Epoch: 2240/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.5 | Wins: 607 | Win percentage: 27.1%\n","Epoch: 2250/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.8 | Wins: 607 | Win percentage: 27.0%\n","Epoch: 2260/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.1 | Wins: 607 | Win percentage: 26.9%\n","Epoch: 2270/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 608 | Win percentage: 26.8%\n","Epoch: 2280/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.9 | Wins: 608 | Win percentage: 26.7%\n","Epoch: 2290/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.6 | Wins: 609 | Win percentage: 26.6%\n","Epoch: 2300/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 612 | Win percentage: 26.6%\n","Epoch: 2310/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.1 | Wins: 613 | Win percentage: 26.5%\n","Epoch: 2320/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 614 | Win percentage: 26.5%\n","Epoch: 2330/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 13.9 | Wins: 616 | Win percentage: 26.4%\n","Epoch: 2340/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 617 | Win percentage: 26.4%\n","Epoch: 2350/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 619 | Win percentage: 26.3%\n","Epoch: 2360/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 620 | Win percentage: 26.3%\n","Epoch: 2370/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.8 | Wins: 621 | Win percentage: 26.2%\n","Epoch: 2380/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.9 | Wins: 621 | Win percentage: 26.1%\n","Epoch: 2390/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 622 | Win percentage: 26.0%\n","Epoch: 2400/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 623 | Win percentage: 26.0%\n","Epoch: 2410/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.6 | Wins: 624 | Win percentage: 25.9%\n","Epoch: 2420/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.9 | Wins: 624 | Win percentage: 25.8%\n","Epoch: 2430/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.0 | Wins: 626 | Win percentage: 25.8%\n","Epoch: 2440/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.2 | Wins: 627 | Win percentage: 25.7%\n","Epoch: 2450/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.9 | Wins: 627 | Win percentage: 25.6%\n","Epoch: 2460/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 10.3 | Wins: 630 | Win percentage: 25.6%\n","Epoch: 2470/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.6 | Wins: 631 | Win percentage: 25.5%\n","Epoch: 2480/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 632 | Win percentage: 25.5%\n","Epoch: 2490/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.8 | Wins: 633 | Win percentage: 25.4%\n","Epoch: 2500/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 22.9 | Wins: 637 | Win percentage: 25.5%\n","Epoch: 2510/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.9 | Wins: 637 | Win percentage: 25.4%\n","Epoch: 2520/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.7 | Wins: 638 | Win percentage: 25.3%\n","Epoch: 2530/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 639 | Win percentage: 25.3%\n","Epoch: 2540/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.3 | Wins: 639 | Win percentage: 25.2%\n","Epoch: 2550/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.5 | Wins: 640 | Win percentage: 25.1%\n","Epoch: 2560/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 641 | Win percentage: 25.0%\n","Epoch: 2570/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.7 | Wins: 641 | Win percentage: 24.9%\n","Epoch: 2580/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.4 | Wins: 642 | Win percentage: 24.9%\n","Epoch: 2590/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.3 | Wins: 643 | Win percentage: 24.8%\n","Epoch: 2600/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.0 | Wins: 645 | Win percentage: 24.8%\n","Epoch: 2610/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.2 | Wins: 645 | Win percentage: 24.7%\n","Epoch: 2620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 646 | Win percentage: 24.7%\n","Epoch: 2630/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.8 | Wins: 648 | Win percentage: 24.6%\n","Epoch: 2640/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.2 | Wins: 649 | Win percentage: 24.6%\n","Epoch: 2650/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.9 | Wins: 649 | Win percentage: 24.5%\n","Epoch: 2660/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 650 | Win percentage: 24.4%\n","Epoch: 2670/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.6 | Wins: 652 | Win percentage: 24.4%\n","Epoch: 2680/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.4 | Wins: 652 | Win percentage: 24.3%\n","Epoch: 2690/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 21.0 | Wins: 654 | Win percentage: 24.3%\n","Epoch: 2700/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.8 | Wins: 657 | Win percentage: 24.3%\n","Epoch: 2710/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.4 | Wins: 657 | Win percentage: 24.2%\n","Epoch: 2720/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 658 | Win percentage: 24.2%\n","Epoch: 2730/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.2 | Wins: 658 | Win percentage: 24.1%\n","Epoch: 2740/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 11.4 | Wins: 660 | Win percentage: 24.1%\n","Epoch: 2750/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 11.4 | Wins: 662 | Win percentage: 24.1%\n","Epoch: 2760/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 663 | Win percentage: 24.0%\n","Epoch: 2770/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 10.8 | Wins: 665 | Win percentage: 24.0%\n","Epoch: 2780/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 10.8 | Wins: 666 | Win percentage: 24.0%\n","Epoch: 2790/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 12.3 | Wins: 667 | Win percentage: 23.9%\n","Epoch: 2800/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.3 | Wins: 670 | Win percentage: 23.9%\n","Epoch: 2810/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.7 | Wins: 670 | Win percentage: 23.8%\n","Epoch: 2820/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.4 | Wins: 671 | Win percentage: 23.8%\n","Epoch: 2830/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 673 | Win percentage: 23.8%\n","Epoch: 2840/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 674 | Win percentage: 23.7%\n","Epoch: 2850/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.4 | Wins: 675 | Win percentage: 23.7%\n","Epoch: 2860/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.2 | Wins: 676 | Win percentage: 23.6%\n","Epoch: 2870/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.4 | Wins: 676 | Win percentage: 23.6%\n","Epoch: 2880/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 19.0 | Wins: 677 | Win percentage: 23.5%\n","Epoch: 2890/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.3 | Wins: 677 | Win percentage: 23.4%\n","Epoch: 2900/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 9.1 | Wins: 680 | Win percentage: 23.4%\n","Epoch: 2910/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 682 | Win percentage: 23.4%\n","Epoch: 2920/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 21.4 | Wins: 686 | Win percentage: 23.5%\n","Epoch: 2930/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 10.7 | Wins: 687 | Win percentage: 23.4%\n","Epoch: 2940/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 16.6 | Wins: 690 | Win percentage: 23.5%\n","Epoch: 2950/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.6 | Wins: 691 | Win percentage: 23.4%\n","Epoch: 2960/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.0 | Wins: 691 | Win percentage: 23.3%\n","Epoch: 2970/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 16.3 | Wins: 692 | Win percentage: 23.3%\n","Epoch: 2980/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 12.8 | Wins: 694 | Win percentage: 23.3%\n","Epoch: 2990/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 10.2 | Wins: 697 | Win percentage: 23.3%\n","Epoch: 3000/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 698 | Win percentage: 23.3%\n","Epoch: 3010/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 11.2 | Wins: 700 | Win percentage: 23.3%\n","Epoch: 3020/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.3 | Wins: 700 | Win percentage: 23.2%\n","Epoch: 3030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.2 | Wins: 700 | Win percentage: 23.1%\n","Epoch: 3040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 701 | Win percentage: 23.1%\n","Epoch: 3050/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.9 | Wins: 701 | Win percentage: 23.0%\n","Epoch: 3060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 702 | Win percentage: 22.9%\n","Epoch: 3070/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.9 | Wins: 703 | Win percentage: 22.9%\n","Epoch: 3080/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 705 | Win percentage: 22.9%\n","Epoch: 3090/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 707 | Win percentage: 22.9%\n","Epoch: 3100/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 7.3 | Wins: 707 | Win percentage: 22.8%\n","Epoch: 3110/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 13.5 | Wins: 709 | Win percentage: 22.8%\n","Epoch: 3120/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.2 | Wins: 709 | Win percentage: 22.7%\n","Epoch: 3130/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.6 | Wins: 710 | Win percentage: 22.7%\n","Epoch: 3140/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 711 | Win percentage: 22.6%\n","Epoch: 3150/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.6 | Wins: 712 | Win percentage: 22.6%\n","Epoch: 3160/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 713 | Win percentage: 22.6%\n","Epoch: 3170/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.6 | Wins: 714 | Win percentage: 22.5%\n","Epoch: 3180/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 10.6 | Wins: 715 | Win percentage: 22.5%\n","Epoch: 3190/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.5 | Wins: 717 | Win percentage: 22.5%\n","Epoch: 3200/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.4 | Wins: 719 | Win percentage: 22.5%\n","Epoch: 3210/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.8 | Wins: 719 | Win percentage: 22.4%\n","Epoch: 3220/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.0 | Wins: 719 | Win percentage: 22.3%\n","Epoch: 3230/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 21.0 | Wins: 720 | Win percentage: 22.3%\n","Epoch: 3240/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 15.3 | Wins: 723 | Win percentage: 22.3%\n","Epoch: 3250/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 725 | Win percentage: 22.3%\n","Epoch: 3260/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 24.8 | Wins: 727 | Win percentage: 22.3%\n","Epoch: 3270/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.6 | Wins: 728 | Win percentage: 22.3%\n","Epoch: 3280/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 731 | Win percentage: 22.3%\n","Epoch: 3290/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.1 | Wins: 731 | Win percentage: 22.2%\n","Epoch: 3300/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.2 | Wins: 731 | Win percentage: 22.2%\n","Epoch: 3310/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.2 | Wins: 731 | Win percentage: 22.1%\n","Epoch: 3320/10000 | Mean size 10: 3.3 | Longest 10: 006 | Mean steps 10: 13.6 | Wins: 732 | Win percentage: 22.0%\n","Epoch: 3330/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.9 | Wins: 732 | Win percentage: 22.0%\n","Epoch: 3340/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 733 | Win percentage: 21.9%\n","Epoch: 3350/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 735 | Win percentage: 21.9%\n","Epoch: 3360/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.2 | Wins: 735 | Win percentage: 21.9%\n","Epoch: 3370/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 738 | Win percentage: 21.9%\n","Epoch: 3380/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.4 | Wins: 741 | Win percentage: 21.9%\n","Epoch: 3390/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 15.1 | Wins: 743 | Win percentage: 21.9%\n","Epoch: 3400/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 13.7 | Wins: 747 | Win percentage: 22.0%\n","Epoch: 3410/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.2 | Wins: 748 | Win percentage: 21.9%\n","Epoch: 3420/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.0 | Wins: 749 | Win percentage: 21.9%\n","Epoch: 3430/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 16.5 | Wins: 753 | Win percentage: 22.0%\n","Epoch: 3440/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.5 | Wins: 754 | Win percentage: 21.9%\n","Epoch: 3450/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.3 | Wins: 754 | Win percentage: 21.9%\n","Epoch: 3460/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 13.7 | Wins: 757 | Win percentage: 21.9%\n","Epoch: 3470/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 758 | Win percentage: 21.8%\n","Epoch: 3480/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.7 | Wins: 758 | Win percentage: 21.8%\n","Epoch: 3490/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.5 | Wins: 759 | Win percentage: 21.7%\n","Epoch: 3500/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 762 | Win percentage: 21.8%\n","Epoch: 3510/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 15.9 | Wins: 763 | Win percentage: 21.7%\n","Epoch: 3520/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.4 | Wins: 763 | Win percentage: 21.7%\n","Epoch: 3530/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 12.9 | Wins: 765 | Win percentage: 21.7%\n","Epoch: 3540/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 767 | Win percentage: 21.7%\n","Epoch: 3550/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 768 | Win percentage: 21.6%\n","Epoch: 3560/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 769 | Win percentage: 21.6%\n","Epoch: 3570/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.2 | Wins: 770 | Win percentage: 21.6%\n","Epoch: 3580/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 772 | Win percentage: 21.6%\n","Epoch: 3590/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.1 | Wins: 773 | Win percentage: 21.5%\n","Epoch: 3600/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 774 | Win percentage: 21.5%\n","Epoch: 3610/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 776 | Win percentage: 21.5%\n","Epoch: 3620/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 12.4 | Wins: 779 | Win percentage: 21.5%\n","Epoch: 3630/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 18.2 | Wins: 779 | Win percentage: 21.5%\n","Epoch: 3640/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 781 | Win percentage: 21.5%\n","Epoch: 3650/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 783 | Win percentage: 21.5%\n","Epoch: 3660/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 786 | Win percentage: 21.5%\n","Epoch: 3670/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.0 | Wins: 787 | Win percentage: 21.4%\n","Epoch: 3680/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 788 | Win percentage: 21.4%\n","Epoch: 3690/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.0 | Wins: 791 | Win percentage: 21.4%\n","Epoch: 3700/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.2 | Wins: 791 | Win percentage: 21.4%\n","Epoch: 3710/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.0 | Wins: 791 | Win percentage: 21.3%\n","Epoch: 3720/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.2 | Wins: 792 | Win percentage: 21.3%\n","Epoch: 3730/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.8 | Wins: 794 | Win percentage: 21.3%\n","Epoch: 3740/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.6 | Wins: 794 | Win percentage: 21.2%\n","Epoch: 3750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.3 | Wins: 795 | Win percentage: 21.2%\n","Epoch: 3760/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 797 | Win percentage: 21.2%\n","Epoch: 3770/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 798 | Win percentage: 21.2%\n","Epoch: 3780/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.3 | Wins: 798 | Win percentage: 21.1%\n","Epoch: 3790/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.6 | Wins: 801 | Win percentage: 21.1%\n","Epoch: 3800/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.2 | Wins: 804 | Win percentage: 21.2%\n","Epoch: 3810/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.8 | Wins: 806 | Win percentage: 21.2%\n","Epoch: 3820/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 808 | Win percentage: 21.2%\n","Epoch: 3830/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 809 | Win percentage: 21.1%\n","Epoch: 3840/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.2 | Wins: 810 | Win percentage: 21.1%\n","Epoch: 3850/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 811 | Win percentage: 21.1%\n","Epoch: 3860/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.2 | Wins: 811 | Win percentage: 21.0%\n","Epoch: 3870/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 812 | Win percentage: 21.0%\n","Epoch: 3880/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 15.6 | Wins: 817 | Win percentage: 21.1%\n","Epoch: 3890/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 818 | Win percentage: 21.0%\n","Epoch: 3900/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.7 | Wins: 820 | Win percentage: 21.0%\n","Epoch: 3910/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.8 | Wins: 820 | Win percentage: 21.0%\n","Epoch: 3920/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.2 | Wins: 820 | Win percentage: 20.9%\n","Epoch: 3930/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 823 | Win percentage: 20.9%\n","Epoch: 3940/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.9 | Wins: 823 | Win percentage: 20.9%\n","Epoch: 3950/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 824 | Win percentage: 20.9%\n","Epoch: 3960/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 825 | Win percentage: 20.8%\n","Epoch: 3970/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 826 | Win percentage: 20.8%\n","Epoch: 3980/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 827 | Win percentage: 20.8%\n","Epoch: 3990/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.3 | Wins: 827 | Win percentage: 20.7%\n","Epoch: 4000/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 828 | Win percentage: 20.7%\n","Epoch: 4010/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 829 | Win percentage: 20.7%\n","Epoch: 4020/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 831 | Win percentage: 20.7%\n","Epoch: 4030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.8 | Wins: 831 | Win percentage: 20.6%\n","Epoch: 4040/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 16.7 | Wins: 834 | Win percentage: 20.6%\n","Epoch: 4050/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.8 | Wins: 835 | Win percentage: 20.6%\n","Epoch: 4060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.3 | Wins: 836 | Win percentage: 20.6%\n","Epoch: 4070/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 837 | Win percentage: 20.6%\n","Epoch: 4080/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.6 | Wins: 838 | Win percentage: 20.5%\n","Epoch: 4090/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 839 | Win percentage: 20.5%\n","Epoch: 4100/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 16.4 | Wins: 841 | Win percentage: 20.5%\n","Epoch: 4110/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 19.5 | Wins: 842 | Win percentage: 20.5%\n","Epoch: 4120/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 12.4 | Wins: 843 | Win percentage: 20.5%\n","Epoch: 4130/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.8 | Wins: 843 | Win percentage: 20.4%\n","Epoch: 4140/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 844 | Win percentage: 20.4%\n","Epoch: 4150/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 845 | Win percentage: 20.4%\n","Epoch: 4160/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 846 | Win percentage: 20.3%\n","Epoch: 4170/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 848 | Win percentage: 20.3%\n","Epoch: 4180/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.6 | Wins: 849 | Win percentage: 20.3%\n","Epoch: 4190/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 852 | Win percentage: 20.3%\n","Epoch: 4200/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 853 | Win percentage: 20.3%\n","Epoch: 4210/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 855 | Win percentage: 20.3%\n","Epoch: 4220/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.4 | Wins: 857 | Win percentage: 20.3%\n","Epoch: 4230/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 19.0 | Wins: 857 | Win percentage: 20.3%\n","Epoch: 4240/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.8 | Wins: 858 | Win percentage: 20.2%\n","Epoch: 4250/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.7 | Wins: 858 | Win percentage: 20.2%\n","Epoch: 4260/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.5 | Wins: 858 | Win percentage: 20.1%\n","Epoch: 4270/10000 | Mean size 10: 3.6 | Longest 10: 006 | Mean steps 10: 15.2 | Wins: 862 | Win percentage: 20.2%\n","Epoch: 4280/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 10.3 | Wins: 867 | Win percentage: 20.3%\n","Epoch: 4290/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.5 | Wins: 867 | Win percentage: 20.2%\n","Epoch: 4300/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 10.6 | Wins: 869 | Win percentage: 20.2%\n","Epoch: 4310/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.7 | Wins: 869 | Win percentage: 20.2%\n","Epoch: 4320/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.2 | Wins: 869 | Win percentage: 20.1%\n","Epoch: 4330/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.8 | Wins: 871 | Win percentage: 20.1%\n","Epoch: 4340/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 872 | Win percentage: 20.1%\n","Epoch: 4350/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 11.2 | Wins: 876 | Win percentage: 20.1%\n","Epoch: 4360/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 13.8 | Wins: 879 | Win percentage: 20.2%\n","Epoch: 4370/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 881 | Win percentage: 20.2%\n","Epoch: 4380/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.7 | Wins: 882 | Win percentage: 20.1%\n","Epoch: 4390/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.9 | Wins: 882 | Win percentage: 20.1%\n","Epoch: 4400/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 17.4 | Wins: 885 | Win percentage: 20.1%\n","Epoch: 4410/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.4 | Wins: 886 | Win percentage: 20.1%\n","Epoch: 4420/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 16.4 | Wins: 889 | Win percentage: 20.1%\n","Epoch: 4430/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.3 | Wins: 889 | Win percentage: 20.1%\n","Epoch: 4440/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.1 | Wins: 890 | Win percentage: 20.0%\n","Epoch: 4450/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 891 | Win percentage: 20.0%\n","Epoch: 4460/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.5 | Wins: 892 | Win percentage: 20.0%\n","Epoch: 4470/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 11.6 | Wins: 895 | Win percentage: 20.0%\n","Epoch: 4480/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.0 | Wins: 895 | Win percentage: 20.0%\n","Epoch: 4490/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 17.1 | Wins: 897 | Win percentage: 20.0%\n","Epoch: 4500/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.8 | Wins: 897 | Win percentage: 19.9%\n","Epoch: 4510/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.9 | Wins: 898 | Win percentage: 19.9%\n","Epoch: 4520/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 9.6 | Wins: 899 | Win percentage: 19.9%\n","Epoch: 4530/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.4 | Wins: 900 | Win percentage: 19.9%\n","Epoch: 4540/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.7 | Wins: 901 | Win percentage: 19.8%\n","Epoch: 4550/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 903 | Win percentage: 19.8%\n","Epoch: 4560/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.6 | Wins: 906 | Win percentage: 19.9%\n","Epoch: 4570/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.6 | Wins: 907 | Win percentage: 19.8%\n","Epoch: 4580/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.8 | Wins: 908 | Win percentage: 19.8%\n","Epoch: 4590/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 910 | Win percentage: 19.8%\n","Epoch: 4600/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.0 | Wins: 910 | Win percentage: 19.8%\n","Epoch: 4610/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.0 | Wins: 910 | Win percentage: 19.7%\n","Epoch: 4620/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 913 | Win percentage: 19.8%\n","Epoch: 4630/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.7 | Wins: 915 | Win percentage: 19.8%\n","Epoch: 4640/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 12.6 | Wins: 916 | Win percentage: 19.7%\n","Epoch: 4650/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 19.3 | Wins: 919 | Win percentage: 19.8%\n","Epoch: 4660/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 920 | Win percentage: 19.7%\n","Epoch: 4670/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.6 | Wins: 922 | Win percentage: 19.7%\n","Epoch: 4680/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 11.2 | Wins: 924 | Win percentage: 19.7%\n","Epoch: 4690/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 926 | Win percentage: 19.7%\n","Epoch: 4700/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 927 | Win percentage: 19.7%\n","Epoch: 4710/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 928 | Win percentage: 19.7%\n","Epoch: 4720/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 21.6 | Wins: 928 | Win percentage: 19.7%\n","Epoch: 4730/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.4 | Wins: 930 | Win percentage: 19.7%\n","Epoch: 4740/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.3 | Wins: 931 | Win percentage: 19.6%\n","Epoch: 4750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.0 | Wins: 932 | Win percentage: 19.6%\n","Epoch: 4760/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.9 | Wins: 932 | Win percentage: 19.6%\n","Epoch: 4770/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.8 | Wins: 933 | Win percentage: 19.6%\n","Epoch: 4780/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.9 | Wins: 935 | Win percentage: 19.6%\n","Epoch: 4790/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 15.3 | Wins: 939 | Win percentage: 19.6%\n","Epoch: 4800/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 942 | Win percentage: 19.6%\n","Epoch: 4810/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 943 | Win percentage: 19.6%\n","Epoch: 4820/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.5 | Wins: 943 | Win percentage: 19.6%\n","Epoch: 4830/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.2 | Wins: 945 | Win percentage: 19.6%\n","Epoch: 4840/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.4 | Wins: 945 | Win percentage: 19.5%\n","Epoch: 4850/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 948 | Win percentage: 19.5%\n","Epoch: 4860/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.4 | Wins: 948 | Win percentage: 19.5%\n","Epoch: 4870/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.4 | Wins: 949 | Win percentage: 19.5%\n","Epoch: 4880/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 21.1 | Wins: 950 | Win percentage: 19.5%\n","Epoch: 4890/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.7 | Wins: 952 | Win percentage: 19.5%\n","Epoch: 4900/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 12.3 | Wins: 953 | Win percentage: 19.4%\n","Epoch: 4910/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.4 | Wins: 953 | Win percentage: 19.4%\n","Epoch: 4920/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 16.5 | Wins: 956 | Win percentage: 19.4%\n","Epoch: 4930/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 960 | Win percentage: 19.5%\n","Epoch: 4940/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 962 | Win percentage: 19.5%\n","Epoch: 4950/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 11.8 | Wins: 963 | Win percentage: 19.5%\n","Epoch: 4960/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.9 | Wins: 963 | Win percentage: 19.4%\n","Epoch: 4970/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 966 | Win percentage: 19.4%\n","Epoch: 4980/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.9 | Wins: 966 | Win percentage: 19.4%\n","Epoch: 4990/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 967 | Win percentage: 19.4%\n","Epoch: 5000/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.8 | Wins: 967 | Win percentage: 19.3%\n","Epoch: 5010/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.8 | Wins: 967 | Win percentage: 19.3%\n","Epoch: 5020/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 968 | Win percentage: 19.3%\n","Epoch: 5030/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 970 | Win percentage: 19.3%\n","Epoch: 5040/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 12.4 | Wins: 971 | Win percentage: 19.3%\n","Epoch: 5050/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.7 | Wins: 972 | Win percentage: 19.2%\n","Epoch: 5060/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.0 | Wins: 972 | Win percentage: 19.2%\n","Epoch: 5070/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 973 | Win percentage: 19.2%\n","Epoch: 5080/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.5 | Wins: 974 | Win percentage: 19.2%\n","Epoch: 5090/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.4 | Wins: 975 | Win percentage: 19.2%\n","Epoch: 5100/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.2 | Wins: 976 | Win percentage: 19.1%\n","Epoch: 5110/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.1 | Wins: 977 | Win percentage: 19.1%\n","Epoch: 5120/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.5 | Wins: 978 | Win percentage: 19.1%\n","Epoch: 5130/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.2 | Wins: 978 | Win percentage: 19.1%\n","Epoch: 5140/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.8 | Wins: 979 | Win percentage: 19.0%\n","Epoch: 5150/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.8 | Wins: 981 | Win percentage: 19.0%\n","Epoch: 5160/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.5 | Wins: 982 | Win percentage: 19.0%\n","Epoch: 5170/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.6 | Wins: 983 | Win percentage: 19.0%\n","Epoch: 5180/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.2 | Wins: 984 | Win percentage: 19.0%\n","Epoch: 5190/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 987 | Win percentage: 19.0%\n","Epoch: 5200/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 17.9 | Wins: 989 | Win percentage: 19.0%\n","Epoch: 5210/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 991 | Win percentage: 19.0%\n","Epoch: 5220/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 9.6 | Wins: 994 | Win percentage: 19.0%\n","Epoch: 5230/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 995 | Win percentage: 19.0%\n","Epoch: 5240/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.1 | Wins: 995 | Win percentage: 19.0%\n","Epoch: 5250/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 16.8 | Wins: 999 | Win percentage: 19.0%\n","Epoch: 5260/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 1001 | Win percentage: 19.0%\n","Epoch: 5270/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 1002 | Win percentage: 19.0%\n","Epoch: 5280/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 1005 | Win percentage: 19.0%\n","Epoch: 5290/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.7 | Wins: 1005 | Win percentage: 19.0%\n","Epoch: 5300/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 1006 | Win percentage: 19.0%\n","Epoch: 5310/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.3 | Wins: 1006 | Win percentage: 18.9%\n","Epoch: 5320/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 1008 | Win percentage: 18.9%\n","Epoch: 5330/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 19.2 | Wins: 1011 | Win percentage: 19.0%\n","Epoch: 5340/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.7 | Wins: 1013 | Win percentage: 19.0%\n","Epoch: 5350/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.8 | Wins: 1013 | Win percentage: 18.9%\n","Epoch: 5360/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 1016 | Win percentage: 19.0%\n","Epoch: 5370/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 13.1 | Wins: 1017 | Win percentage: 18.9%\n","Epoch: 5380/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.9 | Wins: 1017 | Win percentage: 18.9%\n","Epoch: 5390/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.8 | Wins: 1018 | Win percentage: 18.9%\n","Epoch: 5400/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 1023 | Win percentage: 18.9%\n","Epoch: 5410/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.1 | Wins: 1026 | Win percentage: 19.0%\n","Epoch: 5420/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.8 | Wins: 1027 | Win percentage: 18.9%\n","Epoch: 5430/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.8 | Wins: 1029 | Win percentage: 19.0%\n","Epoch: 5440/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 1030 | Win percentage: 18.9%\n","Epoch: 5450/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 1032 | Win percentage: 18.9%\n","Epoch: 5460/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 1033 | Win percentage: 18.9%\n","Epoch: 5470/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.9 | Wins: 1034 | Win percentage: 18.9%\n","Epoch: 5480/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 12.5 | Wins: 1035 | Win percentage: 18.9%\n","Epoch: 5490/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.0 | Wins: 1038 | Win percentage: 18.9%\n","Epoch: 5500/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 7.2 | Wins: 1039 | Win percentage: 18.9%\n","Epoch: 5510/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 1040 | Win percentage: 18.9%\n","Epoch: 5520/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 1041 | Win percentage: 18.9%\n","Epoch: 5530/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 1045 | Win percentage: 18.9%\n","Epoch: 5540/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.0 | Wins: 1045 | Win percentage: 18.9%\n","Epoch: 5550/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 1048 | Win percentage: 18.9%\n","Epoch: 5560/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.8 | Wins: 1049 | Win percentage: 18.9%\n","Epoch: 5570/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.4 | Wins: 1050 | Win percentage: 18.9%\n","Epoch: 5580/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.0 | Wins: 1052 | Win percentage: 18.9%\n","Epoch: 5590/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.3 | Wins: 1053 | Win percentage: 18.8%\n","Epoch: 5600/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.0 | Wins: 1053 | Win percentage: 18.8%\n","Epoch: 5610/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 1055 | Win percentage: 18.8%\n","Epoch: 5620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.3 | Wins: 1056 | Win percentage: 18.8%\n","Epoch: 5630/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.3 | Wins: 1057 | Win percentage: 18.8%\n","Epoch: 5640/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.0 | Wins: 1058 | Win percentage: 18.8%\n","Epoch: 5650/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.9 | Wins: 1060 | Win percentage: 18.8%\n","Epoch: 5660/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.6 | Wins: 1061 | Win percentage: 18.7%\n","Epoch: 5670/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 1062 | Win percentage: 18.7%\n","Epoch: 5680/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.0 | Wins: 1064 | Win percentage: 18.7%\n","Epoch: 5690/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.0 | Wins: 1065 | Win percentage: 18.7%\n","Epoch: 5700/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.1 | Wins: 1067 | Win percentage: 18.7%\n","Epoch: 5710/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 1070 | Win percentage: 18.7%\n","Epoch: 5720/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 20.5 | Wins: 1074 | Win percentage: 18.8%\n","Epoch: 5730/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.6 | Wins: 1075 | Win percentage: 18.8%\n","Epoch: 5740/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 1076 | Win percentage: 18.7%\n","Epoch: 5750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.8 | Wins: 1077 | Win percentage: 18.7%\n","Epoch: 5760/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 6.3 | Wins: 1077 | Win percentage: 18.7%\n","Epoch: 5770/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 1078 | Win percentage: 18.7%\n","Epoch: 5780/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 1080 | Win percentage: 18.7%\n","Epoch: 5790/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 1082 | Win percentage: 18.7%\n","Epoch: 5800/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 23.2 | Wins: 1085 | Win percentage: 18.7%\n","Epoch: 5810/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.7 | Wins: 1086 | Win percentage: 18.7%\n","Epoch: 5820/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.6 | Wins: 1087 | Win percentage: 18.7%\n","Epoch: 5830/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.5 | Wins: 1089 | Win percentage: 18.7%\n","Epoch: 5840/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 21.6 | Wins: 1093 | Win percentage: 18.7%\n","Epoch: 5850/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 1095 | Win percentage: 18.7%\n","Epoch: 5860/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.2 | Wins: 1096 | Win percentage: 18.7%\n","Epoch: 5870/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.2 | Wins: 1096 | Win percentage: 18.7%\n","Epoch: 5880/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 1097 | Win percentage: 18.7%\n","Epoch: 5890/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.0 | Wins: 1097 | Win percentage: 18.6%\n","Epoch: 5900/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.1 | Wins: 1098 | Win percentage: 18.6%\n","Epoch: 5910/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.3 | Wins: 1099 | Win percentage: 18.6%\n","Epoch: 5920/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.6 | Wins: 1100 | Win percentage: 18.6%\n","Epoch: 5930/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.5 | Wins: 1100 | Win percentage: 18.5%\n","Epoch: 5940/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 12.2 | Wins: 1103 | Win percentage: 18.6%\n","Epoch: 5950/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.0 | Wins: 1103 | Win percentage: 18.5%\n","Epoch: 5960/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 1104 | Win percentage: 18.5%\n","Epoch: 5970/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.3 | Wins: 1106 | Win percentage: 18.5%\n","Epoch: 5980/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.0 | Wins: 1107 | Win percentage: 18.5%\n","Epoch: 5990/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.4 | Wins: 1107 | Win percentage: 18.5%\n","Epoch: 6000/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.0 | Wins: 1107 | Win percentage: 18.4%\n","Epoch: 6010/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.8 | Wins: 1109 | Win percentage: 18.5%\n","Epoch: 6020/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.6 | Wins: 1110 | Win percentage: 18.4%\n","Epoch: 6030/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 18.8 | Wins: 1113 | Win percentage: 18.5%\n","Epoch: 6040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.7 | Wins: 1114 | Win percentage: 18.4%\n","Epoch: 6050/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 9.8 | Wins: 1116 | Win percentage: 18.4%\n","Epoch: 6060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 1117 | Win percentage: 18.4%\n","Epoch: 6070/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.1 | Wins: 1117 | Win percentage: 18.4%\n","Epoch: 6080/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 14.5 | Wins: 1121 | Win percentage: 18.4%\n","Epoch: 6090/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 14.2 | Wins: 1122 | Win percentage: 18.4%\n","Epoch: 6100/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.4 | Wins: 1122 | Win percentage: 18.4%\n","Epoch: 6110/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 1123 | Win percentage: 18.4%\n","Epoch: 6120/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.7 | Wins: 1123 | Win percentage: 18.3%\n","Epoch: 6130/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 1125 | Win percentage: 18.4%\n","Epoch: 6140/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.0 | Wins: 1127 | Win percentage: 18.4%\n","Epoch: 6150/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.4 | Wins: 1127 | Win percentage: 18.3%\n","Epoch: 6160/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.8 | Wins: 1127 | Win percentage: 18.3%\n","Epoch: 6170/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 11.9 | Wins: 1128 | Win percentage: 18.3%\n","Epoch: 6180/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.2 | Wins: 1129 | Win percentage: 18.3%\n","Epoch: 6190/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 1132 | Win percentage: 18.3%\n","Epoch: 6200/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.4 | Wins: 1133 | Win percentage: 18.3%\n","Epoch: 6210/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.0 | Wins: 1135 | Win percentage: 18.3%\n","Epoch: 6220/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.8 | Wins: 1135 | Win percentage: 18.2%\n","Epoch: 6230/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.3 | Wins: 1135 | Win percentage: 18.2%\n","Epoch: 6240/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.6 | Wins: 1135 | Win percentage: 18.2%\n","Epoch: 6250/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.9 | Wins: 1137 | Win percentage: 18.2%\n","Epoch: 6260/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.5 | Wins: 1138 | Win percentage: 18.2%\n","Epoch: 6270/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 12.1 | Wins: 1141 | Win percentage: 18.2%\n","Epoch: 6280/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 1143 | Win percentage: 18.2%\n","Epoch: 6290/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 1144 | Win percentage: 18.2%\n","Epoch: 6300/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.3 | Wins: 1145 | Win percentage: 18.2%\n","Epoch: 6310/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.0 | Wins: 1147 | Win percentage: 18.2%\n","Epoch: 6320/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.4 | Wins: 1147 | Win percentage: 18.1%\n","Epoch: 6330/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.5 | Wins: 1149 | Win percentage: 18.2%\n","Epoch: 6340/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.4 | Wins: 1150 | Win percentage: 18.1%\n","Epoch: 6350/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.7 | Wins: 1150 | Win percentage: 18.1%\n","Epoch: 6360/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.3 | Wins: 1151 | Win percentage: 18.1%\n","Epoch: 6370/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.7 | Wins: 1151 | Win percentage: 18.1%\n","Epoch: 6380/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.0 | Wins: 1152 | Win percentage: 18.1%\n","Epoch: 6390/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.4 | Wins: 1154 | Win percentage: 18.1%\n","Epoch: 6400/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 15.9 | Wins: 1157 | Win percentage: 18.1%\n","Epoch: 6410/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 1159 | Win percentage: 18.1%\n","Epoch: 6420/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.6 | Wins: 1161 | Win percentage: 18.1%\n","Epoch: 6430/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.7 | Wins: 1162 | Win percentage: 18.1%\n","Epoch: 6440/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.0 | Wins: 1162 | Win percentage: 18.0%\n","Epoch: 6450/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 1163 | Win percentage: 18.0%\n","Epoch: 6460/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.8 | Wins: 1163 | Win percentage: 18.0%\n","Epoch: 6470/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 1165 | Win percentage: 18.0%\n","Epoch: 6480/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 1167 | Win percentage: 18.0%\n","Epoch: 6490/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.9 | Wins: 1170 | Win percentage: 18.0%\n","Epoch: 6500/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.5 | Wins: 1171 | Win percentage: 18.0%\n","Epoch: 6510/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.9 | Wins: 1173 | Win percentage: 18.0%\n","Epoch: 6520/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 20.8 | Wins: 1176 | Win percentage: 18.0%\n","Epoch: 6530/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.3 | Wins: 1176 | Win percentage: 18.0%\n","Epoch: 6540/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.8 | Wins: 1176 | Win percentage: 18.0%\n","Epoch: 6550/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.2 | Wins: 1176 | Win percentage: 18.0%\n","Epoch: 6560/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.3 | Wins: 1176 | Win percentage: 17.9%\n","Epoch: 6570/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 1178 | Win percentage: 17.9%\n","Epoch: 6580/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.1 | Wins: 1180 | Win percentage: 17.9%\n","Epoch: 6590/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 1181 | Win percentage: 17.9%\n","Epoch: 6600/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 1182 | Win percentage: 17.9%\n","Epoch: 6610/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 17.0 | Wins: 1185 | Win percentage: 17.9%\n","Epoch: 6620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 1186 | Win percentage: 17.9%\n","Epoch: 6630/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 15.9 | Wins: 1190 | Win percentage: 17.9%\n","Epoch: 6640/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 8.9 | Wins: 1193 | Win percentage: 18.0%\n","Epoch: 6650/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.9 | Wins: 1194 | Win percentage: 18.0%\n","Epoch: 6660/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.1 | Wins: 1194 | Win percentage: 17.9%\n","Epoch: 6670/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.5 | Wins: 1195 | Win percentage: 17.9%\n","Epoch: 6680/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 11.6 | Wins: 1198 | Win percentage: 17.9%\n","Epoch: 6690/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.3 | Wins: 1198 | Win percentage: 17.9%\n","Epoch: 6700/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 1199 | Win percentage: 17.9%\n","Epoch: 6710/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 1201 | Win percentage: 17.9%\n","Epoch: 6720/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 20.0 | Wins: 1201 | Win percentage: 17.9%\n","Epoch: 6730/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.9 | Wins: 1202 | Win percentage: 17.9%\n","Epoch: 6740/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 1203 | Win percentage: 17.8%\n","Epoch: 6750/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.1 | Wins: 1205 | Win percentage: 17.9%\n","Epoch: 6760/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.4 | Wins: 1207 | Win percentage: 17.9%\n","Epoch: 6770/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.7 | Wins: 1207 | Win percentage: 17.8%\n","Epoch: 6780/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 14.7 | Wins: 1210 | Win percentage: 17.8%\n","Epoch: 6790/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 1214 | Win percentage: 17.9%\n","Epoch: 6800/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.6 | Wins: 1216 | Win percentage: 17.9%\n","Epoch: 6810/10000 | Mean size 10: 3.3 | Longest 10: 006 | Mean steps 10: 10.5 | Wins: 1217 | Win percentage: 17.9%\n","Epoch: 6820/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.4 | Wins: 1218 | Win percentage: 17.9%\n","Epoch: 6830/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 1219 | Win percentage: 17.8%\n","Epoch: 6840/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 1220 | Win percentage: 17.8%\n","Epoch: 6850/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.2 | Wins: 1220 | Win percentage: 17.8%\n","Epoch: 6860/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 1222 | Win percentage: 17.8%\n","Epoch: 6870/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.0 | Wins: 1222 | Win percentage: 17.8%\n","Epoch: 6880/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.5 | Wins: 1223 | Win percentage: 17.8%\n","Epoch: 6890/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 1224 | Win percentage: 17.8%\n","Epoch: 6900/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 11.0 | Wins: 1225 | Win percentage: 17.8%\n","Epoch: 6910/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.1 | Wins: 1227 | Win percentage: 17.8%\n","Epoch: 6920/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.3 | Wins: 1228 | Win percentage: 17.7%\n","Epoch: 6930/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 1231 | Win percentage: 17.8%\n","Epoch: 6940/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.0 | Wins: 1233 | Win percentage: 17.8%\n","Epoch: 6950/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.9 | Wins: 1233 | Win percentage: 17.7%\n","Epoch: 6960/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.2 | Wins: 1233 | Win percentage: 17.7%\n","Epoch: 6970/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 1234 | Win percentage: 17.7%\n","Epoch: 6980/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 10.0 | Wins: 1237 | Win percentage: 17.7%\n","Epoch: 6990/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.3 | Wins: 1239 | Win percentage: 17.7%\n","Epoch: 7000/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 9.5 | Wins: 1241 | Win percentage: 17.7%\n","Epoch: 7010/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 1242 | Win percentage: 17.7%\n","Epoch: 7020/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 1243 | Win percentage: 17.7%\n","Epoch: 7030/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.1 | Wins: 1244 | Win percentage: 17.7%\n","Epoch: 7040/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.7 | Wins: 1244 | Win percentage: 17.7%\n","Epoch: 7050/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.8 | Wins: 1244 | Win percentage: 17.6%\n","Epoch: 7060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.6 | Wins: 1245 | Win percentage: 17.6%\n","Epoch: 7070/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 1246 | Win percentage: 17.6%\n","Epoch: 7080/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.0 | Wins: 1247 | Win percentage: 17.6%\n","Epoch: 7090/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 15.7 | Wins: 1248 | Win percentage: 17.6%\n","Epoch: 7100/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 1249 | Win percentage: 17.6%\n","Epoch: 7110/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 1252 | Win percentage: 17.6%\n","Epoch: 7120/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.9 | Wins: 1253 | Win percentage: 17.6%\n","Epoch: 7130/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.8 | Wins: 1253 | Win percentage: 17.6%\n","Epoch: 7140/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 1255 | Win percentage: 17.6%\n","Epoch: 7150/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.4 | Wins: 1255 | Win percentage: 17.6%\n","Epoch: 7160/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 12.3 | Wins: 1257 | Win percentage: 17.6%\n","Epoch: 7170/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 1259 | Win percentage: 17.6%\n","Epoch: 7180/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 13.4 | Wins: 1261 | Win percentage: 17.6%\n","Epoch: 7190/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 1262 | Win percentage: 17.6%\n","Epoch: 7200/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 16.4 | Wins: 1265 | Win percentage: 17.6%\n","Epoch: 7210/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.2 | Wins: 1265 | Win percentage: 17.5%\n","Epoch: 7220/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.1 | Wins: 1266 | Win percentage: 17.5%\n","Epoch: 7230/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.1 | Wins: 1268 | Win percentage: 17.5%\n","Epoch: 7240/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.4 | Wins: 1270 | Win percentage: 17.5%\n","Epoch: 7250/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.6 | Wins: 1270 | Win percentage: 17.5%\n","Epoch: 7260/10000 | Mean size 10: 3.3 | Longest 10: 006 | Mean steps 10: 12.6 | Wins: 1271 | Win percentage: 17.5%\n","Epoch: 7270/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.5 | Wins: 1273 | Win percentage: 17.5%\n","Epoch: 7280/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.1 | Wins: 1274 | Win percentage: 17.5%\n","Epoch: 7290/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 1277 | Win percentage: 17.5%\n","Epoch: 7300/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 10.9 | Wins: 1281 | Win percentage: 17.5%\n","Epoch: 7310/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 12.3 | Wins: 1283 | Win percentage: 17.6%\n","Epoch: 7320/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.3 | Wins: 1283 | Win percentage: 17.5%\n","Epoch: 7330/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 1284 | Win percentage: 17.5%\n","Epoch: 7340/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.0 | Wins: 1285 | Win percentage: 17.5%\n","Epoch: 7350/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 16.1 | Wins: 1288 | Win percentage: 17.5%\n","Epoch: 7360/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.1 | Wins: 1289 | Win percentage: 17.5%\n","Epoch: 7370/10000 | Mean size 10: 3.5 | Longest 10: 006 | Mean steps 10: 12.9 | Wins: 1292 | Win percentage: 17.5%\n","Epoch: 7380/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.1 | Wins: 1293 | Win percentage: 17.5%\n","Epoch: 7390/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.4 | Wins: 1295 | Win percentage: 17.5%\n","Epoch: 7400/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.3 | Wins: 1297 | Win percentage: 17.5%\n","Epoch: 7410/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.2 | Wins: 1297 | Win percentage: 17.5%\n","Epoch: 7420/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.8 | Wins: 1297 | Win percentage: 17.5%\n","Epoch: 7430/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.5 | Wins: 1297 | Win percentage: 17.5%\n","Epoch: 7440/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.8 | Wins: 1297 | Win percentage: 17.4%\n","Epoch: 7450/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.3 | Wins: 1297 | Win percentage: 17.4%\n","Epoch: 7460/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 1298 | Win percentage: 17.4%\n","Epoch: 7470/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.4 | Wins: 1298 | Win percentage: 17.4%\n","Epoch: 7480/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.2 | Wins: 1298 | Win percentage: 17.4%\n","Epoch: 7490/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 1300 | Win percentage: 17.4%\n","Epoch: 7500/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.8 | Wins: 1300 | Win percentage: 17.3%\n","Epoch: 7510/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 11.8 | Wins: 1303 | Win percentage: 17.4%\n","Epoch: 7520/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 1305 | Win percentage: 17.4%\n","Epoch: 7530/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 16.4 | Wins: 1308 | Win percentage: 17.4%\n","Epoch: 7540/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 13.2 | Wins: 1309 | Win percentage: 17.4%\n","Epoch: 7550/10000 | Mean size 10: 3.5 | Longest 10: 006 | Mean steps 10: 10.5 | Wins: 1311 | Win percentage: 17.4%\n","Epoch: 7560/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 1312 | Win percentage: 17.4%\n","Epoch: 7570/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 1314 | Win percentage: 17.4%\n","Epoch: 7580/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 8.5 | Wins: 1316 | Win percentage: 17.4%\n","Epoch: 7590/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.2 | Wins: 1318 | Win percentage: 17.4%\n","Epoch: 7600/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 1320 | Win percentage: 17.4%\n","Epoch: 7610/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.3 | Wins: 1322 | Win percentage: 17.4%\n","Epoch: 7620/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.6 | Wins: 1324 | Win percentage: 17.4%\n","Epoch: 7630/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.3 | Wins: 1324 | Win percentage: 17.4%\n","Epoch: 7640/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 1326 | Win percentage: 17.4%\n","Epoch: 7650/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 1327 | Win percentage: 17.3%\n","Epoch: 7660/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.5 | Wins: 1328 | Win percentage: 17.3%\n","Epoch: 7670/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.4 | Wins: 1329 | Win percentage: 17.3%\n","Epoch: 7680/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 1331 | Win percentage: 17.3%\n","Epoch: 7690/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.8 | Wins: 1331 | Win percentage: 17.3%\n","Epoch: 7700/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 18.0 | Wins: 1331 | Win percentage: 17.3%\n","Epoch: 7710/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 1332 | Win percentage: 17.3%\n","Epoch: 7720/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 7.9 | Wins: 1333 | Win percentage: 17.3%\n","Epoch: 7730/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.2 | Wins: 1334 | Win percentage: 17.3%\n","Epoch: 7740/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.3 | Wins: 1335 | Win percentage: 17.2%\n","Epoch: 7750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.5 | Wins: 1336 | Win percentage: 17.2%\n","Epoch: 7760/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.9 | Wins: 1337 | Win percentage: 17.2%\n","Epoch: 7770/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.6 | Wins: 1338 | Win percentage: 17.2%\n","Epoch: 7780/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 1340 | Win percentage: 17.2%\n","Epoch: 7790/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 15.1 | Wins: 1342 | Win percentage: 17.2%\n","Epoch: 7800/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 1345 | Win percentage: 17.2%\n","Epoch: 7810/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 19.6 | Wins: 1348 | Win percentage: 17.3%\n","Epoch: 7820/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.2 | Wins: 1348 | Win percentage: 17.2%\n","Epoch: 7830/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.4 | Wins: 1350 | Win percentage: 17.2%\n","Epoch: 7840/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 1352 | Win percentage: 17.2%\n","Epoch: 7850/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 1355 | Win percentage: 17.3%\n","Epoch: 7860/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.0 | Wins: 1357 | Win percentage: 17.3%\n","Epoch: 7870/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.7 | Wins: 1357 | Win percentage: 17.2%\n","Epoch: 7880/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 1358 | Win percentage: 17.2%\n","Epoch: 7890/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.3 | Wins: 1360 | Win percentage: 17.2%\n","Epoch: 7900/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 1362 | Win percentage: 17.2%\n","Epoch: 7910/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.1 | Wins: 1364 | Win percentage: 17.2%\n","Epoch: 7920/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 1365 | Win percentage: 17.2%\n","Epoch: 7930/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 1366 | Win percentage: 17.2%\n","Epoch: 7940/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.8 | Wins: 1366 | Win percentage: 17.2%\n","Epoch: 7950/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.2 | Wins: 1368 | Win percentage: 17.2%\n","Epoch: 7960/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.3 | Wins: 1369 | Win percentage: 17.2%\n","Epoch: 7970/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 1370 | Win percentage: 17.2%\n","Epoch: 7980/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.4 | Wins: 1372 | Win percentage: 17.2%\n","Epoch: 7990/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 1373 | Win percentage: 17.2%\n","Epoch: 8000/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.3 | Wins: 1376 | Win percentage: 17.2%\n","Epoch: 8010/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.7 | Wins: 1377 | Win percentage: 17.2%\n","Epoch: 8020/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.4 | Wins: 1377 | Win percentage: 17.2%\n","Epoch: 8030/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.8 | Wins: 1379 | Win percentage: 17.2%\n","Epoch: 8040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.8 | Wins: 1380 | Win percentage: 17.2%\n","Epoch: 8050/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.0 | Wins: 1381 | Win percentage: 17.2%\n","Epoch: 8060/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 14.8 | Wins: 1384 | Win percentage: 17.2%\n","Epoch: 8070/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.1 | Wins: 1385 | Win percentage: 17.2%\n","Epoch: 8080/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.7 | Wins: 1385 | Win percentage: 17.1%\n","Epoch: 8090/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 1387 | Win percentage: 17.1%\n","Epoch: 8100/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 17.5 | Wins: 1391 | Win percentage: 17.2%\n","Epoch: 8110/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 22.5 | Wins: 1393 | Win percentage: 17.2%\n","Epoch: 8120/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 14.7 | Wins: 1396 | Win percentage: 17.2%\n","Epoch: 8130/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 1397 | Win percentage: 17.2%\n","Epoch: 8140/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 17.8 | Wins: 1401 | Win percentage: 17.2%\n","Epoch: 8150/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.1 | Wins: 1403 | Win percentage: 17.2%\n","Epoch: 8160/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 1405 | Win percentage: 17.2%\n","Epoch: 8170/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.2 | Wins: 1405 | Win percentage: 17.2%\n","Epoch: 8180/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 1407 | Win percentage: 17.2%\n","Epoch: 8190/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 16.4 | Wins: 1410 | Win percentage: 17.2%\n","Epoch: 8200/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.2 | Wins: 1412 | Win percentage: 17.2%\n","Epoch: 8210/10000 | Mean size 10: 3.3 | Longest 10: 006 | Mean steps 10: 11.7 | Wins: 1413 | Win percentage: 17.2%\n","Epoch: 8220/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.7 | Wins: 1413 | Win percentage: 17.2%\n","Epoch: 8230/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 18.5 | Wins: 1413 | Win percentage: 17.2%\n","Epoch: 8240/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.6 | Wins: 1414 | Win percentage: 17.2%\n","Epoch: 8250/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.5 | Wins: 1414 | Win percentage: 17.1%\n","Epoch: 8260/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.2 | Wins: 1414 | Win percentage: 17.1%\n","Epoch: 8270/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.2 | Wins: 1416 | Win percentage: 17.1%\n","Epoch: 8280/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.8 | Wins: 1417 | Win percentage: 17.1%\n","Epoch: 8290/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.6 | Wins: 1419 | Win percentage: 17.1%\n","Epoch: 8300/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.5 | Wins: 1421 | Win percentage: 17.1%\n","Epoch: 8310/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.0 | Wins: 1421 | Win percentage: 17.1%\n","Epoch: 8320/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.5 | Wins: 1422 | Win percentage: 17.1%\n","Epoch: 8330/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.2 | Wins: 1423 | Win percentage: 17.1%\n","Epoch: 8340/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.8 | Wins: 1425 | Win percentage: 17.1%\n"],"name":"stdout"}]},{"metadata":{"id":"GHs-yd2XkE2b","colab_type":"code","outputId":"01765663-1a79-4610-9d5e-0c2aa46d5c4c","executionInfo":{"status":"ok","timestamp":1543019556168,"user_tz":120,"elapsed":292913,"user":{"displayName":"Victor Neves","photoUrl":"https://lh5.googleusercontent.com/-zfn1KWpcK60/AAAAAAAAAAI/AAAAAAAAZas/oOSqICtc8i0/s64/photo.jpg","userId":"17687780415293205160"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI. - THIS\n","\"\"\"\n","\n","import sys  # To close the window when the game is over\n","from array import array  # Efficient numeric arrays\n","from os import environ, path  # To center the game window the best possible\n","import random  # Random numbers used for the food\n","import logging  # Logging function for movements and errors\n","import json # For file handling (leaderboards)\n","from itertools import tee  # For the color gradient on snake\n","\n","import numpy as np # Used in calculations and math\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","OPTIONS = {'QUIT': 0,\n","           'PLAY': 1,\n","           'BENCHMARK': 2,\n","           'LEADERBOARDS': 3,\n","           'MENU': 4,\n","           'ADD_TO_LEADERBOARDS': 5}\n","RELATIVE_ACTIONS = {'LEFT': 0,\n","                    'FORWARD': 1,\n","                    'RIGHT': 2}\n","ABSOLUTE_ACTIONS = {'LEFT': 0,\n","                    'RIGHT': 1,\n","                    'UP': 2,\n","                    'DOWN': 3,\n","                    'IDLE': 4}\n","FORBIDDEN_MOVES = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","REWARDS = {'MOVE': -0.005,\n","           'GAME_OVER': -1,\n","           'SCORED': 1}\n","\n","# Types of point in the board\n","POINT_TYPE = {'EMPTY': 0,\n","              'FOOD': 1,\n","              'BODY': 2,\n","              'HEAD': 3,\n","              'DANGEROUS': 4}\n","\n","# Speed levels possible to human players. MEGA HARDCORE starts with MEDIUM and\n","# increases with snake size\n","LEVELS = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","SPEEDS = {'EASY': 80,\n","          'MEDIUM': 60,\n","          'HARD': 40,\n","          'MEGA_HARDCORE': 65}\n","\n","# Set the constant FPS limit for the game. Smoothness depend on this.\n","GAME_FPS = 100\n","\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    block_size: int, optional, default = 20\n","        The size in pixels of a block.\n","    head_color: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    tail_color: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    food_color: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    game_speed: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    benchmark: int, optional, default = 10\n","        Ammount of matches to benchmark and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, board_size = 30, block_size = 20,\n","                 head_color = (42, 42, 42), tail_color = (152, 152, 152),\n","                 food_color = (200, 0, 0), game_speed = 80, benchmark = 10):\n","        \"\"\"Initialize all global variables. Updated with argument_handler.\"\"\"\n","        self.board_size = board_size\n","        self.block_size = block_size\n","        self.head_color = head_color\n","        self.tail_color = tail_color\n","        self.food_color = food_color\n","        self.game_speed = game_speed\n","        self.benchmark = benchmark\n","\n","        if self.board_size > 50: # Warn the user about performance\n","            LOGGER.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","    @property\n","    def canvas_size(self):\n","        \"\"\"Canvas size is updated with board_size and block_size.\"\"\"\n","        return self.board_size * self.block_size\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), block_type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.block_type = block_type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((VAR.canvas_size) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.block_type == \"menu\" and not self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.block_type == \"menu\" and self.hovered:\n","            color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [board_size / 4, board_size / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(VAR.board_size / 4), int(VAR.board_size / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def is_movement_invalid(self, action):\n","        \"\"\"Check if the movement is invalid, according to FORBIDDEN_MOVES.\"\"\"\n","        valid = False\n","\n","        if (action, self.previous_action) in FORBIDDEN_MOVES:\n","            valid = True\n","\n","        return valid\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if (action == ABSOLUTE_ACTIONS['IDLE'] or\n","            self.is_movement_invalid(action)):\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == ABSOLUTE_ACTIONS['LEFT']:\n","            self.head[0] -= 1\n","        elif action == ABSOLUTE_ACTIONS['RIGHT']:\n","            self.head[0] += 1\n","        elif action == ABSOLUTE_ACTIONS['UP']:\n","            self.head[1] -= 1\n","        elif action == ABSOLUTE_ACTIONS['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            LOGGER.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((VAR.board_size - 1) * random.random()),\n","                        int((VAR.board_size - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            LOGGER.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        VAR.board_size = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with board_size * block_size dimension.\"\"\"\n","        pygame.init()\n","        flags = pygame.DOUBLEBUF | pygame.HWSURFACE\n","        self.window = pygame.display.set_mode((VAR.canvas_size, VAR.canvas_size),\n","                                              flags)\n","        self.window.set_alpha(None)\n","\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def cycle_menu(self, menu_options, list_menu, dictionary, img = None,\n","                   img_rect = None):\n","        \"\"\"Cycle through a given menu, waiting for an option to be clicked.\"\"\"\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            events = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for i, option in enumerate(menu_options):\n","                if option is not None:\n","                    option.draw()\n","                    option.hovered = False\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        for event in events:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = dictionary[list_menu[i]]\n","\n","            if selected_option is not None:\n","                selected = True\n","            if img is not None:\n","                self.window.blit(img, img_rect.bottomleft)\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def cycle_matches(self, n_matches, mega_hardcore = False):\n","        \"\"\"Cycle through matches until the end.\"\"\"\n","        score = array('i')\n","\n","        for _ in range(n_matches):\n","            self.reset_game()\n","            self.start_match(wait = 3)\n","            score.append(self.single_player(mega_hardcore))\n","\n","        return score\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images\" +\n","                                              \"/snake_logo.png\")).convert()\n","        img = pygame.transform.scale(img, (VAR.canvas_size,\n","                                           int(VAR.canvas_size / 3)))\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","        list_menu = ['PLAY', 'BENCHMARK', 'LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY GAME ',\n","                                  (self.screen_rect.centerx,\n","                                   4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ',\n","                                  (self.screen_rect.centerx,\n","                                   6 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ',\n","                                  (self.screen_rect.centerx,\n","                                   8 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ',\n","                                  (self.screen_rect.centerx,\n","                                   10 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"menu\")]\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS,\n","                                          img, img_rect)\n","\n","        return selected_option\n","\n","    def start_match(self, wait):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(wait):\n","            time = str(wait - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in',\n","                              (self.screen_rect.centerx,\n","                               4 * self.screen_rect.centery / 10),\n","                              self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                     12 * self.screen_rect.centery / 10),\n","                              self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","            pygame.time.wait(1000)\n","\n","        LOGGER.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","\n","        while True:\n","            if opt == OPTIONS['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == OPTIONS['PLAY']:\n","                VAR.game_speed, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = 1,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == OPTIONS['BENCHMARK']:\n","                VAR.game_speed, mega_hardcore = self.select_speed()\n","                score = self.cycle_matches(n_matches = VAR.benchmark,\n","                                           mega_hardcore = mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == OPTIONS['LEADERBOARDS']:\n","                self.view_leaderboards()\n","            elif opt == OPTIONS['MENU']:\n","                opt = self.menu()\n","            if opt == OPTIONS['ADD_TO_LEADERBOARDS']:\n","                self.add_to_leaderboards(score, None) # Gotta improve this logic.\n","                self.view_leaderboards()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        score_option = None\n","\n","        if len(score) == VAR.benchmark:\n","            score_option = TextBlock(' ADD TO LEADERBOARDS ',\n","                                     (self.screen_rect.centerx,\n","                                      8 * self.screen_rect.centery / 10),\n","                                     self.window, (1 / 15), \"menu\")\n","\n","        text_score = 'SCORE: ' + str(int(np.mean(score)))\n","        list_menu = ['PLAY', 'MENU', 'ADD_TO_LEADERBOARDS', 'QUIT']\n","        menu_options = [TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                   4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                   6 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        score_option,\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 15), \"menu\"),\n","                        TextBlock(text_score, (self.screen_rect.centerx,\n","                                               15 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"text\")]\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        LOGGER.info('EVENT: GAME OVER | FINAL %s', text_score)\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS)\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        list_menu = ['EASY', 'MEDIUM', 'HARD', 'MEGA_HARDCORE']\n","        menu_options = [TextBlock(LEVELS[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\"),\n","                        TextBlock(LEVELS[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 10), \"menu\")]\n","\n","        speed = self.cycle_menu(menu_options, list_menu, SPEEDS)\n","        mega_hardcore = False\n","\n","        if speed == SPEEDS['MEGA_HARDCORE']:\n","            mega_hardcore = True\n","\n","        return speed, mega_hardcore\n","\n","    def single_player(self, mega_hardcore = False):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                   previous_size)\n","\n","        # Main loop, where snakes moves after elapsed time is bigger than the\n","        # move_wait time. The last_key pressed is recorded to make the game more\n","        # smooth for human players.\n","        elapsed = 0\n","        last_key = self.snake.previous_action\n","        move_wait = VAR.game_speed\n","\n","        while not self.game_over:\n","            elapsed += self.fps.get_time()  # Get elapsed time since last call.\n","\n","            if mega_hardcore:  # Progressive speed increments, the hardest.\n","                move_wait = VAR.game_speed - (2 * (self.snake.length - 3))\n","\n","            key_input = self.handle_input()  # Receive inputs with tick.\n","            invalid_key = self.snake.is_movement_invalid(key_input)\n","\n","            if key_input is not None and not invalid_key:\n","                last_key = key_input\n","\n","            if elapsed >= move_wait:  # Move and redraw\n","                elapsed = 0\n","                self.play(last_key)\n","                current_size = self.snake.length  # Update the body size\n","\n","                if current_size > previous_size:\n","                    color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                               current_size)\n","\n","                    previous_size = current_size\n","\n","                self.draw(color_list)\n","\n","            pygame.display.update()\n","            self.fps.tick(GAME_FPS)  # Limit FPS to 100\n","\n","        score = current_size - 3  # After the game is over, record score\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (VAR.board_size - 1) or self.snake.head[0] < 0:\n","            LOGGER.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (VAR.board_size - 1) or self.snake.head[1] < 0:\n","            LOGGER.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            LOGGER.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = None\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            LOGGER.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            LOGGER.info('ACTION: KEY PRESSED: LEFT')\n","            action = ABSOLUTE_ACTIONS['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            LOGGER.info('ACTION: KEY PRESSED: RIGHT')\n","            action = ABSOLUTE_ACTIONS['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            LOGGER.info('ACTION: KEY PRESSED: UP')\n","            action = ABSOLUTE_ACTIONS['UP']\n","        elif keys[pygame.K_DOWN]:\n","            LOGGER.info('ACTION: KEY PRESSED: DOWN')\n","            action = ABSOLUTE_ACTIONS['DOWN']\n","\n","        return action\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size board_size**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((VAR.board_size, VAR.board_size))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = POINT_TYPE['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = POINT_TYPE['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = POINT_TYPE['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == RELATIVE_ACTIONS['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == RELATIVE_ACTIONS['LEFT']:\n","            if self.snake.previous_action == ABSOLUTE_ACTIONS['LEFT']:\n","                action = ABSOLUTE_ACTIONS['DOWN']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['RIGHT']:\n","                action = ABSOLUTE_ACTIONS['UP']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['UP']:\n","                action = ABSOLUTE_ACTIONS['LEFT']\n","            else:\n","                action = ABSOLUTE_ACTIONS['RIGHT']\n","        else:\n","            if self.snake.previous_action == ABSOLUTE_ACTIONS['LEFT']:\n","                action = ABSOLUTE_ACTIONS['UP']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['RIGHT']:\n","                action = ABSOLUTE_ACTIONS['DOWN']\n","            elif self.snake.previous_action == ABSOLUTE_ACTIONS['UP']:\n","                action = ABSOLUTE_ACTIONS['RIGHT']\n","            else:\n","                action = ABSOLUTE_ACTIONS['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                self.game_over = True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current reward. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = REWARDS['MOVE']\n","\n","        if self.game_over:\n","            reward = REWARDS['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect((part[0] *\n","                        VAR.block_size), part[1] * VAR.block_size,\n","                        VAR.block_size, VAR.block_size))\n","\n","        pygame.draw.rect(self.window, VAR.food_color,\n","                         pygame.Rect(self.food_pos[0] * VAR.block_size,\n","                         self.food_pos[1] * VAR.block_size, VAR.block_size,\n","                         VAR.block_size))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                   + str(self.snake.length - 3))\n","\n","    def get_name(self):\n","        \"\"\"See test.py in my desktop, for a textbox input in pygame\"\"\"\n","        return None\n","\n","    def add_to_leaderboards(self, score, step):\n","        file_path = resource_path(\"resources/scores.json\")\n","\n","        name = self.get_name()\n","        new_score = {'name': 'test',\n","                     'ranking_data': {'score': score,\n","                                      'step': step}}\n","\n","        with open(file_path, 'w') as leaderboards_file:\n","            json.dump(new_score, leaderboards_file)\n","\n","    def view_leaderboards(self):\n","        list_menu = ['MENU']\n","        menu_options = [TextBlock('LEADERBOARDS',\n","                                  (self.screen_rect.centerx,\n","                                   2 * self.screen_rect.centery / 10),\n","                                  self.window, (1 / 12), \"text\")]\n","\n","        file_path = resource_path(\"resources/scores.json\")\n","\n","        with open(file_path, 'r') as leaderboards_file:\n","            scores_data = json.loads(leaderboards_file.read())\n","\n","        scores_data.sort(key = operator.itemgetter('score'))\n","\n","#        for score in formatted_scores:\n","#            menu_options.append(TextBlock(person_ranked,\n","#                                (self.screen_rect.centerx,\n","#                                10 * self.screen_rect.centery / 10),\n","#                                self.window, (1 / 12), \"text\"))\n","\n","        menu_options.append(TextBlock('MENU',\n","                            (self.screen_rect.centerx,\n","                            10 * self.screen_rect.centery / 10),\n","                            self.window, (1 / 12), \"menu\"))\n","        selected_option = self.cycle_menu(menu_options, list_menu, OPTIONS)\n","\n","    @staticmethod\n","    def format_scores(scores, ammount):\n","        scores = scores[-ammount:]\n","\n","\n","\n","    @staticmethod\n","    def eval_local_safety(canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size board_size**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if ((body[0][0] + 1) > (VAR.board_size - 1)\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]):\n","            canvas[VAR.board_size - 1, 0] = POINT_TYPE['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[VAR.board_size - 1, 1] = POINT_TYPE['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[VAR.board_size - 1, 2] = POINT_TYPE['DANGEROUS']\n","        if ((body[0][1] + 1) > (VAR.board_size - 1)\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]):\n","            canvas[VAR.board_size - 1, 3] = POINT_TYPE['DANGEROUS']\n","\n","        return canvas\n","\n","    @staticmethod\n","    def gradient(colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for substep in range(1, substeps):\n","                yield tuple([(start[component]\n","                              + (float(substep) / (substeps - 1))\n","                              * (finish[component] - start[component]))\n","                             for component in range(components)])\n","\n","        def pairs(seq):\n","            first_color, second_color = tee(seq)\n","            next(second_color, None)\n","\n","            return zip(first_color, second_color)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for first_color, second_color in pairs(colors):\n","            for gradient_color in linear_gradient(first_color, second_color,\n","                                                  substeps):\n","                result.append(gradient_color)\n","\n","        return result\n","\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","VAR = GlobalVariables() # Initializing GlobalVariables\n","LOGGER = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","\n","\"\"\"THIS\"\"\"\n","\n","import numpy as np\n","from random import sample, uniform, random\n","from array import array  # Efficient numeric arrays\n","\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes\n","    ----------\n","    memory: list of experiences\n","        Memory list to insert experiences.\n","    memory_size: int, optional, default = 150000\n","        The ammount of experiences to be stored in the memory.\n","    input_shape: tuple of 3 * int\n","        The shape of the input which will be stored.\n","    \"\"\"\n","    def __init__(self, memory_size = 150000):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        self.memory.append(experience)\n","\n","        if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","            self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\n","\n","        Return\n","        ----------\n","        batch: np.array of batch_size experiences\n","            The batched experiences from memory.\n","        IS_weights: np.array of batch_size of the weights\n","            As it's used only in PER, is an array of ones in this case.\n","        Indexes: list of batch_size * int\n","            As it's used only in PER, return None.\n","        \"\"\"\n","        IS_weights = np.ones((batch_size, ))\n","        batch = np.array(sample(self.memory, batch_size))\n","\n","        return batch, IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            memory_size = 150000\n","\n","        self.memory = []\n","\n","\n","class PrioritizedExperienceReplayNaive:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 150000, alpha = 0.6, epsilon = 0.001,\n","                 beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return self.exp\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        max_priority = self.memory.max_leaf()\n","\n","        if max_priority == 0:\n","            max_priority = self.get_priority(0)\n","\n","        self.memory.insert(experience, max_priority)\n","        self.exp += 1\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        batch = [None] * batch_size\n","        IS_weights = np.zeros((batch_size, ))\n","        tree_indices = [0] * batch_size\n","\n","        memory_sum = self.memory.sum()\n","        len_seg = memory_sum / batch_size\n","        min_prob = self.memory.min_leaf() / memory_sum\n","\n","        for i in range(batch_size):\n","            val = uniform(len_seg * i, len_seg * (i + 1))\n","            tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","            prob = priority / self.memory.sum()\n","            IS_weights[i] = np.power(prob / min_prob, -self.beta)\n","\n","        return np.array(batch), IS_weights, tree_indices\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = SumTree(self.memory_size)\n","        self.exp = 0\n","\n","\n","class PrioritizedExperienceReplay:\n","    def __init__(self, memory_size, nb_epoch = 10000, epsilon = 0.001,\n","                 alpha = 0.6, beta = 0.4, decay = 0.5):\n","        self.memory_size = memory_size\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.max_priority = 1.0\n","        self.reset_memory()\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","        if self.exp_size() < self.memory_size:\n","            self.memory.append(experience)\n","            self.pos += 1\n","        else:\n","            self.memory[self.pos] = experience\n","            self.pos = (self.pos + 1) % self.memory_size\n","\n","        self._it_sum[self.pos] = self.max_priority ** self.alpha\n","        self._it_min[self.pos] = self.max_priority ** self.alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = array('i')\n","\n","        for _ in range(batch_size):\n","            mass = random() * self._it_sum.sum(0, self.exp_size() - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","\n","        return res\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def get_samples(self, batch_size):\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = array('f')\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * self.exp_size()) ** (-self.beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * self.exp_size()) ** (-self.beta)\n","            weights.append(weight / max_weight)\n","\n","        weights = np.array(weights, dtype=np.float32)\n","        samples = [self.memory[idx] for idx in idxes]\n","\n","        return np.array(samples), weights, idxes\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def update_priorities(self, idxes, errors):\n","        priorities = self.get_priority(errors)\n","\n","        for idx, priority in zip(idxes, priorities):\n","            self._it_sum[idx] = priority ** self.alpha\n","            self._it_min[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = []\n","        self.pos = 0\n","\n","        it_capacity = 1\n","\n","        while it_capacity < self.memory_size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n","\n","\n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms\n","----------\n","    * Simple Deep Q-network (DQN with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double Deep Q-network (Double DQN);\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling Deep Q-network (Dueling DQN);\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * Prioritized Experience Replay (PER);\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns (n-steps);\n","        Paper: https://arxiv.org/pdf/1703.01327\n","    * Noisy nets.\n","        Paper: https://arxiv.org/abs/1706.10295\n","\n","Arguments\n","----------\n","--load: 'file.h5'\n","    Load a previously trained model in '.h5' format.\n","--board_size: int, optional, default = 10\n","    Assign the size of the board.\n","--nb_frames: int, optional, default = 4\n","    Assign the number of frames per stack, default = 4.\n","--nb_actions: int, optional, default = 5\n","    Assign the number of actions possible.\n","--update_freq: int, optional, default = 0.001\n","    Whether to soft or hard update the target. Epochs or ammount of the update.\n","--visual: boolean, optional, default = False\n","    Select wheter or not to draw the game in pygame.\n","--double: boolean, optional, default = False\n","    Use a target network with double DQN logic.\n","--dueling: boolean, optional, default = False\n","    Whether to use dueling network logic, Q(s,a) = A + V.\n","--per: boolean, optional, default = False\n","    Use Prioritized Experience Replay (based on Sum Trees).\n","--local_state: boolean, optional, default = True\n","    Verify is possible next moves are dangerous (field expertise)\n","    THIS\n","\"\"\"\n","\n","import numpy as np\n","from array import array\n","import random\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes\n","    ----------\n","    memory: object\n","        Memory used in training. ExperienceReplay or PrioritizedExperienceReplay\n","    memory_size: int, optional, default = -1\n","        Capacity of the memory used.\n","    model: keras model\n","        The input model in Keras.\n","    target: keras model, optional, default = None\n","        The target model, used to calculade the fixed Q-targets.\n","    nb_frames: int, optional, default = 4\n","        Ammount of frames for each experience (sars).\n","    board_size: int, optional, default = 10\n","        Size of the board used.\n","    frames: list of experiences\n","        The buffer of frames, store sars experiences.\n","    per: boolean, optional, default = False\n","        Flag for PER usage.\n","    update_target_freq: int or float, default = 0.001\n","        Whether soft or hard updates occur. If < 1, soft updated target model.\n","    n_steps: int, optional, default = 1\n","        Size of the rewards buffer, to use Multi-step returns.\n","    \"\"\"\n","    def __init__(self, model, sess, target = None, memory_size = -1, nb_frames = 4,\n","                 board_size = 10, per = False, update_target_freq = 0.001):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if per:\n","            self.memory = PrioritizedExperienceReplay(memory_size = memory_size)\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.update_target_freq = update_target_freq\n","        self.sess = sess\n","        self.set_noise_list()\n","        self.clear_frames()\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def set_noise_list(self):\n","        \"\"\"Set a list of noise variables if NoisyNet is involved.\"\"\"\n","        self.noise_list = []\n","        for layer in self.model.layers:\n","            if type(layer) in {NoisyDenseFG}:\n","                self.noise_list.extend(layer.noise_list)\n","\n","    def sample_noise(self):\n","        \"\"\"Resample noise variables in NoisyNet.\"\"\"\n","        for noise in self.noise_list:\n","            self.sess.run(noise.initializer)\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\n","\n","        Return\n","        ----------\n","        expanded_frames: list of experiences\n","            The buffer of frames, shape = (nb_frames, board_size, board_size)\n","        \"\"\"\n","        frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        expanded_frames = np.expand_dims(self.frames, 0)\n","        # expanded_frames = np.transpose(expanded_frames, [0, 3, 2, 1])\n","\n","        return expanded_frames\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model_hard(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def transfer_weights(self):\n","        \"\"\"Transfer Weights from Model to Target at rate update_target_freq.\"\"\"\n","        model_weights = self.model.get_weights()\n","        target_weights = self.target.get_weights()\n","\n","        for i in range(len(W)):\n","            target_weights[i] = (self.update_target_freq * model_weights[i]\n","                                 + ((1 - self.update_target_frequency)\n","                                    * target_weights[i]))\n","\n","        self.target.set_weights(target_weights)\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, policy, value,\n","                      win_count, history_step, history_reward,\n","                      history_loss = None, verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    np.mean(history_size[-10:]),\n","                                    max(history_size[-10:]),\n","                                    np.mean(history_step[-10:]),\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}'  # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            if loss is not None:  # Print training performance\n","                text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                              + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                              + 'Mean loss - 100 episodes: {:.4f}')\n","                print(text_perf.format(history_loss[-1],\n","                                       history_loss[-1] / history_step[-1],\n","                                       np.mean(history_loss[-100:])))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   np.mean(history_step[-100:])))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\n","\n","        Return\n","        ----------\n","        loss: float\n","            Training loss of given batch.\n","        \"\"\"\n","        loss = 0.\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions,\n","                                        n_steps = self.n_steps)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, optim_rounds = 1, policy = \"EpsGreedyQPolicy\",\n","              verbose = 1, n_steps = 1):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        if not hasattr(self, 'n_steps'):\n","            self.n_steps = n_steps  # Set attribute only once\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_loss = array('f')  # Holds all the losses\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        # Select exploration policy. EpsGreedyQPolicy runs faster, but takes\n","        # longer to converge. BoltzmannGumbelQPolicy is the slowest, but\n","        # converge really fast (0.1 * nb_epoch used in EpsGreedyQPolicy).\n","        # BoltzmannQPolicy is in the middle.\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        elif policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        # If optim_rounds is bigger than one, the model will keep optimizing\n","        # after the exploration, in turns of nb_epoch size.\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                    text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                                  + '| Loss: {:.4f}')\n","                    print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:  # Exploration and training\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    game.reset_game()\n","                    self.clear_frames()\n","                    S = self.get_game_data(game)\n","\n","                    if n_steps > 1:  # Create multi-step returns buffer.\n","                        n_step_buffer = array('f')\n","\n","                    while not game.game_over:  # Main loop, until game_over\n","                        game.food_pos = game.generate_food()\n","                        self.sample_noise()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","                        game.play(action)\n","                        r = game.get_reward()\n","                        total_reward += r\n","\n","                        if n_steps > 1:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","\n","                                n_step_buffer.pop(0)\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience)  # Add to the memory\n","                        S = S_prime  # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe:  # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1  # Counter of wins for metrics\n","\n","                    if self.per:  # Advance beta, used in PER\n","                        self.memory.beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None:  # Update the target model\n","                        if update_target_freq >= 1: # Hard updates\n","                            if epoch % self.update_target_freq == 0:\n","                                self.update_target_model_hard()\n","                        elif update_target_freq < 1.:  # Soft updates\n","                            self.transfer_weights()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch = epoch, nb_epoch = nb_epoch,\n","                                           history_size = history_size,\n","                                           history_loss = history_loss,\n","                                           history_step = history_step,\n","                                           history_reward = history_reward,\n","                                           policy = policy, value = value,\n","                                           win_count = win_count,\n","                                           verbose = verbose)\n","\n","    def test(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","\n","            if visual:\n","                game.create_window()\n","                previous_size = game.snake.length  # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","                elapsed = 0\n","\n","            while not game.game_over:\n","                if visual:\n","                    elapsed += game.fps.get_time()  # Get elapsed time since last call.\n","\n","                    if elapsed >= 60:\n","                        elapsed = 0\n","                        S = self.get_game_data(game)\n","                        action, value = q_policy.select_action(self.model, S, epoch, game.nb_actions)\n","                        game.play(action)\n","                        current_size = game.snake.length  # Update the body size\n","\n","                        if current_size > previous_size:\n","                            color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                       game.snake.length)\n","\n","                            previous_size = current_size\n","\n","                        game.draw(color_list)\n","\n","                    pygame.display.update()\n","                    game.fps.tick(120)  # Limit FPS to 100\n","                else:\n","                    S = self.get_game_data(game)\n","                    action, value = q_policy.select_action(self.model, S, epoch, game.nb_actions)\n","                    game.play(action)\n","                    current_size = game.snake.length  # Update the body size\n","\n","                if game.game_over:\n","                    history_size.append(current_size)\n","                    history_step.append(game.step)\n","                    history_reward.append(game.get_reward())\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(history_size), np.max(history_size),\n","                      np.min(history_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(history_step), np.max(history_step),\n","                      np.min(history_step)))\n","        print(\"Mean rewards: {} | Biggest reward: {} | Smallest reward: {}\"\\\n","              .format(np.mean(history_reward), np.max(history_reward),\n","                      np.min(history_reward)))\n","\n","\"\"\"THIS\"\"\"\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.python.framework import tensor_shape\n","from tensorflow.python.layers import base\n","from tensorflow.python.ops.init_ops import Constant\n","\n","class NoisyDense(tf.keras.layers.Dense):\n","\n","    def build(self, input_shape):\n","        input_shape = tensor_shape.TensorShape(input_shape)\n","        if input_shape[-1].value is None:\n","            raise ValueError('The last dimension of the inputs to `Dense` '\n","                             'should be defined. Found `None`.')\n","        self.input_spec = base.InputSpec(min_ndim=2,\n","                                         axes={-1: input_shape[-1].value})\n","        kernel_shape = [input_shape[-1].value, self.units]\n","        kernel_quiet = self.add_variable('kernel_quiet',\n","                                         shape=kernel_shape,\n","                                         initializer=self.kernel_initializer,\n","                                         regularizer=self.kernel_regularizer,\n","                                         constraint=self.kernel_constraint,\n","                                         dtype=self.dtype,\n","                                         trainable=True)\n","        scale_init = Constant(value=(0.5 / np.sqrt(kernel_shape[0])))\n","        kernel_noise_scale = self.add_variable('kernel_noise_scale',\n","                                               shape=kernel_shape,\n","                                               initializer=scale_init,\n","                                               dtype=self.dtype,\n","                                               trainable=True)\n","        kernel_noise = self.make_kernel_noise(shape=kernel_shape)\n","        self.kernel = kernel_quiet + kernel_noise_scale * kernel_noise\n","        if self.use_bias:\n","            bias_shape = [self.units,]\n","            bias_quiet = self.add_variable('bias_quiet',\n","                                           shape=bias_shape,\n","                                           initializer=self.bias_initializer,\n","                                           regularizer=self.bias_regularizer,\n","                                           constraint=self.bias_constraint,\n","                                           dtype=self.dtype,\n","                                           trainable=True)\n","            bias_noise_scale = self.add_variable(name='bias_noise_scale',\n","                                                 shape=bias_shape,\n","                                                 initializer=scale_init,\n","                                                 dtype=self.dtype,\n","                                                 trainable=True)\n","            bias_noise = self.make_bias_noise(shape=bias_shape)\n","            self.bias = bias_quiet + bias_noise_scale * bias_noise\n","        else:\n","            self.bias = None\n","        self.built = True\n","\n","    def make_kernel_noise(self, shape):\n","        raise NotImplementedError\n","\n","    def make_bias_noise(self, shape):\n","        raise NotImplementedError\n","\n","\n","class NoisyDenseIG(NoisyDense):\n","    '''\n","    Noisy dense layer with independent Gaussian noise\n","    '''\n","    def make_kernel_noise(self, shape):\n","        noise = tf.random_normal(shape, dtype=self.dtype)\n","        kernel_noise = tf.Variable(noise, trainable=False, dtype=self.dtype)\n","        self.noise_list = [kernel_noise]\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        noise = tf.random_normal(shape, dtype=self.dtype)\n","        bias_noise = tf.Variable(noise, trainable=False, dtype=self.dtype)\n","        self.noise_list.append(bias_noise)\n","        return bias_noise\n","\n","\n","class NoisyDenseFG(NoisyDense):\n","    '''\n","    Noisy dense layer with factorized Gaussian noise\n","    '''\n","    def make_kernel_noise(self, shape):\n","        kernel_noise_input = self.make_fg_noise(shape=[shape[0]])\n","        kernel_noise_output = self.make_fg_noise(shape=[shape[1]])\n","        self.noise_list = [kernel_noise_input, kernel_noise_output]\n","        kernel_noise = kernel_noise_input[:, tf.newaxis] * kernel_noise_output\n","        return kernel_noise\n","\n","    def make_bias_noise(self, shape):\n","        return self.noise_list[1] # kernel_noise_output\n","\n","    def make_fg_noise(self, shape):\n","        noise = tf.random_normal(shape, dtype=self.dtype)\n","        trans_noise = tf.sign(noise) * tf.sqrt(tf.abs(noise))\n","        return tf.Variable(trans_noise, trainable=False, dtype=self.dtype)\n","        \n","        \n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","import random\n","import numpy as np\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","#!/usr/bin/env python\n","\n","\"\"\"clipped_error: L1 for errors < clip_value else L2 error.\n","\n","Functions:\n","    huber_loss: Return L1 error if absolute error is less than clip_value, else\n","                return L2 error.\n","    clipped_error: Call huber_loss with default clip_value to 1.0.\n","\"\"\"\n","\n","import numpy as np\n","from keras import backend as K\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * tf.square(x)\n","\n","\tcondition = tf.abs(x) < clip_value\n","\tsquared_loss = .5 * tf.square(x)\n","\tlinear_loss = clip_value * (tf.abs(x) - .5 * clip_value)\n","\n","\tif hasattr(tf, 'select'):\n","\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\telse:\n","\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\n","def clipped_error(y_true, y_pred):\n","\treturn tf.keras.backend.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","#def CNN1(optimizer, loss, stack, input_size, output_size):\n"," #   model = Sequential()\n","  #  model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (stack,\n","   #                                                                  input_size,\n","    #                                                                 input_size)))\n","#    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n"," #   model.add(Conv2D(128, (3, 3), activation = 'relu'))\n","  #  model.add(Conv2D(256, (3, 3), activation = 'relu'))\n","   # model.add(Flatten())\n","    #model.add(Dense(1024, activation = 'relu'))\n","    #model.add(Dense(output_size))\n","    #model.compile(optimizer = optimizer, loss = loss)\n","\n","    #return model\n","    \n","def CNN4(optimizer, loss, stack, input_size, output_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    model = Sequential()\n","    model.add(Conv2D(32, (4, 4), activation = 'relu', input_shape = (stack,\n","                                                                    input_size,\n","                                                                    input_size)))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Flatten())\n","    model.add(Dense(3136, activation = 'relu'))\n","    model.add(Dense(output_size))\n","    model.compile(optimizer = optimizer, loss = loss)\n","\n","    return model\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import Sequential, load_model, Model\n","from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Flatten,\\\n","                             Input, Lambda, Add\n","K.set_image_dim_ordering('th')  \n","  \n","board_size = 10\n","nb_frames = 4\n","  \n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","\n","with tf.Session() as sess:\n","    model = CNN4(optimizer = RMSprop(), loss = clipped_error,\n","                                stack = nb_frames, input_size = board_size,\n","                                output_size = game.nb_actions)\n","    target = None\n","\n","    sess.run(tf.global_variables_initializer())\n","    agent = Agent(model = model, sess = sess, target = target, memory_size = -1,\n","                      nb_frames = nb_frames, board_size = board_size,\n","                      per = False, update_target_freq = 500)\n","    agent.train(game, batch_size = 64, nb_epoch = 10000,\n","                    gamma = 0.95, n_steps = 1)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 010/10000 | Mean size 10: 3.6 | Longest 10: 006 | Mean steps 10: 10.7 | Wins: 3 | Win percentage: 30.0%\n","Epoch: 020/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.0 | Wins: 5 | Win percentage: 25.0%\n","Epoch: 030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.9 | Wins: 5 | Win percentage: 16.7%\n","Epoch: 040/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 9.7 | Wins: 8 | Win percentage: 20.0%\n","Epoch: 050/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.1 | Wins: 10 | Win percentage: 20.0%\n","Epoch: 060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 11 | Win percentage: 18.3%\n","Epoch: 070/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.5 | Wins: 11 | Win percentage: 15.7%\n","Epoch: 080/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.0 | Wins: 11 | Win percentage: 13.8%\n","Epoch: 090/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.1 | Wins: 11 | Win percentage: 12.2%\n","Epoch: 100/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.3 | Wins: 12 | Win percentage: 12.0%\n","Epoch: 110/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 13 | Win percentage: 11.8%\n","Epoch: 120/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.8 | Wins: 13 | Win percentage: 10.8%\n","Epoch: 130/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 12.7 | Wins: 14 | Win percentage: 10.8%\n","Epoch: 140/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 18 | Win percentage: 12.9%\n","Epoch: 150/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 11.0 | Wins: 21 | Win percentage: 14.0%\n","Epoch: 160/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.3 | Wins: 22 | Win percentage: 13.8%\n","Epoch: 170/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 7.4 | Wins: 22 | Win percentage: 12.9%\n","Epoch: 180/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.4 | Wins: 23 | Win percentage: 12.8%\n","Epoch: 190/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 25 | Win percentage: 13.2%\n","Epoch: 200/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.3 | Wins: 25 | Win percentage: 12.5%\n","Epoch: 210/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.7 | Wins: 25 | Win percentage: 11.9%\n"],"name":"stdout"}]},{"metadata":{"id":"NyW96nYjkKAq","colab_type":"code","colab":{}},"cell_type":"code","source":["#model.save('keras.h5')\n","\n","#!zip -r model-epsgreedy-bench.zip keras.h5 \n","#from google.colab import files\n","#files.download('model-epsgreedy-bench.zip')\n","#model = load_model('keras.h5', custom_objects={'clipped_error': clipped_error})\n","\n","board_size = 10\n","nb_frames = 4\n","nb_actions = 5\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = -1,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","agent.test(game, visual = False, nb_epoch = 1000)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9zZ60YLvt4tP","colab_type":"code","colab":{}},"cell_type":"code","source":["#!/usr/bin/env python\n","import numpy as np\n","from argparse import ArgumentParser\n","\n","from keras import backend as K\n","import keras.optimizers as optimizers\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","class HandleArguments:\n","        \"\"\"Handle arguments provided in the command line when executing the model.\n","\n","        Attributes:\n","            args: arguments parsed in the command line.\n","            status_load: a flag for usage of --load argument.\n","            status_visual: a flag for usage of --visual argument.\n","\n","            NEED UPDATE!\n","        \"\"\"\n","        def __init__(self):\n","            self.parser = ArgumentParser() # Receive arguments\n","            self.parser.add_argument(\"-l\", \"--load\", help = \"load a previously trained model. the argument is the filename\", required = False, default = \"\")\n","            self.parser.add_argument(\"-v\", \"--visual\", help = \"define board size\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-du\", \"--dueling\", help = \"use dueling DQN\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-do\", \"--double\", help = \"use double DQN\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-p\", \"--per\", help = \"use Prioritized Experience Replay\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-ls\", \"--local_state\", help = \"define board size\", required = False, action = 'store_true')\n","            self.parser.add_argument(\"-g\", \"--board_size\", help = \"define board size\", required = False, default = 10, type = int)\n","            self.parser.add_argument(\"-nf\", \"--nb_frames\", help = \"define board size\", required = False, default = 4, type = int)\n","            self.parser.add_argument(\"-na\", \"--nb_actions\", help = \"define board size\", required = False, default = 5, type = int)\n","            self.parser.add_argument(\"-uf\", \"--update_freq\", help = \"frequency to update target\", required = False, default = 500, type = int)\n","\n","            self.args = self.parser.parse_args()\n","            self.status_load = False\n","            self.status_visual = False\n","            self.local_state = False\n","            self.dueling = False\n","            self.double = False\n","            self.per = False\n","\n","            if self.args.load:\n","                script_dir = path.dirname(__file__) # Absolute dir the script is in\n","                abs_file_path = path.join(script_dir, self.args.load)\n","                model = load_model(abs_file_path)\n","\n","                self.status_load = True\n","\n","            if self.args.visual:\n","                self.status_visual = True\n","\n","            if self.args.local_state:\n","                self.local_state = True\n","\n","            if self.args.dueling:\n","                self.dueling = True\n","\n","            if self.args.double:\n","                self.double = True\n","\n","            if self.args.per:\n","                self.per = True\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\tif K.backend() == 'tensorflow':\n","\t\tif hasattr(tf, 'select'):\n","\t\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\t\telse:\n","\t\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\telif K.backend() == 'theano':\n","\t\tfrom theano import tensor as T\n","\t\treturn T.switch(condition, squared_loss, linear_loss)\n","\telse:\n","\t\traise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","#!/usr/bin/env python\n","\n","\"\"\" Needs update!\n","\"\"\"\n","\n","import numpy as np\n","from keras.models import Sequential, load_model, Model\n","from keras.layers import *\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def weird_CNN(optimizer, loss, stack, input_size, output_size, min_neurons = 16,\n","         max_neurons = 128, kernel_size = (3,3), layers = 4):\n","    # INPUTS\n","    # size     - size of the input images\n","    # n_layers - number of layers\n","    # OUTPUTS\n","    # model    - compiled CNN\n","\n","    # Define hyperparamters\n","    MIN_NEURONS = min_neurons\n","    MAX_NEURONS = max_neurons\n","    KERNEL = kernel_size\n","    n_layers = layers\n","\n","    # Determine the # of neurons in each convolutional layer\n","    steps = np.floor(MAX_NEURONS / (n_layers + 1))\n","    neurons = np.arange(MIN_NEURONS, MAX_NEURONS, steps)\n","    neurons = neurons.astype(np.int32)\n","\n","    # Define a model\n","    model = Sequential()\n","\n","    # Add convolutional layers\n","    for i in range(0, n_layers):\n","        if i == 0:\n","            model.add(Conv2D(neurons[i], KERNEL, input_shape = (stack,\n","                                                                input_size,\n","                                                                input_size)))\n","        else:\n","            model.add(Conv2D(neurons[i], KERNEL))\n","\n","        model.add(Activation('relu'))\n","\n","    # Add max pooling layer\n","    model.add(MaxPooling2D(pool_size = (2, 2)))\n","    model.add(Flatten())\n","    model.add(Dense(MAX_NEURONS * 4))\n","    model.add(Activation('relu'))\n","\n","    # Add output layer\n","    model.add(Dense(output_size))\n","    model.add(Activation('sigmoid'))\n","\n","    # Compile the model\n","    model.compile(loss = loss, optimizer = optimizer)\n","\n","    return model\n","\n","def CNN1(inputs, stack, input_size):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","\n","    return model\n","\n","def CNN2(inputs, stack, input_size):\n","    net = Conv2D(16, (3, 3), activation = 'relu')(inputs)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = Conv2D(32, (3, 3), activation = 'relu')(net)\n","    net = MaxPooling2D(pool_size = (2, 2))(net)\n","    net = Flatten()(net)\n","\n","    return model\n","\n","def CNN3(inputs, stack, input_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    net = Conv2D(32, (4, 4), activation = 'relu')(inputs)\n","    net = Conv2D(64, (2, 2), activation = 'relu')(net)\n","    net = Conv2D(64, (2, 2), activation = 'relu')(net)\n","    net = Flatten()(net)\n","\n","    return net\n","\n","def create_cnn(cnn, inputs, stack, input_size):\n","    if cnn == \"CNN1\":\n","        net = CNN1(inputs, stack, input_size)\n","    elif cnn == \"CNN2\":\n","        net = CNN2(inputs, stack, input_size)\n","    else:\n","        net = CNN3(inputs, stack, input_size)\n","\n","    return net\n","\n","def create_model(optimizer, loss, stack, input_size, output_size,\n","                  dueling = False, cnn = \"CNN3\"):\n","    inputs = Input(shape = (stack, input_size, input_size))\n","    net = create_cnn(cnn, inputs, stack, input_size)\n","\n","    if dueling:\n","        advt = Dense(3136, activation = 'relu')(net)\n","        advt = Dense(output_size)(advt)\n","        value = Dense(3136, activation = 'relu')(net)\n","        value = Dense(1)(value)\n","\n","        # now to combine the two streams\n","        advt = Lambda(lambda advt: advt - tf.reduce_mean(advt, axis = -1,\n","                                                         keepdims = True))(advt)\n","        value = Lambda(lambda value: tf.tile(value, [1, output_size]))(value)\n","        final = Add()([value, advt])\n","    else:\n","        final = Dense(3136, activation = 'relu')(net)\n","        final = Dense(output_size)(final)\n","\n","    model = Model(inputs = inputs, outputs = final)\n","    model.compile(optimizer = optimizer, loss = loss)\n","\n","    return model\n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","# pylint: disable=C0111\n","\n","import numpy as np\n","import sys\n","import time\n","import operator\n","from datetime import timedelta\n","import collections\n","\n","class SegmentTree(object):\n","    def __init__(self, capacity, operation, neutral_element):\n","        \"\"\"Build a Segment Tree data structure.\n","        https://en.wikipedia.org/wiki/Segment_tree\n","        Can be used as regular array, but with two\n","        important differences:\n","            a) setting item's value is slightly slower.\n","               It is O(lg capacity) instead of O(1).\n","            b) user has access to an efficient `reduce`\n","               operation which reduces `operation` over\n","               a contiguous subsequence of items in the\n","               array.\n","        Paramters\n","        ---------\n","        capacity: int\n","            Total size of the array - must be a power of two.\n","        operation: lambda obj, obj -> obj\n","            and operation for combining elements (eg. sum, max)\n","            must for a mathematical group together with the set of\n","            possible values for array elements.\n","        neutral_element: obj\n","            neutral element for the operation above. eg. float('-inf')\n","            for max and 0 for sum.\n","        \"\"\"\n","        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n","        self._capacity = capacity\n","        self._value = [neutral_element for _ in range(2 * capacity)]\n","        self._operation = operation\n","\n","    def _reduce_helper(self, start, end, node, node_start, node_end):\n","        if start == node_start and end == node_end:\n","            return self._value[node]\n","        mid = (node_start + node_end) // 2\n","        if end <= mid:\n","            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n","        else:\n","            if mid + 1 <= start:\n","                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n","            else:\n","                return self._operation(\n","                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n","                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n","                )\n","\n","    def reduce(self, start=0, end=None):\n","        \"\"\"Returns result of applying `self.operation`\n","        to a contiguous subsequence of the array.\n","            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n","        Parameters\n","        ----------\n","        start: int\n","            beginning of the subsequence\n","        end: int\n","            end of the subsequences\n","        Returns\n","        -------\n","        reduced: obj\n","            result of reducing self.operation over the specified range of array elements.\n","        \"\"\"\n","        if end is None:\n","            end = self._capacity\n","        if end < 0:\n","            end += self._capacity\n","        end -= 1\n","        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n","\n","    def __setitem__(self, idx, val):\n","        # index of the leaf\n","        idx += self._capacity\n","        self._value[idx] = val\n","        idx //= 2\n","        while idx >= 1:\n","            self._value[idx] = self._operation(\n","                self._value[2 * idx],\n","                self._value[2 * idx + 1]\n","            )\n","            idx //= 2\n","\n","    def __getitem__(self, idx):\n","        assert 0 <= idx < self._capacity\n","        return self._value[self._capacity + idx]\n","\n","\n","class SumSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(SumSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=operator.add,\n","            neutral_element=0.0\n","        )\n","\n","    def sum(self, start=0, end=None):\n","        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n","        return super(SumSegmentTree, self).reduce(start, end)\n","\n","    def find_prefixsum_idx(self, prefixsum):\n","        \"\"\"Find the highest index `i` in the array such that\n","            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n","        if array values are probabilities, this function\n","        allows to sample indexes according to the discrete\n","        probability efficiently.\n","        Parameters\n","        ----------\n","        perfixsum: float\n","            upperbound on the sum of array prefix\n","        Returns\n","        -------\n","        idx: int\n","            highest index satisfying the prefixsum constraint\n","        \"\"\"\n","        assert 0 <= prefixsum <= self.sum() + 1e-5\n","        idx = 1\n","        while idx < self._capacity:  # while non-leaf\n","            if self._value[2 * idx] > prefixsum:\n","                idx = 2 * idx\n","            else:\n","                prefixsum -= self._value[2 * idx]\n","                idx = 2 * idx + 1\n","        return idx - self._capacity\n","\n","\n","class MinSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(MinSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=min,\n","            neutral_element=float('inf')\n","        )\n","\n","    def min(self, start=0, end=None):\n","        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n","\n","        return super(MinSegmentTree, self).reduce(start, end)\n","\n","\n","class SumTree:\n","    def __init__(self, capacity):\n","        self._capacity = capacity\n","        self._tree = np.zeros(2 * self._capacity - 1)\n","        self._data = np.zeros(self._capacity, dtype = object)\n","        self._data_idx = 0\n","\n","    @property\n","    def capacity(self):\n","        return self._capacity\n","\n","    @property\n","    def tree(self):\n","        return self._tree\n","\n","    @property\n","    def data(self):\n","        return self._data\n","\n","    def sum(self):\n","        return self._tree[0]\n","\n","    def insert(self, data, priority):\n","#        print(\"Data shape: {}\".format(data.shape))\n","#        print(\"Stored data shape: {}\".format(self._data.shape))\n","        self._data[self._data_idx] = data\n","        tree_idx = self._data_idx + self._capacity - 1\n","        self.update(tree_idx, priority)\n","        self._data_idx += 1\n","        if self._data_idx >= self._capacity:\n","            self._data_idx = 0\n","\n","    def update(self, tree_idx, priority):\n","        delta = priority - self._tree[tree_idx]\n","        self._tree[tree_idx] = priority\n","        while tree_idx != 0:\n","            tree_idx = (tree_idx - 1) // 2  # Get parent\n","            self._tree[tree_idx] += delta\n","\n","    def retrieve(self, val):\n","        tree_idx, parent = None, 0\n","        while True:\n","            left = 2 * parent + 1\n","            right = left + 1\n","            if left >= len(self._tree):  # Leaf\n","                tree_idx = parent\n","                break\n","            else:\n","                if val <= self._tree[left]:\n","                    parent = left\n","                else:\n","                    val -= self._tree[left]\n","                    parent = right\n","\n","        priority = self._tree[tree_idx]\n","        data = self._data[tree_idx - self._capacity + 1]\n","\n","        return tree_idx, priority, data\n","\n","    def max_leaf(self):\n","        return np.max(self.leaves())\n","\n","    def min_leaf(self):\n","        return np.min(self.leaves())\n","\n","    def leaves(self):\n","        return self._tree[-self._capacity:]\n","\n","#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI.\n","\"\"\"\n","\n","import sys  # To close the window when the game is over\n","from array import array  # Efficient numeric arrays\n","from os import environ, path  # To center the game window the best possible\n","import random  # Random numbers used for the food\n","import logging  # Logging function for movements and errors\n","from itertools import tee  # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4,\n","           'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","rewards = {'MOVE': -0.005, 'GAME_OVER': -1, 'SCORED': 1}\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","# Speed levels possible to human players, MEGA HARDCORE starts with MEDIUM and\n","# increases with snake size\n","levels = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","speeds = {'EASY': 80, 'MEDIUM': 60, 'HARD': 40}\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    BOARD_SIZE: int, optional, default = 30\n","        The size of the board.\n","    BLOCK_SIZE: int, optional, default = 20\n","        The size in pixels of a block.\n","    HEAD_COLOR: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    TAIL_COLOR: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    FOOD_COLOR: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    GAME_SPEED: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    BENCHMARK: int, optional, default = 10\n","        Ammount of matches to BENCHMARK and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, BOARD_SIZE = 30, BLOCK_SIZE = 20,\n","                 HEAD_COLOR = (42, 42, 42), TAIL_COLOR = (152, 152, 152),\n","                 FOOD_COLOR = (200, 0, 0), GAME_SPEED = 80, GAME_FPS = 100,\n","                 BENCHMARK = 10):\n","        \"\"\"Initialize all global variables. Can be updated with argument_handler.\n","        \"\"\"\n","        self.BOARD_SIZE = BOARD_SIZE\n","        self.BLOCK_SIZE = BLOCK_SIZE\n","        self.HEAD_COLOR = HEAD_COLOR\n","        self.TAIL_COLOR = TAIL_COLOR\n","        self.FOOD_COLOR = FOOD_COLOR\n","        self.GAME_SPEED = GAME_SPEED\n","        self.GAME_FPS = GAME_FPS\n","        self.BENCHMARK = BENCHMARK\n","\n","        if self.BOARD_SIZE > 50: # Warn the user about performance\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                pass\n","            else:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [BOARD_SIZE / 4, BOARD_SIZE / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def is_movement_invalid(self, action):\n","        valid = False\n","\n","        if (action, self.previous_action) in forbidden_moves:\n","            valid = True\n","\n","        return valid\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if action == actions['IDLE'] or self.is_movement_invalid(action):\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with BOARD_SIZE * BLOCK_SIZE dimension.\"\"\"\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images/snake_logo.png\"))\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE, int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                                  4 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                                  6 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                     8 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                option.draw()\n","\n","                if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                    option.hovered = True\n","\n","                    if option == menu_options[0]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['PLAY']\n","                    elif option == menu_options[1]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['BENCHMARK']\n","                    elif option == menu_options[2]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['LEADERBOARDS']\n","                    elif option == menu_options[3]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['QUIT']\n","                else:\n","                    option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            self.window.blit(img, img_rect.bottomleft)\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                                 4 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                                 12 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                var.GAME_SPEED, mega_hardcore = self.select_speed()\n","                self.reset_game()\n","                self.start_match()\n","                score = self.single_player(mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                var.GAME_SPEED, mega_hardcore = self.select_speed()\n","                score = array('i')\n","\n","                for i in range(var.BENCHMARK):\n","                    self.reset_game()\n","                    self.start_match()\n","                    score.append(self.single_player(mega_hardcore))\n","\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        menu_options = [None] * 5\n","        menu_options[0] = TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                     4 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[1] = TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                     6 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[3] = TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                               10 * self.screen_rect.centery / 10),\n","                                               self.window, (1 / 15), \"menu\")\n","\n","        if isinstance(score, int):\n","            text_score = 'SCORE: ' + str(score)\n","        else:\n","            text_score = 'MEAN SCORE: ' + str(sum(score) / var.BENCHMARK)\n","            menu_options[2] = TextBlock(' ADD TO LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                                  8 * self.screen_rect.centery / 10),\n","                                                                  self.window, (1 / 15), \"menu\")\n","\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","        menu_options[4] = TextBlock(text_score, (self.screen_rect.centerx,\n","                                                 15 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\")\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['PLAY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['MENU']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['ADD_LEADERBOARDS']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    pygame.quit()\n","                                    sys.exit()\n","                    else:\n","                        option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        menu_options = [TextBlock(levels[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\")]\n","        mega_hardcore = False\n","        selected = False\n","        speed = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = speeds['EASY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = speeds['MEDIUM']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = speeds['HARD']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = speeds['MEDIUM']\n","                                    mega_hardcore = True\n","\n","                    else:\n","                        option.hovered = False\n","\n","            if speed is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return speed, mega_hardcore\n","\n","    def single_player(self, mega_hardcore = False):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where snakes moves after elapsed time is bigger than the\n","        # move_wait time. The last_key pressed is recorded to make the game more\n","        # smooth for human players.\n","        elapsed = 0\n","        last_key = self.snake.previous_action\n","        move_wait = var.GAME_SPEED\n","\n","        while not self.game_over:\n","            elapsed += self.fps.get_time()  # Get elapsed time since last call.\n","\n","            if mega_hardcore:  # Progressive speed increments, the hardest.\n","                move_wait = var.GAME_SPEED - (2 * (self.snake.length - 3))\n","\n","            key_input = self.handle_input()  # Receive inputs with tick.\n","            invalid_key = self.snake.is_movement_invalid(key_input)\n","\n","            if key_input is not None and not invalid_key:\n","                last_key = key_input\n","\n","            if elapsed >= move_wait:  # Move and redraw\n","                elapsed = 0\n","                self.game_over = self.play(last_key)\n","                current_size = self.snake.length  # Update the body size\n","\n","                if current_size > previous_size:\n","                    color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   current_size)\n","\n","                    previous_size = current_size\n","\n","                self.draw(color_list)\n","\n","            pygame.display.update()\n","            self.fps.tick(100)  # Limit FPS to 100\n","\n","        score = current_size - 3  # After the game is over, record score\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = None\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            action = actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            action = actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            action = actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            action = actions['DOWN']\n","\n","        return action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = point_type['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = rewards['MOVE']\n","\n","        if self.game_over:\n","            reward = rewards['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window    \n","\n","import numpy as np\n","from random import sample, uniform\n","from array import array  # Efficient numeric arrays\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes\n","    ----------\n","    memory: list of experiences\n","        Memory list to insert experiences.\n","    memory_size: int, optional, default = 150000\n","        The ammount of experiences to be stored in the memory.\n","    input_shape: tuple of 3 * int\n","        The shape of the input which will be stored.\n","    \"\"\"\n","    def __init__(self, memory_size = 150000):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        self.memory.append(experience)\n","\n","        if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","            self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\n","\n","        Return\n","        ----------\n","        batch: np.array of batch_size experiences\n","            The batched experiences from memory.\n","        IS_weights: np.array of batch_size of the weights\n","            As it's used only in PER, is an array of ones in this case.\n","        Indexes: list of batch_size * int\n","            As it's used only in PER, return None.\n","        \"\"\"\n","        IS_weights = np.ones((batch_size, ))\n","        batch = np.array(sample(self.memory, batch_size))\n","\n","        return batch, IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            memory_size = 150000\n","\n","        self.memory = []\n","\n","\n","class PrioritizedExperienceReplayNaive:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 150000, alpha = 0.6, epsilon = 0.001,\n","                 beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return self.exp\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        max_priority = self.memory.max_leaf()\n","\n","        if max_priority == 0:\n","            max_priority = self.get_priority(0)\n","\n","        self.memory.insert(experience, max_priority)\n","        self.exp += 1\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        batch = [None] * batch_size\n","        IS_weights = np.zeros((batch_size, ))\n","        tree_indices = [0] * batch_size\n","\n","        memory_sum = self.memory.sum()\n","        len_seg = memory_sum / batch_size\n","        min_prob = self.memory.min_leaf() / memory_sum\n","\n","        for i in range(batch_size):\n","            val = uniform(len_seg * i, len_seg * (i + 1))\n","            tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","            prob = priority / self.memory.sum()\n","            IS_weights[i] = np.power(prob / min_prob, -self.beta)\n","\n","        return np.array(batch), IS_weights, tree_indices\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = SumTree(self.memory_size)\n","        self.exp = 0\n","\n","\n","class PrioritizedExperienceReplay:\n","    def __init__(self, memory_size, nb_epoch = 10000, epsilon = 0.001,\n","                 alpha = 0.6, beta = 0.4, decay = 0.5):\n","        self.memory_size = memory_size\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.max_priority = 1.0\n","        self.reset_memory()\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","        if self.exp_size() < self.memory_size:\n","            self.memory.append(experience)\n","            self.pos += 1\n","        else:\n","            self.memory[self.pos] = experience\n","            self.pos = (self.pos + 1) % self.memory_size\n","\n","        self._it_sum[self.pos] = self.max_priority ** self.alpha\n","        self._it_min[self.pos] = self.max_priority ** self.alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = array('i')\n","\n","        for _ in range(batch_size):\n","            mass = random.random() * self._it_sum.sum(0, self.exp_size() - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","\n","        return res\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def get_samples(self, batch_size):\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = array('f')\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * self.exp_size()) ** (-self.beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * self.exp_size()) ** (-self.beta)\n","            weights.append(weight / max_weight)\n","\n","        weights = np.array(weights, dtype=np.float32)\n","        samples = [self.memory[idx] for idx in idxes]\n","\n","        return np.array(samples), weights, idxes\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def update_priorities(self, idxes, errors):\n","        priorities = self.get_priority(errors)\n","\n","        for idx, priority in zip(idxes, priorities):\n","            self._it_sum[idx] = priority ** self.alpha\n","            self._it_min[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = []\n","        self.pos = 0\n","\n","        it_capacity = 1\n","\n","        while it_capacity < self.memory_size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n","\n","\n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms\n","----------\n","    * Simple Deep Q-network (DQN with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double Deep Q-network (Double DQN);\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling Deep Q-network (Dueling DQN);\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * Prioritized Experience Replay (PER);\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns.\n","        Paper: https://arxiv.org/pdf/1703.01327\n","\n","Arguments\n","----------\n","--load: 'file.h5'\n","    Load a previously trained model in '.h5' format.\n","--board_size: int, optional, default = 10\n","    Assign the size of the board.\n","--nb_frames: int, optional, default = 4\n","    Assign the number of frames per stack, default = 4.\n","--nb_actions: int, optional, default = 5\n","    Assign the number of actions possible.\n","--update_freq: int, optional, default = 0.001\n","    Whether to soft or hard update the target. Epochs or ammount of the update.\n","--visual: boolean, optional, default = False\n","    Select wheter or not to draw the game in pygame.\n","--double: boolean, optional, default = False\n","    Use a target network with double DQN logic.\n","--dueling: boolean, optional, default = False\n","    Whether to use dueling network logic, Q(s,a) = A + V.\n","--per: boolean, optional, default = False\n","    Use Prioritized Experience Replay (based on Sum Trees).\n","--local_state: boolean, optional, default = True\n","    Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from array import array\n","from os import path, environ, sys\n","import random\n","import inspect\n","\n","# Making relative imports from parallel folders possible\n","currentdir = path.dirname(path.abspath(inspect.getfile(inspect.currentframe())))\n","parentdir = path.dirname(currentdir)\n","sys.path.insert(0, parentdir)\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import load_model\n","from keras import backend as K\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","K.set_image_dim_ordering('th')  # Setting keras ordering\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes\n","    ----------\n","    memory: object\n","        Memory used in training. ExperienceReplay or PrioritizedExperienceReplay\n","    memory_size: int, optional, default = -1\n","        Capacity of the memory used.\n","    model: keras model\n","        The input model in Keras.\n","    target: keras model, optional, default = None\n","        The target model, used to calculade the fixed Q-targets.\n","    nb_frames: int, optional, default = 4\n","        Ammount of frames for each experience (sars).\n","    board_size: int, optional, default = 10\n","        Size of the board used.\n","    frames: list of experiences\n","        The buffer of frames, store sars experiences.\n","    per: boolean, optional, default = False\n","        Flag for PER usage.\n","    update_target_freq: int or float, default = 0.001\n","        Whether soft or hard updates occur. If < 1, soft updated target model.\n","    n_steps: int, optional, default = 1\n","        Size of the rewards buffer, to use Multi-step returns.\n","    \"\"\"\n","    def __init__(self, model, target = None, memory_size = -1, nb_frames = 4,\n","                 board_size = 10, per = False, update_target_freq = 0.001):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if per:\n","            self.memory = PrioritizedExperienceReplay(memory_size = memory_size)\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.update_target_freq = update_target_freq\n","        self.clear_frames()\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\n","\n","        Return\n","        ----------\n","        expanded_frames: list of experiences\n","            The buffer of frames, shape = (nb_frames, board_size, board_size)\n","        \"\"\"\n","        frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        expanded_frames = np.expand_dims(self.frames, 0)\n","\n","        return expanded_frames\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model_hard(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def transfer_weights(self):\n","        \"\"\"Transfer Weights from Model to Target at rate update_target_freq.\"\"\"\n","        model_weights = self.model.get_weights()\n","        target_weights = self.target.get_weights()\n","\n","        for i in range(len(W)):\n","            target_weights[i] = (self.update_target_freq * model_weights[i]\n","                                 + ((1 - self.update_target_frequency)\n","                                    * target_weights[i]))\n","\n","        self.target.set_weights(target_weights)\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, policy, value,\n","                      win_count, history_step, history_reward,\n","                      history_loss = None, verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    np.mean(history_size[-10:]),\n","                                    max(history_size[-10:]),\n","                                    np.mean(history_step[-10:]),\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}'  # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            if loss is not None:  # Print training performance\n","                text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                              + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                              + 'Mean loss - 100 episodes: {:.4f}')\n","                print(text_perf.format(history_loss[-1],\n","                                       history_loss[-1] / history_step[-1],\n","                                       np.mean(history_loss[-100:])))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   np.mean(history_step[-100:])))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\n","\n","        Return\n","        ----------\n","        loss: float\n","            Training loss of given batch.\n","        \"\"\"\n","        loss = 0.\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions,\n","                                        n_steps = self.n_steps)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, optim_rounds = 1, policy = \"EpsGreedyQPolicy\",\n","              verbose = 1, n_steps = 1):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        if not hasattr(self, 'n_steps'):\n","            self.n_steps = n_steps  # Set attribute only once\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_loss = array('f')  # Holds all the losses\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        # Select exploration policy. EpsGreedyQPolicy runs faster, but takes\n","        # longer to converge. BoltzmannGumbelQPolicy is the slowest, but\n","        # converge really fast (0.1 * nb_epoch used in EpsGreedyQPolicy).\n","        # BoltzmannQPolicy is in the middle.\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        elif policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        # If optim_rounds is bigger than one, the model will keep optimizing\n","        # after the exploration, in turns of nb_epoch size.\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                              + '| Loss: {:.4f}')\n","                print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:  # Exploration and training\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    game.reset_game()\n","                    self.clear_frames()\n","                    S = self.get_game_data(game)\n","\n","                    if n_steps > 1:  # Create multi-step returns buffer.\n","                        n_step_buffer = array('f')\n","\n","                    while not game.game_over:  # Main loop, until game_over\n","                        game.food_pos = game.generate_food()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","                        game.play(action)\n","                        r = game.get_reward()\n","                        total_reward += r\n","\n","                        if n_steps > 1:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience)  # Add to the memory\n","                        S = S_prime  # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe:  # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1  # Counter of wins for metrics\n","\n","                    if self.per:  # Advance beta, used in PER\n","                        self.memory.beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None:  # Update the target model\n","                        if update_target_freq >= 1: # Hard updates\n","                            if epoch % self.update_target_freq == 0:\n","                                self.update_target_model_hard()\n","                        elif update_target_freq < 1.:  # Soft updates\n","                            self.transfer_weights()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch = epoch, nb_epoch = nb_epoch,\n","                                           history_size = history_size,\n","                                           history_loss = history_loss,\n","                                           history_step = history_step,\n","                                           history_reward = history_reward,\n","                                           policy = policy, value = value,\n","                                           win_count = win_count,\n","                                           verbose = verbose)\n","\n","    def play(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","            S = self.get_game_data(game)\n","\n","            if visual:\n","                game.create_window()\n","                # The main loop, it pump key_presses and update every tick.\n","                environ['SDL_VIDEO_CENTERED'] = '1'  # Centering the window\n","                previous_size = game.snake.length  # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","\n","            while not game.game_over:\n","                action, value = q_policy.select_action(self.model, S, epoch, nb_actions)\n","                game.play(action)\n","                current_size = game.snake.length  # Update the body size\n","\n","                if visual:\n","                    game.draw(color_list)\n","\n","                    if current_size > previous_size:\n","                        color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   game.snake.length)\n","\n","                        previous_size = current_size\n","\n","                S = self.get_game_data(game)\n","\n","                if game.game_over:\n","                    history_size.append(current_size)\n","                    history_step.append(game.step)\n","                    history_reward.append(game.get_reward())\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(history_size), np.max(history_size),\n","                      np.min(history_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(history_step), np.max(history_step),\n","                      np.min(history_step)))\n","        print(\"Mean rewards: {} | Biggest reward: {} | Smallest reward: {}\"\\\n","              .format(np.mean(history_reward), np.max(history_reward),\n","                      np.min(history_reward)))\n","        \n","board_size = 10\n","nb_frames = 4\n","  \n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","model = create_model(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions, dueling = False, cnn = \"CNN3\")\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = 3000000,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = True, update_target_freq = 0.001)\n","\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","agent.train(game, batch_size = 64, nb_epoch = 10000, gamma = 0.95, policy = \"EpsGreedyQPolicy\")        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"AqFM004Ht_Lp","colab_type":"code","colab":{}},"cell_type":"code","source":["model.save('keras.h5')\n","\n","!zip -r model-epsgreedy-per.zip keras.h5 \n","from google.colab import files\n","files.download('model-epsgreedy-per.zip')\n","model = load_model('keras.h5', custom_objects={'clipped_error': clipped_error})\n","\n","board_size = 10\n","nb_frames = 4\n","nb_actions = 5\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = 1500000,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = True)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","agent.play(game, visual = False, nb_epoch = 10000)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eT8JBzDWRlrV","colab_type":"code","outputId":"9e3d83cb-6bf4-4c4a-e7af-3f8dbb75f789","executionInfo":{"status":"ok","timestamp":1541410379254,"user_tz":120,"elapsed":5323305,"user":{"displayName":"Victor Neves","photoUrl":"https://lh5.googleusercontent.com/-zfn1KWpcK60/AAAAAAAAAAI/AAAAAAAAZas/oOSqICtc8i0/s64/photo.jpg","userId":"17687780415293205160"}},"colab":{"base_uri":"https://localhost:8080/","height":3417}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI.\n","\"\"\"\n","\n","import sys # To close the window when the game is over\n","from os import environ, path # To center the game window the best possible\n","import random # Random numbers used for the food\n","import logging # Logging function for movements and errors\n","from itertools import tee # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4,\n","           'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","rewards = {'MOVE': -0.005, 'GAME_OVER': -1, 'SCORED': 1}\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","# Speed levels possible to human players\n","levels = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    BOARD_SIZE: int, optional, default = 30\n","        The size of the board.\n","    BLOCK_SIZE: int, optional, default = 20\n","        The size in pixels of a block.\n","    HEAD_COLOR: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    TAIL_COLOR: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    FOOD_COLOR: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    GAME_SPEED: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    BENCHMARK: int, optional, default = 10\n","        Ammount of matches to BENCHMARK and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, BOARD_SIZE = 30, BLOCK_SIZE = 20,\n","                 HEAD_COLOR = (42, 42, 42), TAIL_COLOR = (152, 152, 152),\n","                 FOOD_COLOR = (200, 0, 0), GAME_SPEED = 10, BENCHMARK = 10):\n","        \"\"\"Initialize all global variables. Can be updated with argument_handler.\n","        \"\"\"\n","        self.BOARD_SIZE = BOARD_SIZE\n","        self.BLOCK_SIZE = BLOCK_SIZE\n","        self.HEAD_COLOR = HEAD_COLOR\n","        self.TAIL_COLOR = TAIL_COLOR\n","        self.FOOD_COLOR = FOOD_COLOR\n","        self.GAME_SPEED = GAME_SPEED\n","        self.BENCHMARK = BENCHMARK\n","\n","        if self.BOARD_SIZE > 50: # Warn the user about performance\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                pass\n","            else:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [BOARD_SIZE / 4, BOARD_SIZE / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if action == actions['IDLE']\\\n","            or (action, self.previous_action) in forbidden_moves:\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","      \n","import numpy as np\n","\n","class SumTree:\n","    def __init__(self, capacity):\n","        self._capacity = capacity\n","        self._tree = np.zeros(2 * self._capacity - 1)\n","        self._data = np.zeros(self._capacity, dtype = object)\n","        self._data_idx = 0\n","\n","    @property\n","    def capacity(self):\n","        return self._capacity\n","\n","    @property\n","    def tree(self):\n","        return self._tree\n","\n","    @property\n","    def data(self):\n","        return self._data\n","\n","    def sum(self):\n","        return self._tree[0]\n","\n","    def insert(self, data, priority):\n","#        print(\"Data shape: {}\".format(data.shape))\n","#        print(\"Stored data shape: {}\".format(self._data.shape))\n","        self._data[self._data_idx] = data\n","        tree_idx = self._data_idx + self._capacity - 1\n","        self.update(tree_idx, priority)\n","        self._data_idx += 1\n","        if self._data_idx >= self._capacity:\n","            self._data_idx = 0\n","\n","    def update(self, tree_idx, priority):\n","        delta = priority - self._tree[tree_idx]\n","        self._tree[tree_idx] = priority\n","        while tree_idx != 0:\n","            tree_idx = (tree_idx - 1) // 2  # Get parent\n","            self._tree[tree_idx] += delta\n","\n","    def retrieve(self, val):\n","        tree_idx, parent = None, 0\n","        while True:\n","            left = 2 * parent + 1\n","            right = left + 1\n","            if left >= len(self._tree):  # Leaf\n","                tree_idx = parent\n","                break\n","            else:\n","                if val <= self._tree[left]:\n","                    parent = left\n","                else:\n","                    val -= self._tree[left]\n","                    parent = right\n","\n","        priority = self._tree[tree_idx]\n","        data = self._data[tree_idx - self._capacity + 1]\n","\n","        return tree_idx, priority, data\n","\n","    def max_leaf(self):\n","        return np.max(self.leaves())\n","\n","    def min_leaf(self):\n","        return np.min(self.leaves())\n","\n","    def leaves(self):\n","        return self._tree[-self._capacity:]      \n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with BOARD_SIZE * BLOCK_SIZE dimension.\"\"\"\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images/snake_logo.png\"))\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE, int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                                  4 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                                  6 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                     8 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                option.draw()\n","\n","                if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                    option.hovered = True\n","\n","                    if option == menu_options[0]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['PLAY']\n","                    elif option == menu_options[1]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['BENCHMARK']\n","                    elif option == menu_options[2]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['LEADERBOARDS']\n","                    elif option == menu_options[3]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['QUIT']\n","                else:\n","                    option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            self.window.blit(img, img_rect.bottomleft)\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                                 4 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                                 12 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                var.GAME_SPEED = self.select_speed()\n","                self.reset_game()\n","                self.start_match()\n","                score = self.single_player()\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                var.GAME_SPEED = self.select_speed()\n","                score = []\n","\n","                for i in range(var.BENCHMARK):\n","                    self.reset_game()\n","                    self.start_match()\n","                    score.append(self.single_player())\n","\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        menu_options = [None] * 5\n","        menu_options[0] = TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                     4 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[1] = TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                     6 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[3] = TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                               10 * self.screen_rect.centery / 10),\n","                                               self.window, (1 / 15), \"menu\")\n","\n","        if isinstance(score, int):\n","            text_score = 'SCORE: ' + str(score)\n","        else:\n","            text_score = 'MEAN SCORE: ' + str(sum(score) / var.BENCHMARK)\n","            menu_options[2] = TextBlock(' ADD TO LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                                  8 * self.screen_rect.centery / 10),\n","                                                                  self.window, (1 / 15), \"menu\")\n","\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","        menu_options[4] = TextBlock(text_score, (self.screen_rect.centerx,\n","                                                 15 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\")\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['PLAY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['MENU']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['ADD_LEADERBOARDS']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    pygame.quit()\n","                                    sys.exit()\n","                    else:\n","                        option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        menu_options = [TextBlock(levels[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\")]\n","        selected = False\n","        speed = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 10\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 20\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 30\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = 45\n","                    else:\n","                        option.hovered = False\n","\n","            if speed is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return speed\n","\n","    def single_player(self):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where the snake keeps going each tick. It generate food,\n","        # check collisions and draw.\n","        while not self.game_over:\n","            action = self.handle_input()\n","            self.game_over = self.play(action)\n","            self.draw(color_list)\n","            current_size = self.snake.length # Update the body size\n","\n","            if current_size > previous_size:\n","                color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                           current_size)\n","\n","                previous_size = current_size\n","\n","        score = current_size - 3\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = self.snake.previous_action\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            action = actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            action = actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            action = actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            action = actions['DOWN']\n","\n","        return action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = point_type['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = rewards['MOVE']\n","\n","        if self.game_over:\n","            reward = rewards['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","        pygame.display.update()\n","        self.fps.tick(var.GAME_SPEED)\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","from keras.layers import Dense\n","from keras.models import Sequential, Model\n","import tensorflow as tf\n","from keras import backend as K\n","from keras.layers import Input\n","import numpy as np\n","\n","class Net(object):\n","    def __init__(self, nb_frames, board_size):\n","        model = Sequential()\n","        model.add(Dense(50, activation='relu', input_dim=(board_size**2)))\n","        model.add(Dense(50, activation='relu'))\n","        self.pi_model = Sequential([model])\n","        self.pi_model.add(Dense(512, activation='relu'))\n","        self.pi_model.add(Dense(5, activation='softmax'))\n","        self.v_model = Sequential([model])\n","        self.v_model.add(Dense(512, activation='relu'))\n","        self.v_model.add(Dense(1))\n","\n","\n","class PCL(object):\n","    def __init__(self, epoch, env, replay_buffer, sess=None, net=None,\n","            pi_optimizer=None, v_optimizer=None, off_policy_rate=20,\n","            pi_lr=7e-4, v_rate=0.5, entropy_tau=0.5, rollout_d=20, gamma=1):\n","        self.epoch = epoch\n","        self.env = env\n","        self.replay_buffer = replay_buffer\n","        self.sess = sess\n","        self.net = net\n","        if pi_optimizer is None:\n","            self.pi_optimizer = tf.train.AdamOptimizer(pi_lr)\n","        else:\n","            self.pi_optimizer = pi_optimizer\n","        if v_optimizer is None:\n","            self.v_optimizer = tf.train.AdamOptimizer(pi_lr*v_rate)\n","        else:\n","            self.v_optimizer = v_optimizer\n","        self.off_policy_rate = off_policy_rate\n","        self.entropy_tau = entropy_tau\n","        self.rollout_d = rollout_d\n","        self.gamma = gamma\n","        self.state_shape = 100\n","        self.action_shape = [5]\n","        self.built = False\n","\n","    def build(self):\n","        pi_model = self.net.pi_model\n","        v_model = self.net.v_model\n","        self.state = tf.placeholder(tf.float32, shape=[None, None, 100], name='state')\n","        self.R = tf.placeholder(tf.float32, shape=[None, None], name='R')\n","        self.action = tf.placeholder(tf.float32, shape=[None, None, 5], name='action')\n","        self.discount = tf.placeholder(tf.float32, shape=[None], name='discount')\n","\n","        v_s_t = v_model(self.state[:, 0, :])\n","        v_s_t_d = v_model(self.state[:, -1, :])\n","        self.pi = pi_model(self.state)\n","        C = K.sum(-v_s_t + self.gamma ** self.rollout_d * v_s_t_d + \\\n","                K.sum(self.R, axis=1) - self.entropy_tau * K.sum(self.discount * \\\n","                K.sum(K.log(self.pi+K.epsilon()) * self.action, axis=2), axis=1), axis=0)\n","        self.loss = C ** 2\n","\n","        self.updater = [self.pi_optimizer.minimize(self.loss, var_list=pi_model.trainable_weights),\n","                self.v_optimizer.minimize(self.loss, var_list=v_model.trainable_weights)]\n","        self.sess.run(tf.global_variables_initializer())\n","        self.built = True\n","\n","    def optimize(self, episode):\n","        if not self.built:\n","            self.build()\n","        if len(episode['states']) < self.rollout_d:\n","            rollout_d = len(episode['states'])\n","        else:\n","            rollout_d = self.rollout_d\n","        discount = np.array([self.gamma**i for i in range(rollout_d)], dtype=np.float32)\n","        state = []\n","        action = []\n","        R = []\n","        for i in range(len(episode['states'])-rollout_d+1):\n","            state.append(episode['states'][i:i+rollout_d])\n","            a = episode['actions'][i:i+rollout_d]\n","            action.append(np.eye(*self.action_shape, dtype=np.int32)[a])\n","            R.append(episode['rewards'][i:i+rollout_d])\n","        feed_in = {self.state: state, self.action: action, self.R: R, self.discount: discount}\n","        self.sess.run(self.updater, feed_in)\n","\n","    def rollout(self):\n","        states = []\n","        actions = []\n","        rewards = []\n","        agent_infos = []\n","        self.env.reset_game()\n","        s = self.env.state().flatten()\n","\n","        while not self.env.game_over:\n","            game.food_pos = game.generate_food()\n","            a, agent_info = self.get_action(s)\n","\n","            self.env.play(a)\n","\n","            r = self.env.get_reward()\n","            next_s = self.env.state().flatten()\n","\n","            states.append(s)\n","            rewards.append(r)\n","            actions.append(a)\n","            agent_infos.append(agent_info)\n","\n","            s = next_s\n","        return dict(\n","            states=np.array(states),\n","            actions=np.array(actions),\n","            rewards=np.array(rewards),\n","            agent_infos=np.array(agent_infos)\n","        )\n","\n","    def get_action(self, state):\n","        if not self.built:\n","            self.build()\n","        pi = self.sess.run(self.pi, {self.state: [[state]]})[0][0]\n","        a = np.random.choice(np.arange(self.action_shape[0]), p=pi)\n","        return a, dict(prob=pi)\n","\n","    def train(self):\n","        rewards = []\n","        entropy = []\n","        for i in range(self.epoch):\n","            episode = self.rollout()\n","            self.optimize(episode)\n","            rewards.append(episode['rewards'].sum())\n","            p = np.array([agent_info['prob'] for agent_info in episode['agent_infos']])\n","            ent = -np.sum(p * np.log(p+K.epsilon()), axis=1)\n","            entropy.append(ent.mean())\n","            if (i + 1) % 100 == 0:\n","                print(\"Epoch: {:03d}/{:03d} | \".format(i + 1, self.epoch), end = \"\")\n","                print(\"Reward: {:.2f} | \".format(np.mean(rewards[-100:])), end = \"\")\n","                print(\"Mean ent: {:.2f}\".format(np.mean(entropy[-100:])))\n","            self.replay_buffer.add(episode)\n","            if self.replay_buffer.trainable:\n","                for _ in range(self.off_policy_rate):\n","                    episode = self.replay_buffer.sample()\n","                    self.optimize(episode)\n","\n","\n","class ReplayBuffer(object):\n","    def __init__(self, max_len=3000000, alpha=1):\n","        self.max_len = max_len\n","        self.alpha = alpha\n","        self.buffer = []\n","        # weight is not normalized\n","        self.weight = np.array([])\n","\n","    def add(self, episode):\n","        self.buffer.append(episode)\n","        self.weight = np.append(self.weight, np.exp(self.alpha*episode['rewards'].sum()))\n","        if len(self.buffer) > self.max_len:\n","            delete_ind = np.random.randint(len(self.buffer))\n","            del self.buffer[delete_ind]\n","            self.weight = np.delete(self.weight, delete_ind)\n","\n","    def sample(self):\n","        return np.random.choice(self.buffer, p=self.weight/self.weight.sum())\n","\n","    @property\n","    def trainable(self):\n","        if len(self.buffer) > 32:\n","            return True\n","        else:\n","            return False\n","\n","board_size = 10\n","nb_frames = 4\n","\n","net = Net(1, board_size)\n","\n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","replay_buffer = ReplayBuffer()\n","\n","sess = tf.Session()\n","\n","agent = PCL(20000, game, replay_buffer, sess, net)\n","agent.train()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 100/20000 | Reward: -0.37 | Mean ent: 1.12\n","Epoch: 200/20000 | Reward: -0.42 | Mean ent: 0.83\n","Epoch: 300/20000 | Reward: -0.29 | Mean ent: 1.01\n","Epoch: 400/20000 | Reward: -0.46 | Mean ent: 0.89\n","Epoch: 500/20000 | Reward: -0.64 | Mean ent: 1.19\n","Epoch: 600/20000 | Reward: -0.32 | Mean ent: 1.11\n","Epoch: 700/20000 | Reward: -0.54 | Mean ent: 0.94\n","Epoch: 800/20000 | Reward: -0.46 | Mean ent: 0.82\n","Epoch: 900/20000 | Reward: -0.51 | Mean ent: 0.80\n","Epoch: 1000/20000 | Reward: -0.30 | Mean ent: 0.89\n","Epoch: 1100/20000 | Reward: -0.29 | Mean ent: 0.68\n","Epoch: 1200/20000 | Reward: -0.78 | Mean ent: 0.59\n","Epoch: 1300/20000 | Reward: -0.16 | Mean ent: 0.67\n","Epoch: 1400/20000 | Reward: -0.43 | Mean ent: 0.68\n","Epoch: 1500/20000 | Reward: 0.17 | Mean ent: 0.67\n","Epoch: 1600/20000 | Reward: 0.17 | Mean ent: 0.68\n","Epoch: 1700/20000 | Reward: -0.47 | Mean ent: 0.78\n","Epoch: 1800/20000 | Reward: -0.04 | Mean ent: 0.77\n","Epoch: 1900/20000 | Reward: 0.52 | Mean ent: 0.71\n","Epoch: 2000/20000 | Reward: -0.31 | Mean ent: 0.72\n","Epoch: 2100/20000 | Reward: 0.32 | Mean ent: 0.78\n","Epoch: 2200/20000 | Reward: -0.20 | Mean ent: 0.85\n","Epoch: 2300/20000 | Reward: 0.07 | Mean ent: 0.89\n","Epoch: 2400/20000 | Reward: 0.24 | Mean ent: 0.81\n","Epoch: 2500/20000 | Reward: 0.12 | Mean ent: 0.80\n","Epoch: 2600/20000 | Reward: -0.03 | Mean ent: 0.75\n","Epoch: 2700/20000 | Reward: -0.21 | Mean ent: 0.91\n","Epoch: 2800/20000 | Reward: 0.24 | Mean ent: 0.90\n","Epoch: 2900/20000 | Reward: 0.63 | Mean ent: 1.00\n","Epoch: 3000/20000 | Reward: -0.01 | Mean ent: 0.96\n","Epoch: 3100/20000 | Reward: 0.22 | Mean ent: 0.91\n","Epoch: 3200/20000 | Reward: 0.23 | Mean ent: 0.99\n","Epoch: 3300/20000 | Reward: 0.02 | Mean ent: 1.02\n","Epoch: 3400/20000 | Reward: -0.01 | Mean ent: 0.95\n","Epoch: 3500/20000 | Reward: -0.59 | Mean ent: 0.84\n","Epoch: 3600/20000 | Reward: -0.25 | Mean ent: 0.91\n","Epoch: 3700/20000 | Reward: -0.26 | Mean ent: 0.90\n","Epoch: 3800/20000 | Reward: -0.58 | Mean ent: 0.92\n","Epoch: 3900/20000 | Reward: -0.15 | Mean ent: 0.94\n","Epoch: 4000/20000 | Reward: 0.07 | Mean ent: 0.96\n","Epoch: 4100/20000 | Reward: 0.19 | Mean ent: 0.85\n","Epoch: 4200/20000 | Reward: 1.19 | Mean ent: 0.76\n","Epoch: 4300/20000 | Reward: 0.60 | Mean ent: 0.68\n","Epoch: 4400/20000 | Reward: 0.98 | Mean ent: 0.65\n","Epoch: 4500/20000 | Reward: -0.50 | Mean ent: 0.58\n","Epoch: 4600/20000 | Reward: -0.36 | Mean ent: 0.59\n","Epoch: 4700/20000 | Reward: -0.45 | Mean ent: 0.60\n","Epoch: 4800/20000 | Reward: -0.15 | Mean ent: 0.47\n","Epoch: 4900/20000 | Reward: -0.45 | Mean ent: 0.41\n","Epoch: 5000/20000 | Reward: 0.37 | Mean ent: 0.48\n","Epoch: 5100/20000 | Reward: -0.64 | Mean ent: 0.52\n","Epoch: 5200/20000 | Reward: -0.55 | Mean ent: 0.46\n","Epoch: 5300/20000 | Reward: -0.55 | Mean ent: 0.42\n","Epoch: 5400/20000 | Reward: 0.34 | Mean ent: 0.38\n","Epoch: 5500/20000 | Reward: 0.71 | Mean ent: 0.47\n","Epoch: 5600/20000 | Reward: -0.23 | Mean ent: 0.39\n","Epoch: 5700/20000 | Reward: 0.01 | Mean ent: 0.37\n","Epoch: 5800/20000 | Reward: -0.52 | Mean ent: 0.41\n","Epoch: 5900/20000 | Reward: -0.47 | Mean ent: 0.43\n","Epoch: 6000/20000 | Reward: -0.25 | Mean ent: 0.48\n","Epoch: 6100/20000 | Reward: -0.62 | Mean ent: 0.42\n","Epoch: 6200/20000 | Reward: -0.71 | Mean ent: 0.31\n","Epoch: 6300/20000 | Reward: 0.01 | Mean ent: 0.46\n","Epoch: 6400/20000 | Reward: 0.48 | Mean ent: 0.46\n","Epoch: 6500/20000 | Reward: -0.11 | Mean ent: 0.49\n","Epoch: 6600/20000 | Reward: -0.67 | Mean ent: 0.46\n","Epoch: 6700/20000 | Reward: 0.27 | Mean ent: 0.44\n","Epoch: 6800/20000 | Reward: 0.08 | Mean ent: 0.46\n","Epoch: 6900/20000 | Reward: 0.25 | Mean ent: 0.49\n","Epoch: 7000/20000 | Reward: 0.86 | Mean ent: 0.55\n","Epoch: 7100/20000 | Reward: 0.76 | Mean ent: 0.62\n","Epoch: 7200/20000 | Reward: 0.82 | Mean ent: 0.59\n","Epoch: 7300/20000 | Reward: 0.25 | Mean ent: 0.52\n","Epoch: 7400/20000 | Reward: 0.28 | Mean ent: 0.55\n","Epoch: 7500/20000 | Reward: 0.30 | Mean ent: 0.67\n","Epoch: 7600/20000 | Reward: 0.12 | Mean ent: 0.60\n","Epoch: 7700/20000 | Reward: 0.83 | Mean ent: 0.64\n","Epoch: 7800/20000 | Reward: -0.05 | Mean ent: 0.64\n","Epoch: 7900/20000 | Reward: 0.59 | Mean ent: 0.68\n","Epoch: 8000/20000 | Reward: 0.70 | Mean ent: 0.71\n","Epoch: 8100/20000 | Reward: 0.65 | Mean ent: 0.72\n","Epoch: 8200/20000 | Reward: 0.36 | Mean ent: 0.75\n","Epoch: 8300/20000 | Reward: 0.72 | Mean ent: 0.71\n","Epoch: 8400/20000 | Reward: 0.65 | Mean ent: 0.81\n","Epoch: 8500/20000 | Reward: 0.69 | Mean ent: 0.76\n","Epoch: 8600/20000 | Reward: -0.15 | Mean ent: 0.57\n","Epoch: 8700/20000 | Reward: -0.08 | Mean ent: 0.76\n","Epoch: 8800/20000 | Reward: 0.16 | Mean ent: 0.83\n","Epoch: 8900/20000 | Reward: -0.10 | Mean ent: 0.77\n","Epoch: 9000/20000 | Reward: 0.35 | Mean ent: 0.81\n","Epoch: 9100/20000 | Reward: 1.32 | Mean ent: 0.85\n","Epoch: 9200/20000 | Reward: 1.30 | Mean ent: 0.90\n","Epoch: 9300/20000 | Reward: 0.56 | Mean ent: 0.93\n","Epoch: 9400/20000 | Reward: 0.07 | Mean ent: 0.97\n","Epoch: 9500/20000 | Reward: 0.86 | Mean ent: 0.90\n","Epoch: 9600/20000 | Reward: 1.44 | Mean ent: 0.96\n","Epoch: 9700/20000 | Reward: 1.44 | Mean ent: 0.97\n","Epoch: 9800/20000 | Reward: 0.79 | Mean ent: 0.90\n","Epoch: 9900/20000 | Reward: 0.33 | Mean ent: 1.03\n","Epoch: 10000/20000 | Reward: -0.03 | Mean ent: 1.02\n","Epoch: 10100/20000 | Reward: 0.94 | Mean ent: 0.99\n","Epoch: 10200/20000 | Reward: 0.77 | Mean ent: 0.92\n","Epoch: 10300/20000 | Reward: 0.52 | Mean ent: 0.89\n","Epoch: 10400/20000 | Reward: 1.05 | Mean ent: 0.79\n","Epoch: 10500/20000 | Reward: 0.64 | Mean ent: 0.58\n","Epoch: 10600/20000 | Reward: -0.32 | Mean ent: 0.19\n","Epoch: 10700/20000 | Reward: -0.19 | Mean ent: 0.38\n","Epoch: 10800/20000 | Reward: -0.32 | Mean ent: 0.47\n","Epoch: 10900/20000 | Reward: -0.17 | Mean ent: 0.40\n","Epoch: 11000/20000 | Reward: 0.02 | Mean ent: 0.42\n","Epoch: 11100/20000 | Reward: -0.12 | Mean ent: 0.47\n","Epoch: 11200/20000 | Reward: 1.20 | Mean ent: 0.85\n","Epoch: 11300/20000 | Reward: -0.09 | Mean ent: 0.78\n","Epoch: 11400/20000 | Reward: 0.10 | Mean ent: 0.79\n","Epoch: 11500/20000 | Reward: 0.83 | Mean ent: 0.83\n","Epoch: 11600/20000 | Reward: 0.28 | Mean ent: 0.89\n","Epoch: 11700/20000 | Reward: 0.51 | Mean ent: 0.89\n","Epoch: 11800/20000 | Reward: 1.08 | Mean ent: 0.87\n","Epoch: 11900/20000 | Reward: 0.13 | Mean ent: 0.83\n","Epoch: 12000/20000 | Reward: -0.13 | Mean ent: 0.75\n","Epoch: 12100/20000 | Reward: -0.23 | Mean ent: 0.79\n","Epoch: 12200/20000 | Reward: -0.48 | Mean ent: 0.78\n","Epoch: 12300/20000 | Reward: -0.09 | Mean ent: 0.84\n","Epoch: 12400/20000 | Reward: -0.39 | Mean ent: 0.83\n","Epoch: 12500/20000 | Reward: 1.09 | Mean ent: 0.88\n","Epoch: 12600/20000 | Reward: 0.71 | Mean ent: 0.80\n","Epoch: 12700/20000 | Reward: 0.29 | Mean ent: 0.84\n","Epoch: 12800/20000 | Reward: -0.38 | Mean ent: 0.82\n","Epoch: 12900/20000 | Reward: 0.09 | Mean ent: 0.75\n","Epoch: 13000/20000 | Reward: 0.22 | Mean ent: 0.75\n","Epoch: 13100/20000 | Reward: -0.14 | Mean ent: 0.77\n","Epoch: 13200/20000 | Reward: -0.18 | Mean ent: 0.76\n","Epoch: 13300/20000 | Reward: -0.63 | Mean ent: 0.72\n","Epoch: 13400/20000 | Reward: -0.74 | Mean ent: 0.66\n","Epoch: 13500/20000 | Reward: -0.38 | Mean ent: 0.72\n","Epoch: 13600/20000 | Reward: -0.25 | Mean ent: 0.71\n","Epoch: 13700/20000 | Reward: -0.30 | Mean ent: 0.67\n","Epoch: 13800/20000 | Reward: -0.86 | Mean ent: 0.60\n","Epoch: 13900/20000 | Reward: -0.35 | Mean ent: 0.59\n","Epoch: 14000/20000 | Reward: -0.40 | Mean ent: 0.60\n","Epoch: 14100/20000 | Reward: 0.05 | Mean ent: 0.69\n","Epoch: 14200/20000 | Reward: -0.06 | Mean ent: 0.57\n","Epoch: 14300/20000 | Reward: -0.05 | Mean ent: 0.72\n","Epoch: 14400/20000 | Reward: 0.09 | Mean ent: 0.56\n","Epoch: 14500/20000 | Reward: 0.00 | Mean ent: 0.60\n","Epoch: 14600/20000 | Reward: -0.43 | Mean ent: 0.43\n","Epoch: 14700/20000 | Reward: -0.38 | Mean ent: 0.33\n","Epoch: 14800/20000 | Reward: -0.15 | Mean ent: 0.44\n","Epoch: 14900/20000 | Reward: -0.25 | Mean ent: 0.44\n","Epoch: 15000/20000 | Reward: -0.10 | Mean ent: 0.77\n","Epoch: 15100/20000 | Reward: -0.31 | Mean ent: 0.72\n","Epoch: 15200/20000 | Reward: 0.10 | Mean ent: 0.74\n","Epoch: 15300/20000 | Reward: -0.29 | Mean ent: 0.65\n","Epoch: 15400/20000 | Reward: -0.69 | Mean ent: 0.62\n","Epoch: 15500/20000 | Reward: -0.41 | Mean ent: 0.56\n","Epoch: 15600/20000 | Reward: -0.64 | Mean ent: 0.45\n","Epoch: 15700/20000 | Reward: -0.35 | Mean ent: 0.54\n","Epoch: 15800/20000 | Reward: -0.47 | Mean ent: 0.61\n","Epoch: 15900/20000 | Reward: -0.49 | Mean ent: 0.64\n","Epoch: 16000/20000 | Reward: -0.56 | Mean ent: 0.60\n","Epoch: 16100/20000 | Reward: -0.45 | Mean ent: 0.79\n","Epoch: 16200/20000 | Reward: -0.48 | Mean ent: 0.69\n","Epoch: 16300/20000 | Reward: -0.57 | Mean ent: 0.68\n","Epoch: 16400/20000 | Reward: -0.80 | Mean ent: 0.56\n","Epoch: 16500/20000 | Reward: -0.67 | Mean ent: 0.51\n","Epoch: 16600/20000 | Reward: -0.44 | Mean ent: 0.62\n","Epoch: 16700/20000 | Reward: -0.78 | Mean ent: 0.56\n","Epoch: 16800/20000 | Reward: -0.35 | Mean ent: 0.77\n","Epoch: 16900/20000 | Reward: -0.52 | Mean ent: 0.72\n","Epoch: 17000/20000 | Reward: -0.69 | Mean ent: 0.79\n","Epoch: 17100/20000 | Reward: -0.42 | Mean ent: 0.72\n","Epoch: 17200/20000 | Reward: -0.67 | Mean ent: 0.78\n","Epoch: 17300/20000 | Reward: -0.54 | Mean ent: 0.66\n","Epoch: 17400/20000 | Reward: -0.21 | Mean ent: 0.80\n","Epoch: 17500/20000 | Reward: -0.03 | Mean ent: 0.79\n","Epoch: 17600/20000 | Reward: -0.41 | Mean ent: 0.79\n","Epoch: 17700/20000 | Reward: -0.17 | Mean ent: 0.76\n","Epoch: 17800/20000 | Reward: -0.28 | Mean ent: 0.66\n","Epoch: 17900/20000 | Reward: 0.22 | Mean ent: 0.78\n","Epoch: 18000/20000 | Reward: -0.08 | Mean ent: 0.85\n","Epoch: 18100/20000 | Reward: 0.59 | Mean ent: 0.91\n","Epoch: 18200/20000 | Reward: -0.69 | Mean ent: 0.47\n","Epoch: 18300/20000 | Reward: -0.50 | Mean ent: 0.44\n","Epoch: 18400/20000 | Reward: -0.27 | Mean ent: 0.64\n","Epoch: 18500/20000 | Reward: -0.38 | Mean ent: 0.73\n","Epoch: 18600/20000 | Reward: -0.58 | Mean ent: 0.70\n","Epoch: 18700/20000 | Reward: -0.23 | Mean ent: 0.83\n","Epoch: 18800/20000 | Reward: 0.12 | Mean ent: 0.87\n","Epoch: 18900/20000 | Reward: -0.28 | Mean ent: 0.71\n","Epoch: 19000/20000 | Reward: -0.12 | Mean ent: 0.75\n","Epoch: 19100/20000 | Reward: -0.25 | Mean ent: 0.69\n","Epoch: 19200/20000 | Reward: -0.30 | Mean ent: 0.62\n","Epoch: 19300/20000 | Reward: -0.32 | Mean ent: 0.68\n","Epoch: 19400/20000 | Reward: -0.22 | Mean ent: 0.72\n","Epoch: 19500/20000 | Reward: -0.37 | Mean ent: 0.66\n","Epoch: 19600/20000 | Reward: -0.23 | Mean ent: 0.76\n","Epoch: 19700/20000 | Reward: 0.29 | Mean ent: 0.74\n","Epoch: 19800/20000 | Reward: 0.15 | Mean ent: 0.83\n","Epoch: 19900/20000 | Reward: -0.15 | Mean ent: 0.83\n","Epoch: 20000/20000 | Reward: 0.10 | Mean ent: 0.81\n"],"name":"stdout"}]},{"metadata":{"id":"KMUKkO3-WBa-","colab_type":"code","colab":{}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by AI algorithms.\n","\"\"\"\n","\n","from sys import exit # To close the window when the game is over\n","from os import environ # To center the game window the best possible\n","import random # Random numbers used for the food\n","import logging # Logging function for movements and errors\n","from itertools import tee # For the color gradient on snake\n","#!pip install pygame # Jupyter Notebook\n","#import pygame # This is the engine used in the game\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions and forbidden moves\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes:\n","        BLOCK_SIZE: The size in pixels of a block.\n","        HEAD_COLOR: Color of the head.\n","        BODY_COLOR: Color of the body.\n","        FOOD_COLOR: Color of the food.\n","        GAME_SPEED: Speed in ticks of the game. The higher the faster.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Initialize all global variables.\"\"\"\n","        self.BOARD_SIZE = 30\n","        self.BLOCK_SIZE = 20\n","        self.HEAD_COLOR = (0, 0, 0)\n","        self.BODY_COLOR = (0, 200, 0)\n","        self.FOOD_COLOR = (200, 0, 0)\n","        self.GAME_SPEED = 24\n","\n","        if self.BOARD_SIZE > 50:\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes:\n","        head: The head of the snake, located according to the board size.\n","        body: Starts with 3 parts and grows when food is eaten.\n","        orientation: Current orientation where head is pointing.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else (food), return without popping.\"\"\"\n","        if action == actions['IDLE']\\\n","            or (action, self.previous_action) in forbidden_moves:\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            return True\n","        else:\n","            self.body.pop()\n","\n","            return False\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body and re-\n","        turn.\"\"\"\n","        if self.head[0] > (var.BOARD_SIZE - 1) or self.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.head[1] > (var.BOARD_SIZE - 1) or self.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.head in self.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","\n","            return True\n","\n","        return False\n","\n","    def return_body(self):\n","        \"\"\"Return the whole body.\"\"\"\n","        return self.body\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes:\n","        pos: Current position of food.\n","        is_food_on_screen: Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","    def set_food_on_screen(self, bool_value):\n","        \"\"\"Set flag for existence (or not) of food.\"\"\"\n","        self.is_food_on_screen = bool_value\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes:\n","        window: pygame window to show the game.\n","        fps: Define Clock and ticks in which the game will be displayed.\n","        snake: The actual snake who is going to be played.\n","        food_generator: Generator of food which responds to the snake.\n","        food_pos: Position of the food on the board.\n","        game_over: Flag for game_over.\n","    \"\"\"\n","    def __init__(self, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score.\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        if self.relative_pos:\n","            self.nb_actions = 3\n","        else:\n","            self.nb_actions = 5\n","        self.reset()\n","\n","    def reset(self):\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.fps = pygame.time.Clock()\n","\n","    def start(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \" +\\\n","                                       str(3 - i) + \" second(s) ...\")\n","            pygame.time.wait(1000)\n","        logger.info('EVENT: GAME START')\n","\n","    def over(self):\n","        \"\"\"If collision with wall or body, end the game.\"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                            + str(self.snake.length - 3)\n","                            + \"  |  GAME OVER. Press any Q or ESC to quit ...\")\n","        logger.info('EVENT: GAME OVER')\n","\n","        while True:\n","            keys = pygame.key.get_pressed()\n","            pygame.event.pump()\n","\n","            if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","                logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","                break\n","\n","        pygame.quit()\n","        exit()\n","\n","    def is_won(self):\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        return self.food_generator.generate_food(self.snake.body)\n","\n","    def handle_input(self, previous_action):\n","        \"\"\"After getting current pressed keys, handle important cases.\"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over()\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            return actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            return actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            return actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            return actions['DOWN']\n","        else:\n","            return previous_action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\"\"\"\n","        body = self.snake.return_body()\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        for part in body:\n","            canvas[part[0], part[1]] = point_type['BODY']\n","\n","        canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","        if self.local_state:\n","            canvas = self.eval_local_safety(canvas, body)\n","\n","        canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action, player):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.set_food_on_screen(False)\n","\n","        if player == \"HUMAN\":\n","            if self.snake.check_collision():\n","                self.over()\n","        elif self.snake.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\"\"\"\n","        if self.game_over:\n","            return -1\n","        elif self.scored:\n","            return self.snake.length\n","\n","        return -0.005\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps.\n","\n","        If component is changed to 4, it does the same to RGBA colors.\"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","        pygame.display.update()\n","        self.fps.tick(var.GAME_SPEED)\n","\n","\n","def main():\n","    \"\"\"The main function where the game will be executed.\"\"\"\n","    # Setup basic configurations for logging in this module\n","    logging.basicConfig(format = '%(asctime)s %(module)s %(levelname)s: %(message)s',\n","                        datefmt = '%m/%d/%Y %I:%M:%S %p', level = logging.INFO)\n","    game = Game()\n","    game.create_window()\n","    game.start()\n","\n","    # The main loop, it pump key_presses and update the board every tick.\n","    previous_size = game.snake.length # Initial size of the snake\n","    current_size = 3 # Initial size of the snake\n","    color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                               previous_size)\n","\n","    # Main loop, where the snake keeps going each tick. It generate food, check\n","    # collisions and draw.\n","    while True:\n","        action = game.handle_input(game.snake.previous_action)\n","        game.play(action, \"HUMAN\")\n","        game.draw(color_list)\n","\n","        current_size = game.snake.length # Update the body size\n","\n","        if current_size > previous_size:\n","            color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                       game.snake.length)\n","\n","        previous_size = current_size\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","logger.setLevel(logging.ERROR)\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","class SumTree(object):\n","    \"\"\"\n","    This SumTree code is modified version of Morvan Zhou: \n","    https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5.2_Prioritized_Replay_DQN/RL_brain.py\n","    \"\"\"\n","    data_pointer = 0\n","    \n","    \"\"\"\n","    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n","    \"\"\"\n","    def __init__(self, capacity):\n","        self.capacity = capacity # Number of leaf nodes (final nodes) that contains experiences\n","        \n","        # Generate the tree with all nodes values = 0\n","        # To understand this calculation (2 * capacity - 1) look at the schema above\n","        # Remember we are in a binary node (each node has max 2 children) so 2x size of leaf (capacity) - 1 (root node)\n","        # Parent nodes = capacity - 1\n","        # Leaf nodes = capacity\n","        self.tree = np.zeros(2 * capacity - 1)\n","        \n","        \"\"\" tree:\n","            0\n","           / \\\n","          0   0\n","         / \\ / \\\n","        0  0 0  0  [Size: capacity] it's at this line that there is the priorities score (aka pi)\n","        \"\"\"\n","        \n","        # Contains the experiences (so the size of data is capacity)\n","        self.data = np.zeros(capacity, dtype=object)\n","    \n","    \n","    \"\"\"\n","    Here we add our priority score in the sumtree leaf and add the experience in data\n","    \"\"\"\n","    def add(self, priority, data):\n","        # Look at what index we want to put the experience\n","        tree_index = self.data_pointer + self.capacity - 1\n","        \n","        \"\"\" tree:\n","            0\n","           / \\\n","          0   0\n","         / \\ / \\\n","tree_index  0 0  0  We fill the leaves from left to right\n","        \"\"\"\n","        \n","        # Update data frame\n","        self.data[self.data_pointer] = data\n","        \n","        # Update the leaf\n","        self.update (tree_index, priority)\n","        \n","        # Add 1 to data_pointer\n","        self.data_pointer += 1\n","        \n","        if self.data_pointer >= self.capacity:  # If we're above the capacity, you go back to first index (we overwrite)\n","            self.data_pointer = 0\n","            \n","    \n","    \"\"\"\n","    Update the leaf priority score and propagate the change through tree\n","    \"\"\"\n","    def update(self, tree_index, priority):\n","        # Change = new priority score - former priority score\n","        change = priority - self.tree[tree_index]\n","        self.tree[tree_index] = priority\n","        \n","        # then propagate the change through tree\n","        while tree_index != 0:    # this method is faster than the recursive loop in the reference code\n","            \n","            \"\"\"\n","            Here we want to access the line above\n","            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n","            \n","                0\n","               / \\\n","              1   2\n","             / \\ / \\\n","            3  4 5  [6] \n","            \n","            If we are in leaf at index 6, we updated the priority score\n","            We need then to update index 2 node\n","            So tree_index = (tree_index - 1) // 2\n","            tree_index = (6-1)//2\n","            tree_index = 2 (because // round the result)\n","            \"\"\"\n","            tree_index = (tree_index - 1) // 2\n","            self.tree[tree_index] += change\n","    \n","    \n","    \"\"\"\n","    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n","    \"\"\"\n","    def get_leaf(self, v):\n","        \"\"\"\n","        Tree structure and array storage:\n","        Tree index:\n","             0         -> storing priority sum\n","            / \\\n","          1     2\n","         / \\   / \\\n","        3   4 5   6    -> storing priority for experiences\n","        Array type for storing:\n","        [0,1,2,3,4,5,6]\n","        \"\"\"\n","        parent_index = 0\n","        \n","        while True: # the while loop is faster than the method in the reference code\n","            left_child_index = 2 * parent_index + 1\n","            right_child_index = left_child_index + 1\n","            \n","            # If we reach bottom, end the search\n","            if left_child_index >= len(self.tree):\n","                leaf_index = parent_index\n","                break\n","            \n","            else: # downward search, always search for a higher priority node\n","                \n","                if v <= self.tree[left_child_index]:\n","                    parent_index = left_child_index\n","                    \n","                else:\n","                    v -= self.tree[left_child_index]\n","                    parent_index = right_child_index\n","            \n","        data_index = leaf_index - self.capacity + 1\n","\n","        return leaf_index, self.tree[leaf_index], self.data[data_index]\n","    \n","    @property\n","    def total_priority(self):\n","        return self.tree[0] # Returns the root node\n","\n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value)\n","and PER (Prioritized Experience Replay, using Sum Trees). You can read more\n","about these on https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682\n","\n","Possible usage:\n","    * Simple DQN;\n","    * DDQN;\n","    * DDDQN;\n","    * DDDQN + PER;\n","    * a combination of any of the above.\n","\n","Arguments:\n","    --load FILE.h5: load a previously trained model in '.h5' format.\n","    --board_size INT: assign the size of the board, default = 10\n","    --nb_frames INT: assign the number of frames per stack, default = 4.\n","    --nb_actions INT: assign the number of actions possible, default = 5.\n","    --update_freq INT: assign how often, in epochs, to update the target,\n","      default = 500.\n","    --visual: select wheter or not to draw the game in pygame.\n","    --double: use a target network with double DQN logic.\n","    --dueling: use dueling network logic, Q(s,a) = A + V.\n","    --per: use Prioritized Experience Replay (based on Sum Trees).\n","    --local_state: Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from os import path, environ, sys\n","import random\n","\n","import inspect # Making relative imports from parallel folders possible\n","currentdir = path.dirname(path.abspath(inspect.getfile(inspect.currentframe())))\n","parentdir = path.dirname(currentdir)\n","sys.path.insert(0, parentdir)\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import load_model, Sequential\n","from keras.layers import *\n","from keras import backend as K\n","K.set_image_dim_ordering('th')\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 100, per = False, alpha = 0.6,\n","                 epsilon = 0.001, beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.per = per\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","        if self.per:\n","            self.per_epsilon = epsilon\n","            self.per_alpha = alpha\n","            self.per_beta = beta\n","            self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        if self.per:\n","            return self.exp\n","        else:\n","            return len(self.memory)\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.per_epsilon) ** self.per_alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        if self.per: # If using PER, insert in the max_priority.\n","            max_priority = self.memory.max_leaf()\n","\n","            if max_priority == 0:\n","                max_priority = self.get_priority(0)\n","\n","            self.memory.insert(experience, max_priority)\n","            self.exp += 1\n","        else: # Else, just append the experience to the list.\n","            self.memory.append(experience)\n","\n","            if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","                self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        if self.per:\n","            batch = [None] * batch_size\n","            IS_weights = np.zeros((batch_size, ))\n","            tree_indices = [0] * batch_size\n","\n","            memory_sum = self.memory.sum()\n","            len_seg = memory_sum / batch_size\n","            min_prob = self.memory.min_leaf() / memory_sum\n","\n","            for i in range(batch_size):\n","                val = uniform(len_seg * i, len_seg * (i + 1))\n","                tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","                prob = priority / self.memory.sum()\n","                IS_weights[i] = np.power(prob / min_prob, -self.per_beta)\n","\n","            return np.array(batch), IS_weights, tree_indices\n","\n","        else:\n","            IS_weights = np.ones((batch_size, ))\n","            batch = random.sample(self.memory, batch_size)\n","            return np.array(batch), IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        r = r.repeat(nb_actions).reshape((batch_size, nb_actions))\n","        game_over = game_over.repeat(nb_actions)\\\n","                             .reshape((batch_size, nb_actions))\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","            for i in range(batch_size):\n","                Qsa[i] = Y_target[i][actions[i]]\n","            Qsa = np.array(Qsa).repeat(nb_actions).reshape((batch_size, nb_actions))\n","\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1).repeat(nb_actions)\\\n","                                                .reshape((batch_size, nb_actions))\n","\n","        # The targets here already take into account\n","        delta = np.zeros((batch_size, nb_actions))\n","        a = np.cast['int'](a)\n","        delta[np.arange(batch_size), a] = 1\n","        targets = ((1 - delta) * Y[:batch_size]\n","                  + delta * (r + gamma * (1 - game_over) * Qsa))\n","\n","        if self.per: # Update the Sum Tree with the absolute error.\n","            errors = np.abs((targets - Y[:batch_size]).max(axis = 1)).clip(max = 1.)\n","            self.update(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.per:\n","            if self.memory_size <= 0:\n","                self.memory_size = 150000\n","\n","            self.memory = SumTree(self.memory_size)\n","            self.exp = 0\n","        else:\n","            self.memory = []\n","\n","\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes:\n","    memory: memory used in the model. Input memory or ExperienceReplay.\n","    model: the input model, Conv2D in Keras.\n","    target: the target model, used to calculade the fixed Q-targets.\n","    nb_frames: ammount of frames for each sars.\n","    frames: the frames in each sars.\n","    per: flag for PER usage.\n","    \"\"\"\n","    def __init__(self, model, target, memory = None, memory_size = 150000,\n","                 nb_frames = 4, board_size = 10, per = False):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if memory:\n","            self.memory = memory\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size, per = per)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.frames = None\n","        self.target_updates = 0\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\"\"\"\n","        if game.game_over:\n","            frame = np.zeros((self.board_size, self.board_size))\n","        else:\n","            frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        return np.expand_dims(self.frames, 0)\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target_updates += 1\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, history_loss,\n","                      history_step, history_reward, policy, value, win_count,\n","                      verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    sum(history_size[-10:]) / 10,\n","                                    max(history_size[-10:]),\n","                                    sum(history_step[-10:]) / 10,\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}' # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            # Print training performance\n","            text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                          + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                          + 'Mean loss - 100 episodes: {:.4f}')\n","            print(text_perf.format(history_loss[-1],\n","                                   history_loss[-1] / history_step[-1],\n","                                   sum(history_loss[-100:]) / 100))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   sum(history_step[-100:]) / 100))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\"\"\"\n","        loss = 0.\n","\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, update_target_freq = 500, optim_rounds = 1,\n","              policy = \"EpsGreedyQPolicy\", verbose = 1, n_steps = None):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","\n","        history_size = []\n","        history_step = []\n","        history_loss = []\n","        history_reward = []\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        if policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","\n","                    print('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d} | '\n","                          + 'Loss: {:.4f}'.format(turn, epoch + 1, nb_epoch, loss))\n","            else:\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    if n_steps is not None:\n","                        n_step_buffer = []\n","                    game.reset()\n","                    self.clear_frames()\n","\n","                    S = self.get_game_data(game)\n","\n","                    while not game.game_over:\n","                        game.food_pos = game.generate_food()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","\n","                        game.play(action, \"ROBOT\")\n","\n","                        r = game.get_reward()\n","                        total_reward += r\n","                        if n_steps is not None:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience) # Add to the memory\n","                        S = S_prime # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe: # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1 # Counter for metric purposes\n","\n","                    if self.per: # Advance beta, used in PER\n","                        self.memory.per_beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None: # Update the target model\n","                        if epoch % update_target_freq == 0:\n","                            self.update_target_model()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch, nb_epoch, history_size,\n","                                           history_loss, history_step,\n","                                           history_reward, policy, value,\n","                                           win_count, verbose)\n","\n","    def play(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","        result_size = []\n","        result_step = []\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset()\n","            self.clear_frames()\n","            S = self.get_game_data(game)\n","\n","            if visual:\n","                game.create_window()\n","                # The main loop, it pump key_presses and update every tick.\n","                environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","                previous_size = game.snake.length # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","\n","            while not game.game_over:\n","                action, value = q_policy.select_action(self.model, S, epoch, nb_actions)\n","                game.play(action, \"ROBOT\")\n","                current_size = game.snake.length # Update the body size\n","\n","                if visual:\n","                    game.draw(color_list)\n","\n","                    if current_size > previous_size:\n","                        color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   game.snake.length)\n","\n","                        previous_size = current_size\n","\n","                S = self.get_game_data(game)\n","\n","                if game.game_over:\n","                    result_size.append(current_size)\n","                    result_step.append(game.step)\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(result_size), np.max(result_size),\n","                      np.min(result_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(result_step), np.max(result_step),\\\n","                      np.min(result_step)))\n","        \n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","#!/usr/bin/env python\n","\n","\"\"\"clipped_error: L1 for errors < clip_value else L2 error.\n","\n","Functions:\n","    huber_loss: Return L1 error if absolute error is less than clip_value, else\n","                return L2 error.\n","    clipped_error: Call huber_loss with default clip_value to 1.0.\n","\"\"\"\n","\n","import numpy as np\n","from keras import backend as K\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\tif K.backend() == 'tensorflow':\n","\t\tif hasattr(tf, 'select'):\n","\t\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\t\telse:\n","\t\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\telif K.backend() == 'theano':\n","\t\tfrom theano import tensor as T\n","\t\treturn T.switch(condition, squared_loss, linear_loss)\n","\telse:\n","\t\traise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","#def CNN1(optimizer, loss, stack, input_size, output_size):\n"," #   model = Sequential()\n","  #  model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (stack,\n","   #                                                                  input_size,\n","    #                                                                 input_size)))\n","#    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n"," #   model.add(Conv2D(128, (3, 3), activation = 'relu'))\n","  #  model.add(Conv2D(256, (3, 3), activation = 'relu'))\n","   # model.add(Flatten())\n","    #model.add(Dense(1024, activation = 'relu'))\n","    #model.add(Dense(output_size))\n","    #model.compile(optimizer = optimizer, loss = loss)\n","\n","    #return model\n","    \n","def CNN4(optimizer, loss, stack, input_size, output_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    model = Sequential()\n","    model.add(Conv2D(32, (4, 4), activation = 'relu', input_shape = (stack,\n","                                                                    input_size,\n","                                                                    input_size)))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Flatten())\n","    model.add(GaussianNoise(stddev = 0.4))\n","    model.add(Dense(3136, activation = 'relu'))\n","    model.add(GaussianNoise(stddev = 0.4))\n","    model.add(Dense(output_size))\n","    model.compile(optimizer = optimizer, loss = loss)\n","\n","    return model\n","  \n","board_size = 10\n","nb_frames = 4\n","  \n","game = Game(board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","model = CNN4(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = -1,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","agent.train(game, batch_size = 64, nb_epoch = 10000, gamma = 0.95, update_target_freq = 500, policy = \"BoltzmannQPolicy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1i2a1XBFWUgH","colab_type":"code","colab":{}},"cell_type":"code","source":["model.save('keras.h5')\n","\n","!zip -r model-epsgreedy-n-steps.zip keras.h5 \n","from google.colab import files\n","files.download('model-epsgreedy-noise.zip')\n","\n","model = load_model('keras.h5', custom_objects={'clipped_error': clipped_error})\n","\n","board_size = 10\n","nb_frames = 4\n","nb_actions = 5\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = 1500000,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","agent.play(game, visual = False, nb_epoch = 1000)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JVPX8PjhrHuj","colab_type":"text"},"cell_type":"markdown","source":["# Testing what changed on the new DQN\n","\n","The same source a the banchmark, only difference is the Snake and Game classes."]},{"metadata":{"id":"Fx2jTsP_rHPQ","colab_type":"code","outputId":"3d4d6744-b5c0-45aa-feb0-f28921a9b71a","executionInfo":{"status":"ok","timestamp":1541743420371,"user_tz":120,"elapsed":16802452,"user":{"displayName":"Victor Neves","photoUrl":"https://lh5.googleusercontent.com/-zfn1KWpcK60/AAAAAAAAAAI/AAAAAAAAZas/oOSqICtc8i0/s64/photo.jpg","userId":"17687780415293205160"}},"colab":{"base_uri":"https://localhost:8080/","height":17034}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by Human and AI.\n","\"\"\"\n","\n","import sys  # To close the window when the game is over\n","from array import array  # Efficient numeric arrays\n","from os import environ, path  # To center the game window the best possible\n","import random  # Random numbers used for the food\n","import logging  # Logging function for movements and errors\n","from itertools import tee  # For the color gradient on snake\n","import numpy as np\n","import os\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4,\n","           'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Possible rewards in the game\n","rewards = {'MOVE': -0.005, 'GAME_OVER': -1, 'SCORED': 1}\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","# Speed levels possible to human players, MEGA HARDCORE starts with MEDIUM and\n","# increases with snake size\n","levels = [\" EASY \", \" MEDIUM \", \" HARD \", \" MEGA HARDCORE \"]\n","speeds = {'EASY': 80, 'MEDIUM': 60, 'HARD': 40}\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes\n","    ----------\n","    BOARD_SIZE: int, optional, default = 30\n","        The size of the board.\n","    BLOCK_SIZE: int, optional, default = 20\n","        The size in pixels of a block.\n","    HEAD_COLOR: tuple of 3 * int, optional, default = (42, 42, 42)\n","        Color of the head. Start of the body color gradient.\n","    TAIL_COLOR: tuple of 3 * int, optional, default = (152, 152, 152)\n","        Color of the tail. End of the body color gradient.\n","    FOOD_COLOR: tuple of 3 * int, optional, default = (200, 0, 0)\n","        Color of the food.\n","    GAME_SPEED: int, optional, default = 10\n","        Speed in ticks of the game. The higher the faster.\n","    BENCHMARK: int, optional, default = 10\n","        Ammount of matches to BENCHMARK and possibly go to leaderboards.\n","    \"\"\"\n","    def __init__(self, BOARD_SIZE = 30, BLOCK_SIZE = 20,\n","                 HEAD_COLOR = (42, 42, 42), TAIL_COLOR = (152, 152, 152),\n","                 FOOD_COLOR = (200, 0, 0), GAME_SPEED = 80, GAME_FPS = 100,\n","                 BENCHMARK = 10):\n","        \"\"\"Initialize all global variables. Can be updated with argument_handler.\n","        \"\"\"\n","        self.BOARD_SIZE = BOARD_SIZE\n","        self.BLOCK_SIZE = BLOCK_SIZE\n","        self.HEAD_COLOR = HEAD_COLOR\n","        self.TAIL_COLOR = TAIL_COLOR\n","        self.FOOD_COLOR = FOOD_COLOR\n","        self.GAME_SPEED = GAME_SPEED\n","        self.GAME_FPS = GAME_FPS\n","        self.BENCHMARK = BENCHMARK\n","\n","        if self.BOARD_SIZE > 50: # Warn the user about performance\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    \"\"\"Block of text class, used by pygame. Can be used to both text and menu.\n","\n","    Attributes:\n","    ----------\n","    text: string\n","        The text to be displayed.\n","    pos: tuple of 2 * int\n","        Color of the tail. End of the body color gradient.\n","    screen: pygame window object\n","        The screen where the text is drawn.\n","    scale: int, optional, default = 1 / 12\n","        Adaptive scale to resize if the board size changes.\n","    type: string, optional, default = \"text\"\n","        Assert whether the BlockText is a text or menu option.\n","    \"\"\"\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        \"\"\"Initialize, set position of the rectangle and render the text block.\"\"\"\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        \"\"\"Set what to render and blit on the pygame screen.\"\"\"\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        \"\"\"Set what to render (font, colors, sizes)\"\"\"\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        \"\"\"Get color to render for text and menu (hovered or not).\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the text block.\n","        \"\"\"\n","        color = pygame.Color(42, 42, 42)\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                pass\n","            else:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def get_background(self):\n","        \"\"\"Get background color to render for text (hovered or not) and menu.\n","\n","        Return\n","        ----------\n","        color: tuple of 3 * int\n","            The color that will be rendered for the background of the text block.\n","        \"\"\"\n","        color = None\n","\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                color = pygame.Color(152, 152, 152)\n","\n","        return color\n","\n","    def set_rect(self):\n","        \"\"\"Set the rectangle and it's position to draw on the screen.\"\"\"\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes\n","    ----------\n","    head: list of 2 * int, default = [BOARD_SIZE / 4, BOARD_SIZE / 4]\n","        The head of the snake, located according to the board size.\n","    body: list of lists of 2 * int\n","        Starts with 3 parts and grows when food is eaten.\n","    previous_action: int, default = 1\n","        Last action which the snake took.\n","    length: int, default = 3\n","        Variable length of the snake, can increase when food is eaten.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def is_movement_invalid(self, action):\n","        valid = False\n","\n","        if (action, self.previous_action) in forbidden_moves:\n","            valid = True\n","\n","        return valid\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else, return without popping.\n","\n","        Return\n","        ----------\n","        ate_food: boolean\n","            Flag which represents whether the snake ate or not food.\n","        \"\"\"\n","        ate_food = False\n","\n","        if action == actions['IDLE'] or self.is_movement_invalid(action):\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            ate_food = True\n","        else:\n","            self.body.pop()\n","\n","        return ate_food\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes\n","    ----------\n","    pos:\n","        Current position of food.\n","    is_food_on_screen:\n","        Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\n","\n","        Return\n","        ----------\n","        pos: tuple of 2 * int\n","            Position of the food that was generated. It can't be in the body.\n","        \"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes\n","    ----------\n","    window: pygame display\n","        Pygame window to show the game.\n","    fps: pygame time clock\n","        Define Clock and ticks in which the game will be displayed.\n","    snake: object\n","        The actual snake who is going to be played.\n","    food_generator: object\n","        Generator of food which responds to the snake.\n","    food_pos: tuple of 2 * int\n","        Position of the food on the board.\n","    game_over: boolean\n","        Flag for game_over.\n","    player: string\n","        Define if human or robots are playing the game.\n","    board_size: int, optional, default = 30\n","        The size of the board.\n","    local_state: boolean, optional, default = False\n","        Whether to use or not game expertise (used mostly by robots players).\n","    relative_pos: boolean, optional, default = False\n","        Whether to use or not relative position of the snake head. Instead of\n","        actions, use relative_actions.\n","    screen_rect: tuple of 2 * int\n","        The screen rectangle, used to draw relatively positioned blocks.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score. Change nb_actions if relative_pos\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        \"\"\"Reset the game environment.\"\"\"\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        \"\"\"Create a pygame display with BOARD_SIZE * BLOCK_SIZE dimension.\"\"\"\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.screen_rect = self.window.get_rect()\n","        self.fps = pygame.time.Clock()\n","\n","    def menu(self):\n","        \"\"\"Main menu of the game.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images/snake_logo.png\"))\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE, int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                                  4 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                                  6 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                     8 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                option.draw()\n","\n","                if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                    option.hovered = True\n","\n","                    if option == menu_options[0]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['PLAY']\n","                    elif option == menu_options[1]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['BENCHMARK']\n","                    elif option == menu_options[2]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['LEADERBOARDS']\n","                    elif option == menu_options[3]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                selected_option = options['QUIT']\n","                else:\n","                    option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            self.window.blit(img, img_rect.bottomleft)\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                                 4 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                                 12 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                var.GAME_SPEED, mega_hardcore = self.select_speed()\n","                self.reset_game()\n","                self.start_match()\n","                score = self.single_player(mega_hardcore)\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                var.GAME_SPEED, mega_hardcore = self.select_speed()\n","                score = array('i')\n","\n","                for i in range(var.BENCHMARK):\n","                    self.reset_game()\n","                    self.start_match()\n","                    score.append(self.single_player(mega_hardcore))\n","\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game and open options.\n","\n","        Return\n","        ----------\n","        selected_option: int\n","            The selected option in the main loop.\n","        \"\"\"\n","        menu_options = [None] * 5\n","        menu_options[0] = TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                     4 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[1] = TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                     6 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[3] = TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                               10 * self.screen_rect.centery / 10),\n","                                               self.window, (1 / 15), \"menu\")\n","\n","        if isinstance(score, int):\n","            text_score = 'SCORE: ' + str(score)\n","        else:\n","            text_score = 'MEAN SCORE: ' + str(sum(score) / var.BENCHMARK)\n","            menu_options[2] = TextBlock(' ADD TO LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                                  8 * self.screen_rect.centery / 10),\n","                                                                  self.window, (1 / 15), \"menu\")\n","\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","        menu_options[4] = TextBlock(text_score, (self.screen_rect.centerx,\n","                                                 15 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\")\n","        selected = False\n","        selected_option = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['PLAY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['MENU']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    selected_option = options['ADD_LEADERBOARDS']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    pygame.quit()\n","                                    sys.exit()\n","                    else:\n","                        option.hovered = False\n","\n","            if selected_option is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return selected_option\n","\n","    def select_speed(self):\n","        \"\"\"Speed menu, right before calling start_match.\n","\n","        Return\n","        ----------\n","        speed: int\n","            The selected speed in the main loop.\n","        \"\"\"\n","        menu_options = [TextBlock(levels[0], (self.screen_rect.centerx,\n","                                              4 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[1], (self.screen_rect.centerx,\n","                                              8 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[2], (self.screen_rect.centerx,\n","                                              12 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\"),\n","                        TextBlock(levels[3], (self.screen_rect.centerx,\n","                                              16 * self.screen_rect.centery / 10),\n","                                              self.window, (1 / 10), \"menu\")]\n","        mega_hardcore = False\n","        selected = False\n","        speed = None\n","\n","        while not selected:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = speeds['EASY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = speeds['MEDIUM']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = speeds['HARD']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    speed = speeds['MEDIUM']\n","                                    mega_hardcore = True\n","\n","                    else:\n","                        option.hovered = False\n","\n","            if speed is not None:\n","                selected = True\n","\n","            pygame.display.update()\n","\n","        return speed, mega_hardcore\n","\n","    def single_player(self, mega_hardcore = False):\n","        \"\"\"Game loop for single_player (HUMANS).\n","\n","        Return\n","        ----------\n","        score: int\n","            The final score for the match (discounted of initial length).\n","        \"\"\"\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where snakes moves after elapsed time is bigger than the\n","        # move_wait time. The last_key pressed is recorded to make the game more\n","        # smooth for human players.\n","        elapsed = 0\n","        last_key = self.snake.previous_action\n","        move_wait = var.GAME_SPEED\n","\n","        while not self.game_over:\n","            elapsed += self.fps.get_time()  # Get elapsed time since last call.\n","\n","            if mega_hardcore:  # Progressive speed increments, the hardest.\n","                move_wait = var.GAME_SPEED - (2 * (self.snake.length - 3))\n","\n","            key_input = self.handle_input()  # Receive inputs with tick.\n","            invalid_key = self.snake.is_movement_invalid(key_input)\n","\n","            if key_input is not None and not invalid_key:\n","                last_key = key_input\n","\n","            if elapsed >= move_wait:  # Move and redraw\n","                elapsed = 0\n","                self.game_over = self.play(last_key)\n","                current_size = self.snake.length  # Update the body size\n","\n","                if current_size > previous_size:\n","                    color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   current_size)\n","\n","                    previous_size = current_size\n","\n","                self.draw(color_list)\n","\n","            pygame.display.update()\n","            self.fps.tick(100)  # Limit FPS to 100\n","\n","        score = current_size - 3  # After the game is over, record score\n","\n","        return score\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body.\n","\n","        Return\n","        ----------\n","        collided: boolean\n","            Whether the snake collided or not.\n","        \"\"\"\n","        collided = False\n","\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","            collided = True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","            collided = True\n","\n","        return collided\n","\n","    def is_won(self):\n","        \"\"\"Verify if the score is greater than 0.\n","\n","        Return\n","        ----------\n","        won: boolean\n","            Whether the score is greater than 0.\n","        \"\"\"\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        \"\"\"Generate new food if needed.\n","\n","        Return\n","        ----------\n","        food_pos: tuple of 2 * int\n","            Current position of the food.\n","        \"\"\"\n","        food_pos = self.food_generator.generate_food(self.snake.body)\n","\n","        return food_pos\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\n","\n","        Return\n","        ----------\n","        action: int\n","            Handle human input to assess the next action.\n","        \"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","        action = None\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            action = actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            action = actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            action = actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            action = actions['DOWN']\n","\n","        return action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            After using game expertise, change canvas values to DANGEROUS if true.\n","        \"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\n","\n","        Return\n","        ----------\n","        canvas: np.array of size BOARD_SIZE**2\n","            Return the current state of the game in a matrix.\n","        \"\"\"\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        if self.game_over:\n","            pass\n","        else:\n","            body = self.snake.body\n","\n","            for part in body:\n","                canvas[part[0], part[1]] = point_type['BODY']\n","\n","            canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","            if self.local_state:\n","                canvas = self.eval_local_safety(canvas, body)\n","\n","            canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        \"\"\"Translate relative actions to absolute.\n","\n","        Return\n","        ----------\n","        action: int\n","            Translated action from relative to absolute.\n","        \"\"\"\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.is_food_on_screen = False\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\n","\n","        Return\n","        ----------\n","        reward: float\n","            Current reward of the game.\n","        \"\"\"\n","        reward = rewards['MOVE']\n","\n","        if self.game_over:\n","            reward = rewards['GAME_OVER']\n","        elif self.scored:\n","            reward = self.snake.length\n","\n","        return reward\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps. If\n","        component is changed to 4, it does the same to RGBA colors.\n","\n","        Return\n","        ----------\n","        result: list of steps length of tuple of 3 * int (if RGBA, 4 * int)\n","            List of colors of calculated gradient from start to end.\n","        \"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","\n","def resource_path(relative_path):\n","    \"\"\"Function to return absolute paths. Used while creating .exe file.\"\"\"\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","import numpy as np\n","\n","from random import sample, uniform\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 100, per = False, alpha = 0.6,\n","                 epsilon = 0.001, beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.per = per\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","        if self.per:\n","            self.per_epsilon = epsilon\n","            self.per_alpha = alpha\n","            self.per_beta = beta\n","            self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        if self.per:\n","            return self.exp\n","        else:\n","            return len(self.memory)\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.per_epsilon) ** self.per_alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        if self.per: # If using PER, insert in the max_priority.\n","            max_priority = self.memory.max_leaf()\n","\n","            if max_priority == 0:\n","                max_priority = self.get_priority(0)\n","\n","            self.memory.insert(experience, max_priority)\n","            self.exp += 1\n","        else: # Else, just append the experience to the list.\n","            self.memory.append(experience)\n","\n","            if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","                self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        if self.per:\n","            batch = [None] * batch_size\n","            IS_weights = np.zeros((batch_size, ))\n","            tree_indices = [0] * batch_size\n","\n","            memory_sum = self.memory.sum()\n","            len_seg = memory_sum / batch_size\n","            min_prob = self.memory.min_leaf() / memory_sum\n","\n","            for i in range(batch_size):\n","                val = uniform(len_seg * i, len_seg * (i + 1))\n","                tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","                prob = priority / self.memory.sum()\n","                IS_weights[i] = np.power(prob / min_prob, -self.per_beta)\n","\n","            return np.array(batch), IS_weights, tree_indices\n","\n","        else:\n","            IS_weights = np.ones((batch_size, ))\n","            batch = sample(self.memory, batch_size)\n","            return np.array(batch), IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        r = r.repeat(nb_actions).reshape((batch_size, nb_actions))\n","        game_over = game_over.repeat(nb_actions)\\\n","                             .reshape((batch_size, nb_actions))\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","            for i in range(batch_size):\n","                Qsa[i] = Y_target[i][actions[i]]\n","            Qsa = np.array(Qsa).repeat(nb_actions).reshape((batch_size, nb_actions))\n","\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1).repeat(nb_actions)\\\n","                                                .reshape((batch_size, nb_actions))\n","\n","        # The targets here already take into account\n","        delta = np.zeros((batch_size, nb_actions))\n","        a = np.cast['int'](a)\n","        delta[np.arange(batch_size), a] = 1\n","        targets = ((1 - delta) * Y[:batch_size]\n","                  + delta * (r + gamma * (1 - game_over) * Qsa))\n","\n","        if self.per: # Update the Sum Tree with the absolute error.\n","            errors = np.abs((targets - Y[:batch_size]).max(axis = 1)).clip(max = 1.)\n","            self.update(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.per:\n","            if self.memory_size <= 0:\n","                self.memory_size = 150000\n","\n","            self.memory = SumTree(self.memory_size)\n","            self.exp = 0\n","        else:\n","            self.memory = []\n","\n","\n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms\n","----------\n","    * Simple Deep Q-network (DQN with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double Deep Q-network (Double DQN);\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling Deep Q-network (Dueling DQN);\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * Prioritized Experience Replay (PER);\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns.\n","        Paper: https://arxiv.org/pdf/1703.01327\n","\n","Arguments\n","----------\n","--load: 'file.h5'\n","    Load a previously trained model in '.h5' format.\n","--board_size: int, optional, default = 10\n","    Assign the size of the board.\n","--nb_frames: int, optional, default = 4\n","    Assign the number of frames per stack, default = 4.\n","--nb_actions: int, optional, default = 5\n","    Assign the number of actions possible.\n","--update_freq: int, optional, default = 0.001\n","    Whether to soft or hard update the target. Epochs or ammount of the update.\n","--visual: boolean, optional, default = False\n","    Select wheter or not to draw the game in pygame.\n","--double: boolean, optional, default = False\n","    Use a target network with double DQN logic.\n","--dueling: boolean, optional, default = False\n","    Whether to use dueling network logic, Q(s,a) = A + V.\n","--per: boolean, optional, default = False\n","    Use Prioritized Experience Replay (based on Sum Trees).\n","--local_state: boolean, optional, default = True\n","    Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from array import array\n","from os import path, environ, sys\n","import random\n","import inspect\n","\n","# Making relative imports from parallel folders possible\n","currentdir = path.dirname(path.abspath(inspect.getfile(inspect.currentframe())))\n","parentdir = path.dirname(currentdir)\n","sys.path.insert(0, parentdir)\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import load_model, Sequential\n","from keras.layers import *\n","from keras import backend as K\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","K.set_image_dim_ordering('th')  # Setting keras ordering\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes\n","    ----------\n","    memory: object\n","        Memory used in training. ExperienceReplay or PrioritizedExperienceReplay\n","    memory_size: int, optional, default = -1\n","        Capacity of the memory used.\n","    model: keras model\n","        The input model in Keras.\n","    target: keras model, optional, default = None\n","        The target model, used to calculade the fixed Q-targets.\n","    nb_frames: int, optional, default = 4\n","        Ammount of frames for each experience (sars).\n","    board_size: int, optional, default = 10\n","        Size of the board used.\n","    frames: list of experiences\n","        The buffer of frames, store sars experiences.\n","    per: boolean, optional, default = False\n","        Flag for PER usage.\n","    update_target_freq: int or float, default = 0.001\n","        Whether soft or hard updates occur. If < 1, soft updated target model.\n","    n_steps: int, optional, default = 1\n","        Size of the rewards buffer, to use Multi-step returns.\n","    \"\"\"\n","    def __init__(self, model, target = None, memory_size = -1, nb_frames = 4,\n","                 board_size = 10, per = False, update_target_freq = 0.001):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if per:\n","            self.memory = PrioritizedExperienceReplay(memory_size = memory_size)\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.update_target_freq = update_target_freq\n","        self.clear_frames()\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\n","\n","        Return\n","        ----------\n","        expanded_frames: list of experiences\n","            The buffer of frames, shape = (nb_frames, board_size, board_size)\n","        \"\"\"\n","        frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        expanded_frames = np.expand_dims(self.frames, 0)\n","\n","        return expanded_frames\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model_hard(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def transfer_weights(self):\n","        \"\"\"Transfer Weights from Model to Target at rate update_target_freq.\"\"\"\n","        model_weights = self.model.get_weights()\n","        target_weights = self.target.get_weights()\n","\n","        for i in range(len(W)):\n","            target_weights[i] = (self.update_target_freq * model_weights[i]\n","                                 + ((1 - self.update_target_frequency)\n","                                    * target_weights[i]))\n","\n","        self.target.set_weights(target_weights)\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, policy, value,\n","                      win_count, history_step, history_reward,\n","                      history_loss = None, verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    np.mean(history_size[-10:]),\n","                                    max(history_size[-10:]),\n","                                    np.mean(history_step[-10:]),\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}'  # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            if loss is not None:  # Print training performance\n","                text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                              + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                              + 'Mean loss - 100 episodes: {:.4f}')\n","                print(text_perf.format(history_loss[-1],\n","                                       history_loss[-1] / history_step[-1],\n","                                       np.mean(history_loss[-100:])))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   np.mean(history_step[-100:])))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\n","\n","        Return\n","        ----------\n","        loss: float\n","            Training loss of given batch.\n","        \"\"\"\n","        loss = 0.\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, optim_rounds = 1, policy = \"EpsGreedyQPolicy\",\n","              verbose = 1, n_steps = 1):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        if not hasattr(self, 'n_steps'):\n","            self.n_steps = n_steps  # Set attribute only once\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_loss = array('f')  # Holds all the losses\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        # Select exploration policy. EpsGreedyQPolicy runs faster, but takes\n","        # longer to converge. BoltzmannGumbelQPolicy is the slowest, but\n","        # converge really fast (0.1 * nb_epoch used in EpsGreedyQPolicy).\n","        # BoltzmannQPolicy is in the middle.\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        elif policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        # If optim_rounds is bigger than one, the model will keep optimizing\n","        # after the exploration, in turns of nb_epoch size.\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                              + '| Loss: {:.4f}')\n","                print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:  # Exploration and training\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    game.reset_game()\n","                    self.clear_frames()\n","                    S = self.get_game_data(game)\n","\n","                    if n_steps > 1:  # Create multi-step returns buffer.\n","                        n_step_buffer = array('f')\n","\n","                    while not game.game_over:  # Main loop, until game_over\n","                        game.food_pos = game.generate_food()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","                        game.play(action)\n","                        r = game.get_reward()\n","                        total_reward += r\n","\n","                        if n_steps > 1:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience)  # Add to the memory\n","                        S = S_prime  # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe:  # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1  # Counter of wins for metrics\n","\n","                    if self.per:  # Advance beta, used in PER\n","                        self.memory.beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None:  # Update the target model\n","                        if update_target_freq >= 1: # Hard updates\n","                            if epoch % self.update_target_freq == 0:\n","                                self.update_target_model_hard()\n","                        elif update_target_freq < 1.:  # Soft updates\n","                            self.transfer_weights()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch = epoch, nb_epoch = nb_epoch,\n","                                           history_size = history_size,\n","                                           history_loss = history_loss,\n","                                           history_step = history_step,\n","                                           history_reward = history_reward,\n","                                           policy = policy, value = value,\n","                                           win_count = win_count,\n","                                           verbose = verbose)\n","\n","    def play(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","\n","        history_size = array('i')  # Holds all the sizes\n","        history_step = array('f')  # Holds all the steps\n","        history_reward = array('f')  # Holds all the rewards\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","            S = self.get_game_data(game)\n","\n","            if visual:\n","                game.create_window()\n","                # The main loop, it pump key_presses and update every tick.\n","                environ['SDL_VIDEO_CENTERED'] = '1'  # Centering the window\n","                previous_size = game.snake.length  # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","\n","            while not game.game_over:\n","                action, value = q_policy.select_action(self.model, S, epoch, nb_actions)\n","                game.play(action)\n","                current_size = game.snake.length  # Update the body size\n","\n","                if visual:\n","                    game.draw(color_list)\n","\n","                    if current_size > previous_size:\n","                        color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   game.snake.length)\n","\n","                        previous_size = current_size\n","\n","                S = self.get_game_data(game)\n","\n","                if game.game_over:\n","                    history_size.append(current_size)\n","                    history_step.append(game.step)\n","                    history_reward.append(game.get_reward())\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(history_size), np.max(history_size),\n","                      np.min(history_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(history_step), np.max(history_step),\n","                      np.min(history_step)))\n","        print(\"Mean rewards: {} | Biggest reward: {} | Smallest reward: {}\"\\\n","              .format(np.mean(history_reward), np.max(history_reward),\n","                      np.min(history_reward)))\n","       \n","\n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","#!/usr/bin/env python\n","\n","\"\"\"clipped_error: L1 for errors < clip_value else L2 error.\n","\n","Functions:\n","    huber_loss: Return L1 error if absolute error is less than clip_value, else\n","                return L2 error.\n","    clipped_error: Call huber_loss with default clip_value to 1.0.\n","\"\"\"\n","\n","import numpy as np\n","from keras import backend as K\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\tif K.backend() == 'tensorflow':\n","\t\tif hasattr(tf, 'select'):\n","\t\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\t\telse:\n","\t\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\telif K.backend() == 'theano':\n","\t\tfrom theano import tensor as T\n","\t\treturn T.switch(condition, squared_loss, linear_loss)\n","\telse:\n","\t\traise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","#def CNN1(optimizer, loss, stack, input_size, output_size):\n"," #   model = Sequential()\n","  #  model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (stack,\n","   #                                                                  input_size,\n","    #                                                                 input_size)))\n","#    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n"," #   model.add(Conv2D(128, (3, 3), activation = 'relu'))\n","  #  model.add(Conv2D(256, (3, 3), activation = 'relu'))\n","   # model.add(Flatten())\n","    #model.add(Dense(1024, activation = 'relu'))\n","    #model.add(Dense(output_size))\n","    #model.compile(optimizer = optimizer, loss = loss)\n","\n","    #return model\n","    \n","def CNN4(optimizer, loss, stack, input_size, output_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    model = Sequential()\n","    model.add(Conv2D(32, (4, 4), activation = 'relu', input_shape = (stack,\n","                                                                    input_size,\n","                                                                    input_size)))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Flatten())\n","    model.add(Dense(3136, activation = 'relu'))\n","    model.add(Dense(output_size))\n","    model.compile(optimizer = optimizer, loss = loss)\n","\n","    return model\n","\n","board_size = 10\n","nb_frames = 4\n","  \n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","model = CNN4(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = -1,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False, update_target_freq = 0.001)\n","\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","agent.train(game, batch_size = 64, nb_epoch = 10000, gamma = 0.95, policy = \"EpsGreedyQPolicy\")        "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 010/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 14.9 | Wins: 2 | Win percentage: 20.0%\n","Epoch: 020/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 6.0 | Wins: 2 | Win percentage: 10.0%\n","Epoch: 030/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.8 | Wins: 3 | Win percentage: 10.0%\n","Epoch: 040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.5 | Wins: 4 | Win percentage: 10.0%\n","Epoch: 050/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.0 | Wins: 6 | Win percentage: 12.0%\n","Epoch: 060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 7 | Win percentage: 11.7%\n","Epoch: 070/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 10 | Win percentage: 14.3%\n","Epoch: 080/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.4 | Wins: 13 | Win percentage: 16.2%\n","Epoch: 090/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 14 | Win percentage: 15.6%\n","Epoch: 100/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.7 | Wins: 14 | Win percentage: 14.0%\n","Epoch: 110/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.1 | Wins: 14 | Win percentage: 12.7%\n","Epoch: 120/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.1 | Wins: 14 | Win percentage: 11.7%\n","Epoch: 130/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 16 | Win percentage: 12.3%\n","Epoch: 140/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 17 | Win percentage: 12.1%\n","Epoch: 150/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 6.0 | Wins: 18 | Win percentage: 12.0%\n","Epoch: 160/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 15.1 | Wins: 22 | Win percentage: 13.8%\n","Epoch: 170/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.5 | Wins: 23 | Win percentage: 13.5%\n","Epoch: 180/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.9 | Wins: 24 | Win percentage: 13.3%\n","Epoch: 190/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 10.4 | Wins: 26 | Win percentage: 13.7%\n","Epoch: 200/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 28 | Win percentage: 14.0%\n","Epoch: 210/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.0 | Wins: 29 | Win percentage: 13.8%\n","Epoch: 220/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.5 | Wins: 30 | Win percentage: 13.6%\n","Epoch: 230/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 8.1 | Wins: 32 | Win percentage: 13.9%\n","Epoch: 240/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 33 | Win percentage: 13.8%\n","Epoch: 250/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 34 | Win percentage: 13.6%\n","Epoch: 260/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.9 | Wins: 35 | Win percentage: 13.5%\n","Epoch: 270/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 7.8 | Wins: 36 | Win percentage: 13.3%\n","Epoch: 280/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 14.5 | Wins: 37 | Win percentage: 13.2%\n","Epoch: 290/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 38 | Win percentage: 13.1%\n","Epoch: 300/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 39 | Win percentage: 13.0%\n","Epoch: 310/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.5 | Wins: 40 | Win percentage: 12.9%\n","Epoch: 320/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 41 | Win percentage: 12.8%\n","Epoch: 330/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.6 | Wins: 43 | Win percentage: 13.0%\n","Epoch: 340/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 45 | Win percentage: 13.2%\n","Epoch: 350/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.4 | Wins: 45 | Win percentage: 12.9%\n","Epoch: 360/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.8 | Wins: 46 | Win percentage: 12.8%\n","Epoch: 370/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 16.8 | Wins: 48 | Win percentage: 13.0%\n","Epoch: 380/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 50 | Win percentage: 13.2%\n","Epoch: 390/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 52 | Win percentage: 13.3%\n","Epoch: 400/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 19.9 | Wins: 55 | Win percentage: 13.8%\n","Epoch: 410/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.4 | Wins: 56 | Win percentage: 13.7%\n","Epoch: 420/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 18.1 | Wins: 58 | Win percentage: 13.8%\n","Epoch: 430/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.7 | Wins: 59 | Win percentage: 13.7%\n","Epoch: 440/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.7 | Wins: 59 | Win percentage: 13.4%\n","Epoch: 450/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.8 | Wins: 59 | Win percentage: 13.1%\n","Epoch: 460/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 12.3 | Wins: 61 | Win percentage: 13.3%\n","Epoch: 470/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.9 | Wins: 61 | Win percentage: 13.0%\n","Epoch: 480/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 62 | Win percentage: 12.9%\n","Epoch: 490/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.7 | Wins: 64 | Win percentage: 13.1%\n","Epoch: 500/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.5 | Wins: 66 | Win percentage: 13.2%\n","Epoch: 510/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 11.6 | Wins: 68 | Win percentage: 13.3%\n","Epoch: 520/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.4 | Wins: 71 | Win percentage: 13.7%\n","Epoch: 530/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 72 | Win percentage: 13.6%\n","Epoch: 540/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.3 | Wins: 72 | Win percentage: 13.3%\n","Epoch: 550/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 73 | Win percentage: 13.3%\n","Epoch: 560/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.7 | Wins: 73 | Win percentage: 13.0%\n","Epoch: 570/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.7 | Wins: 73 | Win percentage: 12.8%\n","Epoch: 580/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.6 | Wins: 73 | Win percentage: 12.6%\n","Epoch: 590/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.7 | Wins: 74 | Win percentage: 12.5%\n","Epoch: 600/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.4 | Wins: 74 | Win percentage: 12.3%\n","Epoch: 610/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 8.3 | Wins: 74 | Win percentage: 12.1%\n","Epoch: 620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.9 | Wins: 75 | Win percentage: 12.1%\n","Epoch: 630/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.8 | Wins: 75 | Win percentage: 11.9%\n","Epoch: 640/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.5 | Wins: 76 | Win percentage: 11.9%\n","Epoch: 650/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 18.6 | Wins: 82 | Win percentage: 12.6%\n","Epoch: 660/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.4 | Wins: 85 | Win percentage: 12.9%\n","Epoch: 670/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 86 | Win percentage: 12.8%\n","Epoch: 680/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 16.3 | Wins: 87 | Win percentage: 12.8%\n","Epoch: 690/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.5 | Wins: 88 | Win percentage: 12.8%\n","Epoch: 700/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 89 | Win percentage: 12.7%\n","Epoch: 710/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.5 | Wins: 89 | Win percentage: 12.5%\n","Epoch: 720/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 16.0 | Wins: 91 | Win percentage: 12.6%\n","Epoch: 730/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 93 | Win percentage: 12.7%\n","Epoch: 740/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.6 | Wins: 95 | Win percentage: 12.8%\n","Epoch: 750/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.3 | Wins: 97 | Win percentage: 12.9%\n","Epoch: 760/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.6 | Wins: 100 | Win percentage: 13.2%\n","Epoch: 770/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 23.8 | Wins: 103 | Win percentage: 13.4%\n","Epoch: 780/10000 | Mean size 10: 3.4 | Longest 10: 006 | Mean steps 10: 16.4 | Wins: 105 | Win percentage: 13.5%\n","Epoch: 790/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.9 | Wins: 106 | Win percentage: 13.4%\n","Epoch: 800/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 15.0 | Wins: 110 | Win percentage: 13.8%\n","Epoch: 810/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 111 | Win percentage: 13.7%\n","Epoch: 820/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 114 | Win percentage: 13.9%\n","Epoch: 830/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.8 | Wins: 116 | Win percentage: 14.0%\n","Epoch: 840/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.9 | Wins: 117 | Win percentage: 13.9%\n","Epoch: 850/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.5 | Wins: 118 | Win percentage: 13.9%\n","Epoch: 860/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.0 | Wins: 120 | Win percentage: 14.0%\n","Epoch: 870/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.4 | Wins: 120 | Win percentage: 13.8%\n","Epoch: 880/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 122 | Win percentage: 13.9%\n","Epoch: 890/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.7 | Wins: 124 | Win percentage: 13.9%\n","Epoch: 900/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.0 | Wins: 127 | Win percentage: 14.1%\n","Epoch: 910/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 17.2 | Wins: 131 | Win percentage: 14.4%\n","Epoch: 920/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.3 | Wins: 131 | Win percentage: 14.2%\n","Epoch: 930/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 14.3 | Wins: 132 | Win percentage: 14.2%\n","Epoch: 940/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 17.4 | Wins: 134 | Win percentage: 14.3%\n","Epoch: 950/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 13.4 | Wins: 136 | Win percentage: 14.3%\n","Epoch: 960/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.1 | Wins: 137 | Win percentage: 14.3%\n","Epoch: 970/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.4 | Wins: 138 | Win percentage: 14.2%\n","Epoch: 980/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 11.4 | Wins: 142 | Win percentage: 14.5%\n","Epoch: 990/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.5 | Wins: 142 | Win percentage: 14.3%\n","Epoch: 1000/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 145 | Win percentage: 14.5%\n","Epoch: 1010/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.6 | Wins: 146 | Win percentage: 14.5%\n","Epoch: 1020/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.7 | Wins: 147 | Win percentage: 14.4%\n","Epoch: 1030/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 17.9 | Wins: 150 | Win percentage: 14.6%\n","Epoch: 1040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.4 | Wins: 151 | Win percentage: 14.5%\n","Epoch: 1050/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 24.2 | Wins: 153 | Win percentage: 14.6%\n","Epoch: 1060/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 14.7 | Wins: 155 | Win percentage: 14.6%\n","Epoch: 1070/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 19.7 | Wins: 155 | Win percentage: 14.5%\n","Epoch: 1080/10000 | Mean size 10: 3.4 | Longest 10: 006 | Mean steps 10: 17.9 | Wins: 157 | Win percentage: 14.5%\n","Epoch: 1090/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.4 | Wins: 157 | Win percentage: 14.4%\n","Epoch: 1100/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 19.8 | Wins: 160 | Win percentage: 14.5%\n","Epoch: 1110/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.5 | Wins: 161 | Win percentage: 14.5%\n","Epoch: 1120/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.8 | Wins: 164 | Win percentage: 14.6%\n","Epoch: 1130/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.9 | Wins: 164 | Win percentage: 14.5%\n","Epoch: 1140/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.1 | Wins: 166 | Win percentage: 14.6%\n","Epoch: 1150/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.9 | Wins: 166 | Win percentage: 14.4%\n","Epoch: 1160/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 25.2 | Wins: 168 | Win percentage: 14.5%\n","Epoch: 1170/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 19.9 | Wins: 170 | Win percentage: 14.5%\n","Epoch: 1180/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 25.9 | Wins: 172 | Win percentage: 14.6%\n","Epoch: 1190/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.7 | Wins: 172 | Win percentage: 14.5%\n","Epoch: 1200/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 16.8 | Wins: 177 | Win percentage: 14.8%\n","Epoch: 1210/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 178 | Win percentage: 14.7%\n","Epoch: 1220/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.6 | Wins: 181 | Win percentage: 14.8%\n","Epoch: 1230/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 14.7 | Wins: 183 | Win percentage: 14.9%\n","Epoch: 1240/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 18.8 | Wins: 188 | Win percentage: 15.2%\n","Epoch: 1250/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 191 | Win percentage: 15.3%\n","Epoch: 1260/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 192 | Win percentage: 15.2%\n","Epoch: 1270/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 19.9 | Wins: 196 | Win percentage: 15.4%\n","Epoch: 1280/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 19.4 | Wins: 199 | Win percentage: 15.5%\n","Epoch: 1290/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 14.3 | Wins: 201 | Win percentage: 15.6%\n","Epoch: 1300/10000 | Mean size 10: 3.9 | Longest 10: 007 | Mean steps 10: 29.6 | Wins: 206 | Win percentage: 15.8%\n","Epoch: 1310/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 23.6 | Wins: 207 | Win percentage: 15.8%\n","Epoch: 1320/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 24.1 | Wins: 210 | Win percentage: 15.9%\n","Epoch: 1330/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 21.8 | Wins: 213 | Win percentage: 16.0%\n","Epoch: 1340/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 31.4 | Wins: 218 | Win percentage: 16.3%\n","Epoch: 1350/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 28.5 | Wins: 223 | Win percentage: 16.5%\n","Epoch: 1360/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 27.6 | Wins: 226 | Win percentage: 16.6%\n","Epoch: 1370/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 22.3 | Wins: 227 | Win percentage: 16.6%\n","Epoch: 1380/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 22.0 | Wins: 230 | Win percentage: 16.7%\n","Epoch: 1390/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 39.6 | Wins: 233 | Win percentage: 16.8%\n","Epoch: 1400/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 24.2 | Wins: 236 | Win percentage: 16.9%\n","Epoch: 1410/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 27.2 | Wins: 238 | Win percentage: 16.9%\n","Epoch: 1420/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 20.6 | Wins: 241 | Win percentage: 17.0%\n","Epoch: 1430/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 21.8 | Wins: 243 | Win percentage: 17.0%\n","Epoch: 1440/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 21.1 | Wins: 248 | Win percentage: 17.2%\n","Epoch: 1450/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 14.1 | Wins: 252 | Win percentage: 17.4%\n","Epoch: 1460/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.6 | Wins: 253 | Win percentage: 17.3%\n","Epoch: 1470/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 20.9 | Wins: 258 | Win percentage: 17.6%\n","Epoch: 1480/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 23.4 | Wins: 262 | Win percentage: 17.7%\n","Epoch: 1490/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 263 | Win percentage: 17.7%\n","Epoch: 1500/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 24.8 | Wins: 265 | Win percentage: 17.7%\n","Epoch: 1510/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 14.2 | Wins: 268 | Win percentage: 17.7%\n","Epoch: 1520/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 17.7 | Wins: 272 | Win percentage: 17.9%\n","Epoch: 1530/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.9 | Wins: 275 | Win percentage: 18.0%\n","Epoch: 1540/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 278 | Win percentage: 18.1%\n","Epoch: 1550/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.0 | Wins: 281 | Win percentage: 18.1%\n","Epoch: 1560/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 20.1 | Wins: 284 | Win percentage: 18.2%\n","Epoch: 1570/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.3 | Wins: 284 | Win percentage: 18.1%\n","Epoch: 1580/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 22.3 | Wins: 287 | Win percentage: 18.2%\n","Epoch: 1590/10000 | Mean size 10: 3.6 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 293 | Win percentage: 18.4%\n","Epoch: 1600/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 296 | Win percentage: 18.5%\n","Epoch: 1610/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 15.3 | Wins: 300 | Win percentage: 18.6%\n","Epoch: 1620/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.0 | Wins: 302 | Win percentage: 18.6%\n","Epoch: 1630/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 19.5 | Wins: 306 | Win percentage: 18.8%\n","Epoch: 1640/10000 | Mean size 10: 3.5 | Longest 10: 006 | Mean steps 10: 16.9 | Wins: 309 | Win percentage: 18.8%\n","Epoch: 1650/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 23.7 | Wins: 310 | Win percentage: 18.8%\n","Epoch: 1660/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.5 | Wins: 311 | Win percentage: 18.7%\n","Epoch: 1670/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 12.7 | Wins: 314 | Win percentage: 18.8%\n","Epoch: 1680/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 19.3 | Wins: 316 | Win percentage: 18.8%\n","Epoch: 1690/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 29.4 | Wins: 321 | Win percentage: 19.0%\n","Epoch: 1700/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 25.2 | Wins: 324 | Win percentage: 19.1%\n","Epoch: 1710/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 24.5 | Wins: 327 | Win percentage: 19.1%\n","Epoch: 1720/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 13.8 | Wins: 331 | Win percentage: 19.2%\n","Epoch: 1730/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 332 | Win percentage: 19.2%\n","Epoch: 1740/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 17.3 | Wins: 336 | Win percentage: 19.3%\n","Epoch: 1750/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 339 | Win percentage: 19.4%\n","Epoch: 1760/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 21.3 | Wins: 342 | Win percentage: 19.4%\n","Epoch: 1770/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 343 | Win percentage: 19.4%\n","Epoch: 1780/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 23.0 | Wins: 348 | Win percentage: 19.6%\n","Epoch: 1790/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.0 | Wins: 349 | Win percentage: 19.5%\n","Epoch: 1800/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.8 | Wins: 352 | Win percentage: 19.6%\n","Epoch: 1810/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 13.8 | Wins: 355 | Win percentage: 19.6%\n","Epoch: 1820/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 359 | Win percentage: 19.7%\n","Epoch: 1830/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 7.6 | Wins: 362 | Win percentage: 19.8%\n","Epoch: 1840/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.7 | Wins: 364 | Win percentage: 19.8%\n","Epoch: 1850/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.1 | Wins: 364 | Win percentage: 19.7%\n","Epoch: 1860/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 14.3 | Wins: 368 | Win percentage: 19.8%\n","Epoch: 1870/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 19.4 | Wins: 370 | Win percentage: 19.8%\n","Epoch: 1880/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 21.5 | Wins: 373 | Win percentage: 19.8%\n","Epoch: 1890/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 23.3 | Wins: 377 | Win percentage: 19.9%\n","Epoch: 1900/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 19.9 | Wins: 380 | Win percentage: 20.0%\n","Epoch: 1910/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 11.8 | Wins: 383 | Win percentage: 20.1%\n","Epoch: 1920/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 8.9 | Wins: 385 | Win percentage: 20.1%\n","Epoch: 1930/10000 | Mean size 10: 3.6 | Longest 10: 004 | Mean steps 10: 21.5 | Wins: 391 | Win percentage: 20.3%\n","Epoch: 1940/10000 | Mean size 10: 3.6 | Longest 10: 006 | Mean steps 10: 17.8 | Wins: 395 | Win percentage: 20.4%\n","Epoch: 1950/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 399 | Win percentage: 20.5%\n","Epoch: 1960/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 15.4 | Wins: 402 | Win percentage: 20.5%\n","Epoch: 1970/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.5 | Wins: 403 | Win percentage: 20.5%\n","Epoch: 1980/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 23.1 | Wins: 407 | Win percentage: 20.6%\n","Epoch: 1990/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 21.4 | Wins: 408 | Win percentage: 20.5%\n","Epoch: 2000/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 17.4 | Wins: 410 | Win percentage: 20.5%\n","Epoch: 2010/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 18.4 | Wins: 415 | Win percentage: 20.6%\n","Epoch: 2020/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.3 | Wins: 417 | Win percentage: 20.6%\n","Epoch: 2030/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 16.7 | Wins: 421 | Win percentage: 20.7%\n","Epoch: 2040/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 22.3 | Wins: 422 | Win percentage: 20.7%\n","Epoch: 2050/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 13.2 | Wins: 425 | Win percentage: 20.7%\n","Epoch: 2060/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 31.3 | Wins: 428 | Win percentage: 20.8%\n","Epoch: 2070/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 17.3 | Wins: 432 | Win percentage: 20.9%\n","Epoch: 2080/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 17.5 | Wins: 436 | Win percentage: 21.0%\n","Epoch: 2090/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 22.6 | Wins: 441 | Win percentage: 21.1%\n","Epoch: 2100/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 17.9 | Wins: 444 | Win percentage: 21.1%\n","Epoch: 2110/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 33.6 | Wins: 449 | Win percentage: 21.3%\n","Epoch: 2120/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 17.8 | Wins: 453 | Win percentage: 21.4%\n","Epoch: 2130/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 15.3 | Wins: 457 | Win percentage: 21.5%\n","Epoch: 2140/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 22.3 | Wins: 461 | Win percentage: 21.5%\n","Epoch: 2150/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 30.1 | Wins: 467 | Win percentage: 21.7%\n","Epoch: 2160/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 10.9 | Wins: 470 | Win percentage: 21.8%\n","Epoch: 2170/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 22.6 | Wins: 472 | Win percentage: 21.8%\n","Epoch: 2180/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 27.0 | Wins: 476 | Win percentage: 21.8%\n","Epoch: 2190/10000 | Mean size 10: 3.7 | Longest 10: 004 | Mean steps 10: 28.9 | Wins: 483 | Win percentage: 22.1%\n","Epoch: 2200/10000 | Mean size 10: 4.1 | Longest 10: 005 | Mean steps 10: 27.7 | Wins: 490 | Win percentage: 22.3%\n","Epoch: 2210/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 29.1 | Wins: 493 | Win percentage: 22.3%\n","Epoch: 2220/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 17.2 | Wins: 497 | Win percentage: 22.4%\n","Epoch: 2230/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 16.9 | Wins: 500 | Win percentage: 22.4%\n","Epoch: 2240/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 13.3 | Wins: 504 | Win percentage: 22.5%\n","Epoch: 2250/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.6 | Wins: 507 | Win percentage: 22.5%\n","Epoch: 2260/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 17.1 | Wins: 509 | Win percentage: 22.5%\n","Epoch: 2270/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 23.0 | Wins: 512 | Win percentage: 22.6%\n","Epoch: 2280/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 514 | Win percentage: 22.5%\n","Epoch: 2290/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 516 | Win percentage: 22.5%\n","Epoch: 2300/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 19.1 | Wins: 521 | Win percentage: 22.7%\n","Epoch: 2310/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 21.9 | Wins: 526 | Win percentage: 22.8%\n","Epoch: 2320/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 33.3 | Wins: 533 | Win percentage: 23.0%\n","Epoch: 2330/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 16.1 | Wins: 535 | Win percentage: 23.0%\n","Epoch: 2340/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 20.4 | Wins: 540 | Win percentage: 23.1%\n","Epoch: 2350/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 16.9 | Wins: 543 | Win percentage: 23.1%\n","Epoch: 2360/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 26.6 | Wins: 547 | Win percentage: 23.2%\n","Epoch: 2370/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 29.2 | Wins: 552 | Win percentage: 23.3%\n","Epoch: 2380/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 26.7 | Wins: 555 | Win percentage: 23.3%\n","Epoch: 2390/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 35.1 | Wins: 563 | Win percentage: 23.6%\n","Epoch: 2400/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 18.3 | Wins: 568 | Win percentage: 23.7%\n","Epoch: 2410/10000 | Mean size 10: 4.4 | Longest 10: 006 | Mean steps 10: 27.3 | Wins: 574 | Win percentage: 23.8%\n","Epoch: 2420/10000 | Mean size 10: 3.9 | Longest 10: 005 | Mean steps 10: 20.5 | Wins: 580 | Win percentage: 24.0%\n","Epoch: 2430/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 585 | Win percentage: 24.1%\n","Epoch: 2440/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 23.5 | Wins: 589 | Win percentage: 24.1%\n","Epoch: 2450/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.1 | Wins: 591 | Win percentage: 24.1%\n","Epoch: 2460/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 30.9 | Wins: 595 | Win percentage: 24.2%\n","Epoch: 2470/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 32.4 | Wins: 600 | Win percentage: 24.3%\n","Epoch: 2480/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 20.5 | Wins: 607 | Win percentage: 24.5%\n","Epoch: 2490/10000 | Mean size 10: 3.8 | Longest 10: 007 | Mean steps 10: 19.9 | Wins: 611 | Win percentage: 24.5%\n","Epoch: 2500/10000 | Mean size 10: 3.8 | Longest 10: 007 | Mean steps 10: 20.2 | Wins: 615 | Win percentage: 24.6%\n","Epoch: 2510/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 18.4 | Wins: 620 | Win percentage: 24.7%\n","Epoch: 2520/10000 | Mean size 10: 3.9 | Longest 10: 005 | Mean steps 10: 35.7 | Wins: 627 | Win percentage: 24.9%\n","Epoch: 2530/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 23.0 | Wins: 631 | Win percentage: 24.9%\n","Epoch: 2540/10000 | Mean size 10: 4.3 | Longest 10: 005 | Mean steps 10: 30.5 | Wins: 639 | Win percentage: 25.2%\n","Epoch: 2550/10000 | Mean size 10: 3.9 | Longest 10: 008 | Mean steps 10: 22.6 | Wins: 644 | Win percentage: 25.3%\n","Epoch: 2560/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 647 | Win percentage: 25.3%\n","Epoch: 2570/10000 | Mean size 10: 4.2 | Longest 10: 007 | Mean steps 10: 30.0 | Wins: 655 | Win percentage: 25.5%\n","Epoch: 2580/10000 | Mean size 10: 4.3 | Longest 10: 007 | Mean steps 10: 32.2 | Wins: 661 | Win percentage: 25.6%\n","Epoch: 2590/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 27.3 | Wins: 666 | Win percentage: 25.7%\n","Epoch: 2600/10000 | Mean size 10: 4.2 | Longest 10: 006 | Mean steps 10: 27.3 | Wins: 673 | Win percentage: 25.9%\n","Epoch: 2610/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 25.7 | Wins: 677 | Win percentage: 25.9%\n","Epoch: 2620/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 14.8 | Wins: 684 | Win percentage: 26.1%\n","Epoch: 2630/10000 | Mean size 10: 4.1 | Longest 10: 005 | Mean steps 10: 21.8 | Wins: 691 | Win percentage: 26.3%\n","Epoch: 2640/10000 | Mean size 10: 4.2 | Longest 10: 006 | Mean steps 10: 28.9 | Wins: 698 | Win percentage: 26.4%\n","Epoch: 2650/10000 | Mean size 10: 3.5 | Longest 10: 006 | Mean steps 10: 27.1 | Wins: 701 | Win percentage: 26.5%\n","Epoch: 2660/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 32.4 | Wins: 707 | Win percentage: 26.6%\n","Epoch: 2670/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 26.1 | Wins: 713 | Win percentage: 26.7%\n","Epoch: 2680/10000 | Mean size 10: 4.1 | Longest 10: 005 | Mean steps 10: 35.3 | Wins: 720 | Win percentage: 26.9%\n","Epoch: 2690/10000 | Mean size 10: 4.1 | Longest 10: 005 | Mean steps 10: 38.7 | Wins: 727 | Win percentage: 27.0%\n","Epoch: 2700/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 16.8 | Wins: 732 | Win percentage: 27.1%\n","Epoch: 2710/10000 | Mean size 10: 4.9 | Longest 10: 007 | Mean steps 10: 47.0 | Wins: 741 | Win percentage: 27.3%\n","Epoch: 2720/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 22.8 | Wins: 746 | Win percentage: 27.4%\n","Epoch: 2730/10000 | Mean size 10: 4.1 | Longest 10: 009 | Mean steps 10: 26.3 | Wins: 751 | Win percentage: 27.5%\n","Epoch: 2740/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 22.6 | Wins: 757 | Win percentage: 27.6%\n","Epoch: 2750/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 21.0 | Wins: 762 | Win percentage: 27.7%\n","Epoch: 2760/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 24.5 | Wins: 767 | Win percentage: 27.8%\n","Epoch: 2770/10000 | Mean size 10: 4.2 | Longest 10: 005 | Mean steps 10: 26.6 | Wins: 776 | Win percentage: 28.0%\n","Epoch: 2780/10000 | Mean size 10: 4.5 | Longest 10: 006 | Mean steps 10: 30.8 | Wins: 785 | Win percentage: 28.2%\n","Epoch: 2790/10000 | Mean size 10: 4.6 | Longest 10: 006 | Mean steps 10: 29.0 | Wins: 793 | Win percentage: 28.4%\n","Epoch: 2800/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 26.8 | Wins: 799 | Win percentage: 28.5%\n","Epoch: 2810/10000 | Mean size 10: 3.8 | Longest 10: 008 | Mean steps 10: 29.1 | Wins: 803 | Win percentage: 28.6%\n","Epoch: 2820/10000 | Mean size 10: 4.3 | Longest 10: 005 | Mean steps 10: 28.0 | Wins: 812 | Win percentage: 28.8%\n","Epoch: 2830/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 17.0 | Wins: 815 | Win percentage: 28.8%\n","Epoch: 2840/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 29.8 | Wins: 820 | Win percentage: 28.9%\n","Epoch: 2850/10000 | Mean size 10: 4.6 | Longest 10: 008 | Mean steps 10: 40.6 | Wins: 826 | Win percentage: 29.0%\n","Epoch: 2860/10000 | Mean size 10: 4.3 | Longest 10: 007 | Mean steps 10: 28.8 | Wins: 833 | Win percentage: 29.1%\n","Epoch: 2870/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 19.1 | Wins: 840 | Win percentage: 29.3%\n","Epoch: 2880/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 18.6 | Wins: 846 | Win percentage: 29.4%\n","Epoch: 2890/10000 | Mean size 10: 4.0 | Longest 10: 007 | Mean steps 10: 24.1 | Wins: 852 | Win percentage: 29.5%\n","Epoch: 2900/10000 | Mean size 10: 4.2 | Longest 10: 005 | Mean steps 10: 27.6 | Wins: 859 | Win percentage: 29.6%\n","Epoch: 2910/10000 | Mean size 10: 4.5 | Longest 10: 009 | Mean steps 10: 37.5 | Wins: 865 | Win percentage: 29.7%\n","Epoch: 2920/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 18.5 | Wins: 871 | Win percentage: 29.8%\n","Epoch: 2930/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.3 | Wins: 874 | Win percentage: 29.8%\n","Epoch: 2940/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 25.6 | Wins: 880 | Win percentage: 29.9%\n","Epoch: 2950/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 22.2 | Wins: 885 | Win percentage: 30.0%\n","Epoch: 2960/10000 | Mean size 10: 3.9 | Longest 10: 005 | Mean steps 10: 17.2 | Wins: 892 | Win percentage: 30.1%\n","Epoch: 2970/10000 | Mean size 10: 4.2 | Longest 10: 007 | Mean steps 10: 24.5 | Wins: 897 | Win percentage: 30.2%\n","Epoch: 2980/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 20.7 | Wins: 903 | Win percentage: 30.3%\n","Epoch: 2990/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 28.6 | Wins: 910 | Win percentage: 30.4%\n","Epoch: 3000/10000 | Mean size 10: 4.1 | Longest 10: 007 | Mean steps 10: 43.0 | Wins: 915 | Win percentage: 30.5%\n","Epoch: 3010/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 21.7 | Wins: 920 | Win percentage: 30.6%\n","Epoch: 3020/10000 | Mean size 10: 4.2 | Longest 10: 006 | Mean steps 10: 25.0 | Wins: 926 | Win percentage: 30.7%\n","Epoch: 3030/10000 | Mean size 10: 4.5 | Longest 10: 008 | Mean steps 10: 27.0 | Wins: 935 | Win percentage: 30.9%\n","Epoch: 3040/10000 | Mean size 10: 4.5 | Longest 10: 008 | Mean steps 10: 33.5 | Wins: 943 | Win percentage: 31.0%\n","Epoch: 3050/10000 | Mean size 10: 4.1 | Longest 10: 005 | Mean steps 10: 33.0 | Wins: 950 | Win percentage: 31.1%\n","Epoch: 3060/10000 | Mean size 10: 4.6 | Longest 10: 005 | Mean steps 10: 38.9 | Wins: 959 | Win percentage: 31.3%\n","Epoch: 3070/10000 | Mean size 10: 4.3 | Longest 10: 007 | Mean steps 10: 25.4 | Wins: 966 | Win percentage: 31.5%\n","Epoch: 3080/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 23.0 | Wins: 972 | Win percentage: 31.6%\n","Epoch: 3090/10000 | Mean size 10: 4.3 | Longest 10: 007 | Mean steps 10: 33.4 | Wins: 979 | Win percentage: 31.7%\n","Epoch: 3100/10000 | Mean size 10: 4.6 | Longest 10: 006 | Mean steps 10: 42.2 | Wins: 986 | Win percentage: 31.8%\n","Epoch: 3110/10000 | Mean size 10: 4.7 | Longest 10: 006 | Mean steps 10: 30.0 | Wins: 995 | Win percentage: 32.0%\n","Epoch: 3120/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 31.2 | Wins: 1003 | Win percentage: 32.1%\n","Epoch: 3130/10000 | Mean size 10: 4.8 | Longest 10: 010 | Mean steps 10: 26.7 | Wins: 1009 | Win percentage: 32.2%\n","Epoch: 3140/10000 | Mean size 10: 5.0 | Longest 10: 008 | Mean steps 10: 32.6 | Wins: 1019 | Win percentage: 32.5%\n","Epoch: 3150/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 19.7 | Wins: 1026 | Win percentage: 32.6%\n","Epoch: 3160/10000 | Mean size 10: 4.5 | Longest 10: 008 | Mean steps 10: 22.4 | Wins: 1032 | Win percentage: 32.7%\n","Epoch: 3170/10000 | Mean size 10: 4.3 | Longest 10: 009 | Mean steps 10: 30.9 | Wins: 1037 | Win percentage: 32.7%\n","Epoch: 3180/10000 | Mean size 10: 3.9 | Longest 10: 005 | Mean steps 10: 16.7 | Wins: 1044 | Win percentage: 32.8%\n","Epoch: 3190/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 16.8 | Wins: 1051 | Win percentage: 32.9%\n","Epoch: 3200/10000 | Mean size 10: 4.7 | Longest 10: 010 | Mean steps 10: 30.5 | Wins: 1058 | Win percentage: 33.1%\n","Epoch: 3210/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 30.0 | Wins: 1065 | Win percentage: 33.2%\n","Epoch: 3220/10000 | Mean size 10: 4.4 | Longest 10: 008 | Mean steps 10: 24.1 | Wins: 1073 | Win percentage: 33.3%\n","Epoch: 3230/10000 | Mean size 10: 4.3 | Longest 10: 007 | Mean steps 10: 22.0 | Wins: 1079 | Win percentage: 33.4%\n","Epoch: 3240/10000 | Mean size 10: 4.5 | Longest 10: 006 | Mean steps 10: 29.2 | Wins: 1087 | Win percentage: 33.5%\n","Epoch: 3250/10000 | Mean size 10: 4.7 | Longest 10: 008 | Mean steps 10: 28.2 | Wins: 1094 | Win percentage: 33.7%\n","Epoch: 3260/10000 | Mean size 10: 4.2 | Longest 10: 008 | Mean steps 10: 27.2 | Wins: 1100 | Win percentage: 33.7%\n","Epoch: 3270/10000 | Mean size 10: 4.5 | Longest 10: 006 | Mean steps 10: 26.7 | Wins: 1109 | Win percentage: 33.9%\n","Epoch: 3280/10000 | Mean size 10: 4.2 | Longest 10: 006 | Mean steps 10: 22.6 | Wins: 1116 | Win percentage: 34.0%\n","Epoch: 3290/10000 | Mean size 10: 5.3 | Longest 10: 007 | Mean steps 10: 31.5 | Wins: 1126 | Win percentage: 34.2%\n","Epoch: 3300/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 22.4 | Wins: 1133 | Win percentage: 34.3%\n","Epoch: 3310/10000 | Mean size 10: 5.2 | Longest 10: 011 | Mean steps 10: 40.6 | Wins: 1141 | Win percentage: 34.5%\n","Epoch: 3320/10000 | Mean size 10: 4.5 | Longest 10: 009 | Mean steps 10: 27.4 | Wins: 1148 | Win percentage: 34.6%\n","Epoch: 3330/10000 | Mean size 10: 4.7 | Longest 10: 006 | Mean steps 10: 24.2 | Wins: 1158 | Win percentage: 34.8%\n","Epoch: 3340/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 21.9 | Wins: 1166 | Win percentage: 34.9%\n","Epoch: 3350/10000 | Mean size 10: 4.4 | Longest 10: 006 | Mean steps 10: 21.7 | Wins: 1173 | Win percentage: 35.0%\n","Epoch: 3360/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 27.2 | Wins: 1181 | Win percentage: 35.1%\n","Epoch: 3370/10000 | Mean size 10: 5.1 | Longest 10: 009 | Mean steps 10: 42.7 | Wins: 1190 | Win percentage: 35.3%\n","Epoch: 3380/10000 | Mean size 10: 5.3 | Longest 10: 008 | Mean steps 10: 29.9 | Wins: 1199 | Win percentage: 35.5%\n","Epoch: 3390/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 35.7 | Wins: 1205 | Win percentage: 35.5%\n","Epoch: 3400/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 33.3 | Wins: 1212 | Win percentage: 35.6%\n","Epoch: 3410/10000 | Mean size 10: 5.1 | Longest 10: 008 | Mean steps 10: 36.0 | Wins: 1221 | Win percentage: 35.8%\n","Epoch: 3420/10000 | Mean size 10: 5.3 | Longest 10: 007 | Mean steps 10: 32.0 | Wins: 1230 | Win percentage: 36.0%\n","Epoch: 3430/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 20.2 | Wins: 1235 | Win percentage: 36.0%\n","Epoch: 3440/10000 | Mean size 10: 4.6 | Longest 10: 006 | Mean steps 10: 32.7 | Wins: 1244 | Win percentage: 36.2%\n","Epoch: 3450/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 28.3 | Wins: 1252 | Win percentage: 36.3%\n","Epoch: 3460/10000 | Mean size 10: 4.5 | Longest 10: 007 | Mean steps 10: 29.0 | Wins: 1260 | Win percentage: 36.4%\n","Epoch: 3470/10000 | Mean size 10: 5.3 | Longest 10: 009 | Mean steps 10: 47.1 | Wins: 1268 | Win percentage: 36.5%\n","Epoch: 3480/10000 | Mean size 10: 4.8 | Longest 10: 008 | Mean steps 10: 35.5 | Wins: 1275 | Win percentage: 36.6%\n","Epoch: 3490/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 26.4 | Wins: 1283 | Win percentage: 36.8%\n","Epoch: 3500/10000 | Mean size 10: 4.3 | Longest 10: 006 | Mean steps 10: 23.2 | Wins: 1290 | Win percentage: 36.9%\n","Epoch: 3510/10000 | Mean size 10: 4.3 | Longest 10: 007 | Mean steps 10: 32.8 | Wins: 1296 | Win percentage: 36.9%\n","Epoch: 3520/10000 | Mean size 10: 4.9 | Longest 10: 008 | Mean steps 10: 33.9 | Wins: 1305 | Win percentage: 37.1%\n","Epoch: 3530/10000 | Mean size 10: 4.6 | Longest 10: 008 | Mean steps 10: 25.3 | Wins: 1314 | Win percentage: 37.2%\n","Epoch: 3540/10000 | Mean size 10: 5.3 | Longest 10: 007 | Mean steps 10: 40.6 | Wins: 1324 | Win percentage: 37.4%\n","Epoch: 3550/10000 | Mean size 10: 5.9 | Longest 10: 010 | Mean steps 10: 50.6 | Wins: 1333 | Win percentage: 37.5%\n","Epoch: 3560/10000 | Mean size 10: 4.9 | Longest 10: 007 | Mean steps 10: 36.5 | Wins: 1343 | Win percentage: 37.7%\n","Epoch: 3570/10000 | Mean size 10: 5.1 | Longest 10: 007 | Mean steps 10: 37.2 | Wins: 1352 | Win percentage: 37.9%\n","Epoch: 3580/10000 | Mean size 10: 5.7 | Longest 10: 010 | Mean steps 10: 44.8 | Wins: 1362 | Win percentage: 38.0%\n","Epoch: 3590/10000 | Mean size 10: 5.1 | Longest 10: 007 | Mean steps 10: 34.2 | Wins: 1370 | Win percentage: 38.2%\n","Epoch: 3600/10000 | Mean size 10: 4.6 | Longest 10: 008 | Mean steps 10: 31.3 | Wins: 1377 | Win percentage: 38.2%\n","Epoch: 3610/10000 | Mean size 10: 5.6 | Longest 10: 009 | Mean steps 10: 45.8 | Wins: 1386 | Win percentage: 38.4%\n","Epoch: 3620/10000 | Mean size 10: 5.1 | Longest 10: 008 | Mean steps 10: 35.1 | Wins: 1396 | Win percentage: 38.6%\n","Epoch: 3630/10000 | Mean size 10: 4.5 | Longest 10: 006 | Mean steps 10: 36.4 | Wins: 1404 | Win percentage: 38.7%\n","Epoch: 3640/10000 | Mean size 10: 5.7 | Longest 10: 011 | Mean steps 10: 51.4 | Wins: 1413 | Win percentage: 38.8%\n","Epoch: 3650/10000 | Mean size 10: 4.6 | Longest 10: 006 | Mean steps 10: 21.1 | Wins: 1422 | Win percentage: 39.0%\n","Epoch: 3660/10000 | Mean size 10: 4.9 | Longest 10: 009 | Mean steps 10: 28.0 | Wins: 1430 | Win percentage: 39.1%\n","Epoch: 3670/10000 | Mean size 10: 5.0 | Longest 10: 008 | Mean steps 10: 39.1 | Wins: 1439 | Win percentage: 39.2%\n","Epoch: 3680/10000 | Mean size 10: 4.6 | Longest 10: 008 | Mean steps 10: 27.9 | Wins: 1446 | Win percentage: 39.3%\n","Epoch: 3690/10000 | Mean size 10: 5.5 | Longest 10: 008 | Mean steps 10: 50.7 | Wins: 1455 | Win percentage: 39.4%\n","Epoch: 3700/10000 | Mean size 10: 5.4 | Longest 10: 007 | Mean steps 10: 37.5 | Wins: 1464 | Win percentage: 39.6%\n","Epoch: 3710/10000 | Mean size 10: 4.8 | Longest 10: 006 | Mean steps 10: 32.9 | Wins: 1473 | Win percentage: 39.7%\n","Epoch: 3720/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 26.1 | Wins: 1480 | Win percentage: 39.8%\n","Epoch: 3730/10000 | Mean size 10: 4.4 | Longest 10: 006 | Mean steps 10: 30.4 | Wins: 1488 | Win percentage: 39.9%\n","Epoch: 3740/10000 | Mean size 10: 4.9 | Longest 10: 006 | Mean steps 10: 32.9 | Wins: 1498 | Win percentage: 40.1%\n","Epoch: 3750/10000 | Mean size 10: 5.6 | Longest 10: 008 | Mean steps 10: 51.0 | Wins: 1507 | Win percentage: 40.2%\n","Epoch: 3760/10000 | Mean size 10: 5.3 | Longest 10: 008 | Mean steps 10: 31.2 | Wins: 1515 | Win percentage: 40.3%\n","Epoch: 3770/10000 | Mean size 10: 5.3 | Longest 10: 007 | Mean steps 10: 37.9 | Wins: 1525 | Win percentage: 40.5%\n","Epoch: 3780/10000 | Mean size 10: 6.2 | Longest 10: 008 | Mean steps 10: 46.6 | Wins: 1535 | Win percentage: 40.6%\n","Epoch: 3790/10000 | Mean size 10: 5.5 | Longest 10: 006 | Mean steps 10: 50.5 | Wins: 1545 | Win percentage: 40.8%\n","Epoch: 3800/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 29.4 | Wins: 1553 | Win percentage: 40.9%\n","Epoch: 3810/10000 | Mean size 10: 5.5 | Longest 10: 007 | Mean steps 10: 39.1 | Wins: 1561 | Win percentage: 41.0%\n","Epoch: 3820/10000 | Mean size 10: 5.6 | Longest 10: 008 | Mean steps 10: 44.7 | Wins: 1571 | Win percentage: 41.1%\n","Epoch: 3830/10000 | Mean size 10: 5.6 | Longest 10: 008 | Mean steps 10: 42.5 | Wins: 1580 | Win percentage: 41.3%\n","Epoch: 3840/10000 | Mean size 10: 5.3 | Longest 10: 008 | Mean steps 10: 36.2 | Wins: 1589 | Win percentage: 41.4%\n","Epoch: 3850/10000 | Mean size 10: 5.7 | Longest 10: 008 | Mean steps 10: 38.6 | Wins: 1598 | Win percentage: 41.5%\n","Epoch: 3860/10000 | Mean size 10: 5.2 | Longest 10: 010 | Mean steps 10: 36.6 | Wins: 1608 | Win percentage: 41.7%\n","Epoch: 3870/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 23.7 | Wins: 1618 | Win percentage: 41.8%\n","Epoch: 3880/10000 | Mean size 10: 6.3 | Longest 10: 009 | Mean steps 10: 44.6 | Wins: 1628 | Win percentage: 42.0%\n","Epoch: 3890/10000 | Mean size 10: 5.0 | Longest 10: 007 | Mean steps 10: 27.2 | Wins: 1637 | Win percentage: 42.1%\n","Epoch: 3900/10000 | Mean size 10: 5.6 | Longest 10: 009 | Mean steps 10: 47.1 | Wins: 1647 | Win percentage: 42.2%\n","Epoch: 3910/10000 | Mean size 10: 5.3 | Longest 10: 007 | Mean steps 10: 46.7 | Wins: 1657 | Win percentage: 42.4%\n","Epoch: 3920/10000 | Mean size 10: 5.8 | Longest 10: 008 | Mean steps 10: 34.3 | Wins: 1666 | Win percentage: 42.5%\n","Epoch: 3930/10000 | Mean size 10: 4.7 | Longest 10: 006 | Mean steps 10: 40.8 | Wins: 1675 | Win percentage: 42.6%\n","Epoch: 3940/10000 | Mean size 10: 5.3 | Longest 10: 009 | Mean steps 10: 42.4 | Wins: 1683 | Win percentage: 42.7%\n","Epoch: 3950/10000 | Mean size 10: 5.2 | Longest 10: 007 | Mean steps 10: 40.5 | Wins: 1691 | Win percentage: 42.8%\n","Epoch: 3960/10000 | Mean size 10: 5.5 | Longest 10: 010 | Mean steps 10: 30.9 | Wins: 1700 | Win percentage: 42.9%\n","Epoch: 3970/10000 | Mean size 10: 5.6 | Longest 10: 011 | Mean steps 10: 56.1 | Wins: 1709 | Win percentage: 43.0%\n","Epoch: 3980/10000 | Mean size 10: 5.6 | Longest 10: 009 | Mean steps 10: 37.2 | Wins: 1717 | Win percentage: 43.1%\n","Epoch: 3990/10000 | Mean size 10: 5.1 | Longest 10: 007 | Mean steps 10: 34.0 | Wins: 1724 | Win percentage: 43.2%\n","Epoch: 4000/10000 | Mean size 10: 5.7 | Longest 10: 009 | Mean steps 10: 36.4 | Wins: 1732 | Win percentage: 43.3%\n","Epoch: 4010/10000 | Mean size 10: 6.4 | Longest 10: 009 | Mean steps 10: 50.5 | Wins: 1742 | Win percentage: 43.4%\n","Epoch: 4020/10000 | Mean size 10: 6.0 | Longest 10: 009 | Mean steps 10: 39.1 | Wins: 1751 | Win percentage: 43.6%\n","Epoch: 4030/10000 | Mean size 10: 5.6 | Longest 10: 008 | Mean steps 10: 36.4 | Wins: 1760 | Win percentage: 43.7%\n","Epoch: 4040/10000 | Mean size 10: 5.7 | Longest 10: 008 | Mean steps 10: 39.4 | Wins: 1767 | Win percentage: 43.7%\n","Epoch: 4050/10000 | Mean size 10: 5.5 | Longest 10: 008 | Mean steps 10: 39.5 | Wins: 1775 | Win percentage: 43.8%\n","Epoch: 4060/10000 | Mean size 10: 6.2 | Longest 10: 009 | Mean steps 10: 43.6 | Wins: 1783 | Win percentage: 43.9%\n","Epoch: 4070/10000 | Mean size 10: 5.8 | Longest 10: 009 | Mean steps 10: 38.7 | Wins: 1793 | Win percentage: 44.1%\n","Epoch: 4080/10000 | Mean size 10: 5.6 | Longest 10: 008 | Mean steps 10: 36.2 | Wins: 1801 | Win percentage: 44.1%\n","Epoch: 4090/10000 | Mean size 10: 6.4 | Longest 10: 011 | Mean steps 10: 47.4 | Wins: 1811 | Win percentage: 44.3%\n","Epoch: 4100/10000 | Mean size 10: 6.5 | Longest 10: 009 | Mean steps 10: 46.5 | Wins: 1819 | Win percentage: 44.4%\n","Epoch: 4110/10000 | Mean size 10: 5.2 | Longest 10: 009 | Mean steps 10: 38.5 | Wins: 1827 | Win percentage: 44.5%\n","Epoch: 4120/10000 | Mean size 10: 6.3 | Longest 10: 009 | Mean steps 10: 44.3 | Wins: 1837 | Win percentage: 44.6%\n","Epoch: 4130/10000 | Mean size 10: 5.9 | Longest 10: 009 | Mean steps 10: 38.7 | Wins: 1847 | Win percentage: 44.7%\n","Epoch: 4140/10000 | Mean size 10: 5.4 | Longest 10: 010 | Mean steps 10: 34.0 | Wins: 1856 | Win percentage: 44.8%\n","Epoch: 4150/10000 | Mean size 10: 5.3 | Longest 10: 010 | Mean steps 10: 30.3 | Wins: 1862 | Win percentage: 44.9%\n","Epoch: 4160/10000 | Mean size 10: 6.5 | Longest 10: 011 | Mean steps 10: 47.0 | Wins: 1871 | Win percentage: 45.0%\n","Epoch: 4170/10000 | Mean size 10: 7.4 | Longest 10: 013 | Mean steps 10: 61.9 | Wins: 1881 | Win percentage: 45.1%\n","Epoch: 4180/10000 | Mean size 10: 7.2 | Longest 10: 010 | Mean steps 10: 62.4 | Wins: 1891 | Win percentage: 45.2%\n","Epoch: 4190/10000 | Mean size 10: 4.8 | Longest 10: 008 | Mean steps 10: 33.4 | Wins: 1899 | Win percentage: 45.3%\n","Epoch: 4200/10000 | Mean size 10: 5.3 | Longest 10: 009 | Mean steps 10: 36.2 | Wins: 1908 | Win percentage: 45.4%\n","Epoch: 4210/10000 | Mean size 10: 5.8 | Longest 10: 008 | Mean steps 10: 34.7 | Wins: 1918 | Win percentage: 45.6%\n","Epoch: 4220/10000 | Mean size 10: 6.2 | Longest 10: 010 | Mean steps 10: 43.6 | Wins: 1927 | Win percentage: 45.7%\n","Epoch: 4230/10000 | Mean size 10: 5.9 | Longest 10: 008 | Mean steps 10: 42.5 | Wins: 1936 | Win percentage: 45.8%\n","Epoch: 4240/10000 | Mean size 10: 7.4 | Longest 10: 011 | Mean steps 10: 55.4 | Wins: 1946 | Win percentage: 45.9%\n","Epoch: 4250/10000 | Mean size 10: 6.5 | Longest 10: 010 | Mean steps 10: 55.5 | Wins: 1955 | Win percentage: 46.0%\n","Epoch: 4260/10000 | Mean size 10: 6.2 | Longest 10: 009 | Mean steps 10: 46.3 | Wins: 1964 | Win percentage: 46.1%\n","Epoch: 4270/10000 | Mean size 10: 7.8 | Longest 10: 013 | Mean steps 10: 62.0 | Wins: 1973 | Win percentage: 46.2%\n","Epoch: 4280/10000 | Mean size 10: 6.3 | Longest 10: 009 | Mean steps 10: 44.5 | Wins: 1983 | Win percentage: 46.3%\n","Epoch: 4290/10000 | Mean size 10: 6.5 | Longest 10: 009 | Mean steps 10: 52.6 | Wins: 1993 | Win percentage: 46.5%\n","Epoch: 4300/10000 | Mean size 10: 7.1 | Longest 10: 014 | Mean steps 10: 50.0 | Wins: 2003 | Win percentage: 46.6%\n","Epoch: 4310/10000 | Mean size 10: 6.4 | Longest 10: 008 | Mean steps 10: 63.1 | Wins: 2012 | Win percentage: 46.7%\n","Epoch: 4320/10000 | Mean size 10: 5.9 | Longest 10: 010 | Mean steps 10: 46.8 | Wins: 2020 | Win percentage: 46.8%\n","Epoch: 4330/10000 | Mean size 10: 6.1 | Longest 10: 011 | Mean steps 10: 48.2 | Wins: 2028 | Win percentage: 46.8%\n","Epoch: 4340/10000 | Mean size 10: 7.6 | Longest 10: 014 | Mean steps 10: 64.0 | Wins: 2036 | Win percentage: 46.9%\n","Epoch: 4350/10000 | Mean size 10: 6.3 | Longest 10: 012 | Mean steps 10: 47.8 | Wins: 2046 | Win percentage: 47.0%\n","Epoch: 4360/10000 | Mean size 10: 6.0 | Longest 10: 011 | Mean steps 10: 35.1 | Wins: 2056 | Win percentage: 47.2%\n","Epoch: 4370/10000 | Mean size 10: 5.5 | Longest 10: 007 | Mean steps 10: 32.1 | Wins: 2066 | Win percentage: 47.3%\n","Epoch: 4380/10000 | Mean size 10: 8.8 | Longest 10: 012 | Mean steps 10: 70.3 | Wins: 2076 | Win percentage: 47.4%\n","Epoch: 4390/10000 | Mean size 10: 7.7 | Longest 10: 011 | Mean steps 10: 71.0 | Wins: 2086 | Win percentage: 47.5%\n","Epoch: 4400/10000 | Mean size 10: 7.2 | Longest 10: 014 | Mean steps 10: 55.1 | Wins: 2095 | Win percentage: 47.6%\n","Epoch: 4410/10000 | Mean size 10: 7.0 | Longest 10: 010 | Mean steps 10: 46.1 | Wins: 2105 | Win percentage: 47.7%\n","Epoch: 4420/10000 | Mean size 10: 7.1 | Longest 10: 010 | Mean steps 10: 45.2 | Wins: 2114 | Win percentage: 47.8%\n","Epoch: 4430/10000 | Mean size 10: 5.6 | Longest 10: 009 | Mean steps 10: 49.4 | Wins: 2124 | Win percentage: 47.9%\n","Epoch: 4440/10000 | Mean size 10: 7.0 | Longest 10: 011 | Mean steps 10: 48.9 | Wins: 2134 | Win percentage: 48.1%\n","Epoch: 4450/10000 | Mean size 10: 6.5 | Longest 10: 011 | Mean steps 10: 45.4 | Wins: 2144 | Win percentage: 48.2%\n","Epoch: 4460/10000 | Mean size 10: 8.5 | Longest 10: 015 | Mean steps 10: 66.5 | Wins: 2153 | Win percentage: 48.3%\n","Epoch: 4470/10000 | Mean size 10: 8.4 | Longest 10: 013 | Mean steps 10: 64.0 | Wins: 2162 | Win percentage: 48.4%\n","Epoch: 4480/10000 | Mean size 10: 6.8 | Longest 10: 013 | Mean steps 10: 43.4 | Wins: 2171 | Win percentage: 48.5%\n","Epoch: 4490/10000 | Mean size 10: 8.3 | Longest 10: 011 | Mean steps 10: 102.1 | Wins: 2181 | Win percentage: 48.6%\n","Epoch: 4500/10000 | Mean size 10: 8.4 | Longest 10: 014 | Mean steps 10: 85.7 | Wins: 2191 | Win percentage: 48.7%\n","Epoch: 4510/10000 | Mean size 10: 8.0 | Longest 10: 012 | Mean steps 10: 62.0 | Wins: 2201 | Win percentage: 48.8%\n","Epoch: 4520/10000 | Mean size 10: 7.4 | Longest 10: 013 | Mean steps 10: 56.2 | Wins: 2210 | Win percentage: 48.9%\n","Epoch: 4530/10000 | Mean size 10: 6.1 | Longest 10: 010 | Mean steps 10: 41.3 | Wins: 2218 | Win percentage: 49.0%\n","Epoch: 4540/10000 | Mean size 10: 7.1 | Longest 10: 011 | Mean steps 10: 49.0 | Wins: 2227 | Win percentage: 49.1%\n","Epoch: 4550/10000 | Mean size 10: 7.0 | Longest 10: 014 | Mean steps 10: 56.3 | Wins: 2237 | Win percentage: 49.2%\n","Epoch: 4560/10000 | Mean size 10: 7.2 | Longest 10: 013 | Mean steps 10: 48.1 | Wins: 2247 | Win percentage: 49.3%\n","Epoch: 4570/10000 | Mean size 10: 8.6 | Longest 10: 014 | Mean steps 10: 73.6 | Wins: 2257 | Win percentage: 49.4%\n","Epoch: 4580/10000 | Mean size 10: 8.3 | Longest 10: 012 | Mean steps 10: 56.5 | Wins: 2267 | Win percentage: 49.5%\n","Epoch: 4590/10000 | Mean size 10: 7.9 | Longest 10: 013 | Mean steps 10: 67.4 | Wins: 2277 | Win percentage: 49.6%\n","Epoch: 4600/10000 | Mean size 10: 7.3 | Longest 10: 013 | Mean steps 10: 52.6 | Wins: 2287 | Win percentage: 49.7%\n","Epoch: 4610/10000 | Mean size 10: 8.3 | Longest 10: 014 | Mean steps 10: 72.7 | Wins: 2297 | Win percentage: 49.8%\n","Epoch: 4620/10000 | Mean size 10: 7.4 | Longest 10: 010 | Mean steps 10: 51.0 | Wins: 2307 | Win percentage: 49.9%\n","Epoch: 4630/10000 | Mean size 10: 7.0 | Longest 10: 009 | Mean steps 10: 44.9 | Wins: 2316 | Win percentage: 50.0%\n","Epoch: 4640/10000 | Mean size 10: 9.3 | Longest 10: 016 | Mean steps 10: 76.9 | Wins: 2325 | Win percentage: 50.1%\n","Epoch: 4650/10000 | Mean size 10: 8.3 | Longest 10: 012 | Mean steps 10: 64.3 | Wins: 2335 | Win percentage: 50.2%\n","Epoch: 4660/10000 | Mean size 10: 8.7 | Longest 10: 016 | Mean steps 10: 69.6 | Wins: 2345 | Win percentage: 50.3%\n","Epoch: 4670/10000 | Mean size 10: 9.6 | Longest 10: 015 | Mean steps 10: 87.3 | Wins: 2355 | Win percentage: 50.4%\n","Epoch: 4680/10000 | Mean size 10: 7.3 | Longest 10: 013 | Mean steps 10: 49.6 | Wins: 2364 | Win percentage: 50.5%\n","Epoch: 4690/10000 | Mean size 10: 10.3 | Longest 10: 016 | Mean steps 10: 88.6 | Wins: 2374 | Win percentage: 50.6%\n","Epoch: 4700/10000 | Mean size 10: 9.9 | Longest 10: 014 | Mean steps 10: 107.0 | Wins: 2384 | Win percentage: 50.7%\n","Epoch: 4710/10000 | Mean size 10: 7.4 | Longest 10: 015 | Mean steps 10: 59.1 | Wins: 2393 | Win percentage: 50.8%\n","Epoch: 4720/10000 | Mean size 10: 9.4 | Longest 10: 014 | Mean steps 10: 89.3 | Wins: 2403 | Win percentage: 50.9%\n","Epoch: 4730/10000 | Mean size 10: 6.8 | Longest 10: 011 | Mean steps 10: 43.8 | Wins: 2413 | Win percentage: 51.0%\n","Epoch: 4740/10000 | Mean size 10: 6.3 | Longest 10: 010 | Mean steps 10: 44.3 | Wins: 2422 | Win percentage: 51.1%\n","Epoch: 4750/10000 | Mean size 10: 9.9 | Longest 10: 016 | Mean steps 10: 93.4 | Wins: 2432 | Win percentage: 51.2%\n","Epoch: 4760/10000 | Mean size 10: 10.5 | Longest 10: 017 | Mean steps 10: 97.7 | Wins: 2442 | Win percentage: 51.3%\n","Epoch: 4770/10000 | Mean size 10: 8.1 | Longest 10: 013 | Mean steps 10: 61.2 | Wins: 2451 | Win percentage: 51.4%\n","Epoch: 4780/10000 | Mean size 10: 9.2 | Longest 10: 015 | Mean steps 10: 87.9 | Wins: 2461 | Win percentage: 51.5%\n","Epoch: 4790/10000 | Mean size 10: 9.8 | Longest 10: 014 | Mean steps 10: 84.0 | Wins: 2471 | Win percentage: 51.6%\n","Epoch: 4800/10000 | Mean size 10: 9.4 | Longest 10: 016 | Mean steps 10: 77.5 | Wins: 2481 | Win percentage: 51.7%\n","Epoch: 4810/10000 | Mean size 10: 11.0 | Longest 10: 015 | Mean steps 10: 105.0 | Wins: 2491 | Win percentage: 51.8%\n","Epoch: 4820/10000 | Mean size 10: 8.0 | Longest 10: 012 | Mean steps 10: 71.5 | Wins: 2501 | Win percentage: 51.9%\n","Epoch: 4830/10000 | Mean size 10: 10.0 | Longest 10: 016 | Mean steps 10: 79.9 | Wins: 2511 | Win percentage: 52.0%\n","Epoch: 4840/10000 | Mean size 10: 9.5 | Longest 10: 014 | Mean steps 10: 79.8 | Wins: 2521 | Win percentage: 52.1%\n","Epoch: 4850/10000 | Mean size 10: 9.2 | Longest 10: 012 | Mean steps 10: 78.9 | Wins: 2531 | Win percentage: 52.2%\n","Epoch: 4860/10000 | Mean size 10: 8.7 | Longest 10: 014 | Mean steps 10: 72.2 | Wins: 2541 | Win percentage: 52.3%\n","Epoch: 4870/10000 | Mean size 10: 10.8 | Longest 10: 017 | Mean steps 10: 114.3 | Wins: 2551 | Win percentage: 52.4%\n","Epoch: 4880/10000 | Mean size 10: 9.2 | Longest 10: 013 | Mean steps 10: 78.4 | Wins: 2561 | Win percentage: 52.5%\n","Epoch: 4890/10000 | Mean size 10: 12.4 | Longest 10: 018 | Mean steps 10: 122.2 | Wins: 2571 | Win percentage: 52.6%\n","Epoch: 4900/10000 | Mean size 10: 12.8 | Longest 10: 017 | Mean steps 10: 136.0 | Wins: 2581 | Win percentage: 52.7%\n","Epoch: 4910/10000 | Mean size 10: 9.7 | Longest 10: 015 | Mean steps 10: 83.3 | Wins: 2590 | Win percentage: 52.7%\n","Epoch: 4920/10000 | Mean size 10: 11.9 | Longest 10: 015 | Mean steps 10: 103.3 | Wins: 2600 | Win percentage: 52.8%\n","Epoch: 4930/10000 | Mean size 10: 9.5 | Longest 10: 015 | Mean steps 10: 81.1 | Wins: 2610 | Win percentage: 52.9%\n","Epoch: 4940/10000 | Mean size 10: 11.1 | Longest 10: 015 | Mean steps 10: 94.7 | Wins: 2620 | Win percentage: 53.0%\n","Epoch: 4950/10000 | Mean size 10: 8.9 | Longest 10: 015 | Mean steps 10: 72.2 | Wins: 2630 | Win percentage: 53.1%\n","Epoch: 4960/10000 | Mean size 10: 12.5 | Longest 10: 020 | Mean steps 10: 136.5 | Wins: 2640 | Win percentage: 53.2%\n","Epoch: 4970/10000 | Mean size 10: 11.6 | Longest 10: 017 | Mean steps 10: 120.2 | Wins: 2650 | Win percentage: 53.3%\n","Epoch: 4980/10000 | Mean size 10: 12.6 | Longest 10: 017 | Mean steps 10: 116.6 | Wins: 2660 | Win percentage: 53.4%\n","Epoch: 4990/10000 | Mean size 10: 9.7 | Longest 10: 015 | Mean steps 10: 87.6 | Wins: 2670 | Win percentage: 53.5%\n","Epoch: 5000/10000 | Mean size 10: 11.9 | Longest 10: 017 | Mean steps 10: 116.2 | Wins: 2680 | Win percentage: 53.6%\n","Epoch: 5010/10000 | Mean size 10: 10.0 | Longest 10: 013 | Mean steps 10: 84.5 | Wins: 2690 | Win percentage: 53.7%\n","Epoch: 5020/10000 | Mean size 10: 12.9 | Longest 10: 018 | Mean steps 10: 134.7 | Wins: 2700 | Win percentage: 53.8%\n","Epoch: 5030/10000 | Mean size 10: 10.2 | Longest 10: 015 | Mean steps 10: 103.9 | Wins: 2709 | Win percentage: 53.9%\n","Epoch: 5040/10000 | Mean size 10: 12.5 | Longest 10: 020 | Mean steps 10: 119.9 | Wins: 2719 | Win percentage: 53.9%\n","Epoch: 5050/10000 | Mean size 10: 10.5 | Longest 10: 014 | Mean steps 10: 90.8 | Wins: 2729 | Win percentage: 54.0%\n","Epoch: 5060/10000 | Mean size 10: 11.2 | Longest 10: 014 | Mean steps 10: 103.8 | Wins: 2739 | Win percentage: 54.1%\n","Epoch: 5070/10000 | Mean size 10: 11.0 | Longest 10: 015 | Mean steps 10: 95.3 | Wins: 2749 | Win percentage: 54.2%\n","Epoch: 5080/10000 | Mean size 10: 11.6 | Longest 10: 014 | Mean steps 10: 100.5 | Wins: 2759 | Win percentage: 54.3%\n","Epoch: 5090/10000 | Mean size 10: 13.6 | Longest 10: 018 | Mean steps 10: 144.5 | Wins: 2769 | Win percentage: 54.4%\n","Epoch: 5100/10000 | Mean size 10: 12.8 | Longest 10: 020 | Mean steps 10: 118.4 | Wins: 2779 | Win percentage: 54.5%\n","Epoch: 5110/10000 | Mean size 10: 12.0 | Longest 10: 018 | Mean steps 10: 121.9 | Wins: 2789 | Win percentage: 54.6%\n","Epoch: 5120/10000 | Mean size 10: 12.0 | Longest 10: 017 | Mean steps 10: 119.0 | Wins: 2799 | Win percentage: 54.7%\n","Epoch: 5130/10000 | Mean size 10: 13.0 | Longest 10: 022 | Mean steps 10: 130.6 | Wins: 2809 | Win percentage: 54.8%\n","Epoch: 5140/10000 | Mean size 10: 12.4 | Longest 10: 017 | Mean steps 10: 115.1 | Wins: 2819 | Win percentage: 54.8%\n","Epoch: 5150/10000 | Mean size 10: 14.2 | Longest 10: 020 | Mean steps 10: 167.8 | Wins: 2829 | Win percentage: 54.9%\n","Epoch: 5160/10000 | Mean size 10: 13.6 | Longest 10: 018 | Mean steps 10: 117.9 | Wins: 2839 | Win percentage: 55.0%\n","Epoch: 5170/10000 | Mean size 10: 12.9 | Longest 10: 017 | Mean steps 10: 112.6 | Wins: 2849 | Win percentage: 55.1%\n","Epoch: 5180/10000 | Mean size 10: 12.3 | Longest 10: 018 | Mean steps 10: 101.8 | Wins: 2859 | Win percentage: 55.2%\n","Epoch: 5190/10000 | Mean size 10: 11.5 | Longest 10: 015 | Mean steps 10: 109.3 | Wins: 2869 | Win percentage: 55.3%\n","Epoch: 5200/10000 | Mean size 10: 11.4 | Longest 10: 016 | Mean steps 10: 106.0 | Wins: 2879 | Win percentage: 55.4%\n","Epoch: 5210/10000 | Mean size 10: 12.1 | Longest 10: 019 | Mean steps 10: 98.1 | Wins: 2889 | Win percentage: 55.5%\n","Epoch: 5220/10000 | Mean size 10: 12.9 | Longest 10: 019 | Mean steps 10: 121.6 | Wins: 2899 | Win percentage: 55.5%\n","Epoch: 5230/10000 | Mean size 10: 12.2 | Longest 10: 021 | Mean steps 10: 96.3 | Wins: 2909 | Win percentage: 55.6%\n","Epoch: 5240/10000 | Mean size 10: 12.6 | Longest 10: 019 | Mean steps 10: 114.6 | Wins: 2919 | Win percentage: 55.7%\n","Epoch: 5250/10000 | Mean size 10: 12.1 | Longest 10: 020 | Mean steps 10: 106.6 | Wins: 2929 | Win percentage: 55.8%\n","Epoch: 5260/10000 | Mean size 10: 12.1 | Longest 10: 017 | Mean steps 10: 110.8 | Wins: 2939 | Win percentage: 55.9%\n","Epoch: 5270/10000 | Mean size 10: 11.9 | Longest 10: 021 | Mean steps 10: 121.0 | Wins: 2949 | Win percentage: 56.0%\n","Epoch: 5280/10000 | Mean size 10: 12.6 | Longest 10: 017 | Mean steps 10: 112.1 | Wins: 2959 | Win percentage: 56.0%\n","Epoch: 5290/10000 | Mean size 10: 14.0 | Longest 10: 018 | Mean steps 10: 144.7 | Wins: 2969 | Win percentage: 56.1%\n","Epoch: 5300/10000 | Mean size 10: 9.4 | Longest 10: 014 | Mean steps 10: 79.0 | Wins: 2979 | Win percentage: 56.2%\n","Epoch: 5310/10000 | Mean size 10: 13.2 | Longest 10: 025 | Mean steps 10: 133.5 | Wins: 2989 | Win percentage: 56.3%\n","Epoch: 5320/10000 | Mean size 10: 12.3 | Longest 10: 020 | Mean steps 10: 118.4 | Wins: 2999 | Win percentage: 56.4%\n","Epoch: 5330/10000 | Mean size 10: 13.2 | Longest 10: 020 | Mean steps 10: 125.6 | Wins: 3009 | Win percentage: 56.5%\n","Epoch: 5340/10000 | Mean size 10: 13.8 | Longest 10: 023 | Mean steps 10: 134.7 | Wins: 3019 | Win percentage: 56.5%\n","Epoch: 5350/10000 | Mean size 10: 14.0 | Longest 10: 021 | Mean steps 10: 131.2 | Wins: 3029 | Win percentage: 56.6%\n","Epoch: 5360/10000 | Mean size 10: 12.3 | Longest 10: 019 | Mean steps 10: 112.1 | Wins: 3039 | Win percentage: 56.7%\n","Epoch: 5370/10000 | Mean size 10: 12.0 | Longest 10: 019 | Mean steps 10: 114.0 | Wins: 3049 | Win percentage: 56.8%\n","Epoch: 5380/10000 | Mean size 10: 14.2 | Longest 10: 021 | Mean steps 10: 139.2 | Wins: 3059 | Win percentage: 56.9%\n","Epoch: 5390/10000 | Mean size 10: 13.9 | Longest 10: 020 | Mean steps 10: 136.0 | Wins: 3069 | Win percentage: 56.9%\n","Epoch: 5400/10000 | Mean size 10: 12.4 | Longest 10: 018 | Mean steps 10: 113.7 | Wins: 3079 | Win percentage: 57.0%\n","Epoch: 5410/10000 | Mean size 10: 12.7 | Longest 10: 016 | Mean steps 10: 134.4 | Wins: 3089 | Win percentage: 57.1%\n","Epoch: 5420/10000 | Mean size 10: 14.2 | Longest 10: 019 | Mean steps 10: 129.8 | Wins: 3099 | Win percentage: 57.2%\n","Epoch: 5430/10000 | Mean size 10: 13.9 | Longest 10: 023 | Mean steps 10: 152.2 | Wins: 3109 | Win percentage: 57.3%\n","Epoch: 5440/10000 | Mean size 10: 14.3 | Longest 10: 021 | Mean steps 10: 136.1 | Wins: 3119 | Win percentage: 57.3%\n","Epoch: 5450/10000 | Mean size 10: 14.6 | Longest 10: 023 | Mean steps 10: 139.5 | Wins: 3129 | Win percentage: 57.4%\n","Epoch: 5460/10000 | Mean size 10: 12.3 | Longest 10: 022 | Mean steps 10: 116.1 | Wins: 3139 | Win percentage: 57.5%\n","Epoch: 5470/10000 | Mean size 10: 13.9 | Longest 10: 017 | Mean steps 10: 138.4 | Wins: 3149 | Win percentage: 57.6%\n","Epoch: 5480/10000 | Mean size 10: 16.2 | Longest 10: 023 | Mean steps 10: 184.6 | Wins: 3159 | Win percentage: 57.6%\n","Epoch: 5490/10000 | Mean size 10: 14.0 | Longest 10: 020 | Mean steps 10: 124.8 | Wins: 3169 | Win percentage: 57.7%\n","Epoch: 5500/10000 | Mean size 10: 13.9 | Longest 10: 022 | Mean steps 10: 126.0 | Wins: 3178 | Win percentage: 57.8%\n","Epoch: 5510/10000 | Mean size 10: 14.9 | Longest 10: 020 | Mean steps 10: 163.6 | Wins: 3188 | Win percentage: 57.9%\n","Epoch: 5520/10000 | Mean size 10: 14.2 | Longest 10: 021 | Mean steps 10: 131.3 | Wins: 3198 | Win percentage: 57.9%\n","Epoch: 5530/10000 | Mean size 10: 13.4 | Longest 10: 021 | Mean steps 10: 130.5 | Wins: 3208 | Win percentage: 58.0%\n","Epoch: 5540/10000 | Mean size 10: 14.5 | Longest 10: 021 | Mean steps 10: 131.0 | Wins: 3218 | Win percentage: 58.1%\n","Epoch: 5550/10000 | Mean size 10: 13.9 | Longest 10: 019 | Mean steps 10: 138.3 | Wins: 3228 | Win percentage: 58.2%\n","Epoch: 5560/10000 | Mean size 10: 14.9 | Longest 10: 019 | Mean steps 10: 133.6 | Wins: 3238 | Win percentage: 58.2%\n","Epoch: 5570/10000 | Mean size 10: 15.9 | Longest 10: 023 | Mean steps 10: 166.5 | Wins: 3248 | Win percentage: 58.3%\n","Epoch: 5580/10000 | Mean size 10: 13.3 | Longest 10: 019 | Mean steps 10: 125.9 | Wins: 3258 | Win percentage: 58.4%\n","Epoch: 5590/10000 | Mean size 10: 15.5 | Longest 10: 021 | Mean steps 10: 149.0 | Wins: 3268 | Win percentage: 58.5%\n","Epoch: 5600/10000 | Mean size 10: 14.3 | Longest 10: 020 | Mean steps 10: 138.8 | Wins: 3278 | Win percentage: 58.5%\n","Epoch: 5610/10000 | Mean size 10: 13.6 | Longest 10: 020 | Mean steps 10: 112.6 | Wins: 3287 | Win percentage: 58.6%\n","Epoch: 5620/10000 | Mean size 10: 14.4 | Longest 10: 023 | Mean steps 10: 144.3 | Wins: 3297 | Win percentage: 58.7%\n","Epoch: 5630/10000 | Mean size 10: 14.7 | Longest 10: 022 | Mean steps 10: 146.9 | Wins: 3307 | Win percentage: 58.7%\n","Epoch: 5640/10000 | Mean size 10: 14.5 | Longest 10: 024 | Mean steps 10: 128.2 | Wins: 3317 | Win percentage: 58.8%\n","Epoch: 5650/10000 | Mean size 10: 14.6 | Longest 10: 023 | Mean steps 10: 145.1 | Wins: 3327 | Win percentage: 58.9%\n","Epoch: 5660/10000 | Mean size 10: 15.4 | Longest 10: 023 | Mean steps 10: 155.0 | Wins: 3337 | Win percentage: 59.0%\n","Epoch: 5670/10000 | Mean size 10: 15.7 | Longest 10: 027 | Mean steps 10: 171.4 | Wins: 3347 | Win percentage: 59.0%\n","Epoch: 5680/10000 | Mean size 10: 15.6 | Longest 10: 021 | Mean steps 10: 157.9 | Wins: 3357 | Win percentage: 59.1%\n","Epoch: 5690/10000 | Mean size 10: 13.1 | Longest 10: 018 | Mean steps 10: 130.2 | Wins: 3367 | Win percentage: 59.2%\n","Epoch: 5700/10000 | Mean size 10: 14.5 | Longest 10: 020 | Mean steps 10: 126.8 | Wins: 3377 | Win percentage: 59.2%\n","Epoch: 5710/10000 | Mean size 10: 14.7 | Longest 10: 023 | Mean steps 10: 154.9 | Wins: 3387 | Win percentage: 59.3%\n","Epoch: 5720/10000 | Mean size 10: 15.0 | Longest 10: 023 | Mean steps 10: 153.4 | Wins: 3397 | Win percentage: 59.4%\n","Epoch: 5730/10000 | Mean size 10: 15.8 | Longest 10: 022 | Mean steps 10: 153.4 | Wins: 3407 | Win percentage: 59.5%\n","Epoch: 5740/10000 | Mean size 10: 14.3 | Longest 10: 020 | Mean steps 10: 131.9 | Wins: 3417 | Win percentage: 59.5%\n","Epoch: 5750/10000 | Mean size 10: 14.3 | Longest 10: 020 | Mean steps 10: 137.3 | Wins: 3427 | Win percentage: 59.6%\n","Epoch: 5760/10000 | Mean size 10: 13.7 | Longest 10: 021 | Mean steps 10: 134.6 | Wins: 3437 | Win percentage: 59.7%\n","Epoch: 5770/10000 | Mean size 10: 11.9 | Longest 10: 019 | Mean steps 10: 93.7 | Wins: 3446 | Win percentage: 59.7%\n","Epoch: 5780/10000 | Mean size 10: 17.0 | Longest 10: 022 | Mean steps 10: 178.3 | Wins: 3456 | Win percentage: 59.8%\n","Epoch: 5790/10000 | Mean size 10: 15.6 | Longest 10: 020 | Mean steps 10: 138.7 | Wins: 3466 | Win percentage: 59.9%\n","Epoch: 5800/10000 | Mean size 10: 12.6 | Longest 10: 017 | Mean steps 10: 114.7 | Wins: 3476 | Win percentage: 59.9%\n","Epoch: 5810/10000 | Mean size 10: 16.5 | Longest 10: 022 | Mean steps 10: 159.4 | Wins: 3486 | Win percentage: 60.0%\n","Epoch: 5820/10000 | Mean size 10: 14.6 | Longest 10: 021 | Mean steps 10: 134.7 | Wins: 3496 | Win percentage: 60.1%\n","Epoch: 5830/10000 | Mean size 10: 15.4 | Longest 10: 023 | Mean steps 10: 140.0 | Wins: 3506 | Win percentage: 60.1%\n","Epoch: 5840/10000 | Mean size 10: 12.5 | Longest 10: 021 | Mean steps 10: 111.7 | Wins: 3516 | Win percentage: 60.2%\n","Epoch: 5850/10000 | Mean size 10: 13.7 | Longest 10: 022 | Mean steps 10: 145.9 | Wins: 3526 | Win percentage: 60.3%\n","Epoch: 5860/10000 | Mean size 10: 11.7 | Longest 10: 018 | Mean steps 10: 101.3 | Wins: 3536 | Win percentage: 60.3%\n","Epoch: 5870/10000 | Mean size 10: 13.6 | Longest 10: 019 | Mean steps 10: 115.3 | Wins: 3546 | Win percentage: 60.4%\n","Epoch: 5880/10000 | Mean size 10: 18.1 | Longest 10: 023 | Mean steps 10: 201.7 | Wins: 3556 | Win percentage: 60.5%\n","Epoch: 5890/10000 | Mean size 10: 13.8 | Longest 10: 022 | Mean steps 10: 133.5 | Wins: 3566 | Win percentage: 60.5%\n","Epoch: 5900/10000 | Mean size 10: 15.0 | Longest 10: 023 | Mean steps 10: 143.8 | Wins: 3576 | Win percentage: 60.6%\n","Epoch: 5910/10000 | Mean size 10: 16.1 | Longest 10: 023 | Mean steps 10: 154.1 | Wins: 3586 | Win percentage: 60.7%\n","Epoch: 5920/10000 | Mean size 10: 16.0 | Longest 10: 023 | Mean steps 10: 143.2 | Wins: 3596 | Win percentage: 60.7%\n","Epoch: 5930/10000 | Mean size 10: 16.5 | Longest 10: 023 | Mean steps 10: 186.5 | Wins: 3606 | Win percentage: 60.8%\n","Epoch: 5940/10000 | Mean size 10: 13.7 | Longest 10: 027 | Mean steps 10: 126.4 | Wins: 3616 | Win percentage: 60.9%\n","Epoch: 5950/10000 | Mean size 10: 16.3 | Longest 10: 021 | Mean steps 10: 160.3 | Wins: 3626 | Win percentage: 60.9%\n","Epoch: 5960/10000 | Mean size 10: 15.2 | Longest 10: 022 | Mean steps 10: 142.4 | Wins: 3636 | Win percentage: 61.0%\n","Epoch: 5970/10000 | Mean size 10: 14.0 | Longest 10: 026 | Mean steps 10: 116.8 | Wins: 3646 | Win percentage: 61.1%\n","Epoch: 5980/10000 | Mean size 10: 13.1 | Longest 10: 025 | Mean steps 10: 121.7 | Wins: 3656 | Win percentage: 61.1%\n","Epoch: 5990/10000 | Mean size 10: 15.1 | Longest 10: 021 | Mean steps 10: 147.2 | Wins: 3666 | Win percentage: 61.2%\n","Epoch: 6000/10000 | Mean size 10: 17.6 | Longest 10: 029 | Mean steps 10: 179.5 | Wins: 3676 | Win percentage: 61.3%\n","Epoch: 6010/10000 | Mean size 10: 15.2 | Longest 10: 021 | Mean steps 10: 146.8 | Wins: 3686 | Win percentage: 61.3%\n","Epoch: 6020/10000 | Mean size 10: 17.4 | Longest 10: 023 | Mean steps 10: 185.8 | Wins: 3696 | Win percentage: 61.4%\n","Epoch: 6030/10000 | Mean size 10: 14.7 | Longest 10: 021 | Mean steps 10: 145.6 | Wins: 3706 | Win percentage: 61.5%\n","Epoch: 6040/10000 | Mean size 10: 16.8 | Longest 10: 027 | Mean steps 10: 158.5 | Wins: 3716 | Win percentage: 61.5%\n","Epoch: 6050/10000 | Mean size 10: 14.1 | Longest 10: 024 | Mean steps 10: 131.9 | Wins: 3726 | Win percentage: 61.6%\n","Epoch: 6060/10000 | Mean size 10: 11.3 | Longest 10: 014 | Mean steps 10: 95.9 | Wins: 3736 | Win percentage: 61.7%\n","Epoch: 6070/10000 | Mean size 10: 15.2 | Longest 10: 023 | Mean steps 10: 150.5 | Wins: 3746 | Win percentage: 61.7%\n","Epoch: 6080/10000 | Mean size 10: 14.4 | Longest 10: 019 | Mean steps 10: 158.8 | Wins: 3756 | Win percentage: 61.8%\n","Epoch: 6090/10000 | Mean size 10: 13.3 | Longest 10: 021 | Mean steps 10: 121.8 | Wins: 3766 | Win percentage: 61.8%\n","Epoch: 6100/10000 | Mean size 10: 17.3 | Longest 10: 024 | Mean steps 10: 173.9 | Wins: 3776 | Win percentage: 61.9%\n","Epoch: 6110/10000 | Mean size 10: 11.4 | Longest 10: 018 | Mean steps 10: 96.1 | Wins: 3786 | Win percentage: 62.0%\n","Epoch: 6120/10000 | Mean size 10: 13.7 | Longest 10: 022 | Mean steps 10: 124.3 | Wins: 3796 | Win percentage: 62.0%\n","Epoch: 6130/10000 | Mean size 10: 16.6 | Longest 10: 022 | Mean steps 10: 156.3 | Wins: 3806 | Win percentage: 62.1%\n","Epoch: 6140/10000 | Mean size 10: 14.7 | Longest 10: 022 | Mean steps 10: 136.6 | Wins: 3816 | Win percentage: 62.1%\n","Epoch: 6150/10000 | Mean size 10: 16.1 | Longest 10: 025 | Mean steps 10: 164.2 | Wins: 3826 | Win percentage: 62.2%\n","Epoch: 6160/10000 | Mean size 10: 13.4 | Longest 10: 022 | Mean steps 10: 123.0 | Wins: 3836 | Win percentage: 62.3%\n","Epoch: 6170/10000 | Mean size 10: 13.5 | Longest 10: 025 | Mean steps 10: 123.5 | Wins: 3846 | Win percentage: 62.3%\n","Epoch: 6180/10000 | Mean size 10: 15.0 | Longest 10: 022 | Mean steps 10: 151.4 | Wins: 3856 | Win percentage: 62.4%\n","Epoch: 6190/10000 | Mean size 10: 15.9 | Longest 10: 031 | Mean steps 10: 153.9 | Wins: 3866 | Win percentage: 62.5%\n","Epoch: 6200/10000 | Mean size 10: 19.0 | Longest 10: 024 | Mean steps 10: 201.8 | Wins: 3876 | Win percentage: 62.5%\n","Epoch: 6210/10000 | Mean size 10: 17.8 | Longest 10: 026 | Mean steps 10: 178.2 | Wins: 3886 | Win percentage: 62.6%\n","Epoch: 6220/10000 | Mean size 10: 16.4 | Longest 10: 020 | Mean steps 10: 162.0 | Wins: 3896 | Win percentage: 62.6%\n","Epoch: 6230/10000 | Mean size 10: 18.6 | Longest 10: 025 | Mean steps 10: 225.7 | Wins: 3906 | Win percentage: 62.7%\n","Epoch: 6240/10000 | Mean size 10: 15.9 | Longest 10: 025 | Mean steps 10: 152.7 | Wins: 3916 | Win percentage: 62.8%\n","Epoch: 6250/10000 | Mean size 10: 15.6 | Longest 10: 026 | Mean steps 10: 134.2 | Wins: 3926 | Win percentage: 62.8%\n","Epoch: 6260/10000 | Mean size 10: 15.4 | Longest 10: 023 | Mean steps 10: 137.2 | Wins: 3936 | Win percentage: 62.9%\n","Epoch: 6270/10000 | Mean size 10: 16.2 | Longest 10: 025 | Mean steps 10: 148.9 | Wins: 3946 | Win percentage: 62.9%\n","Epoch: 6280/10000 | Mean size 10: 14.9 | Longest 10: 023 | Mean steps 10: 144.3 | Wins: 3956 | Win percentage: 63.0%\n","Epoch: 6290/10000 | Mean size 10: 15.6 | Longest 10: 030 | Mean steps 10: 143.0 | Wins: 3966 | Win percentage: 63.1%\n","Epoch: 6300/10000 | Mean size 10: 16.0 | Longest 10: 022 | Mean steps 10: 142.4 | Wins: 3976 | Win percentage: 63.1%\n","Epoch: 6310/10000 | Mean size 10: 14.1 | Longest 10: 024 | Mean steps 10: 126.5 | Wins: 3986 | Win percentage: 63.2%\n","Epoch: 6320/10000 | Mean size 10: 16.9 | Longest 10: 023 | Mean steps 10: 172.0 | Wins: 3996 | Win percentage: 63.2%\n","Epoch: 6330/10000 | Mean size 10: 18.6 | Longest 10: 023 | Mean steps 10: 197.1 | Wins: 4006 | Win percentage: 63.3%\n","Epoch: 6340/10000 | Mean size 10: 14.2 | Longest 10: 023 | Mean steps 10: 122.1 | Wins: 4016 | Win percentage: 63.3%\n","Epoch: 6350/10000 | Mean size 10: 16.9 | Longest 10: 023 | Mean steps 10: 139.2 | Wins: 4026 | Win percentage: 63.4%\n","Epoch: 6360/10000 | Mean size 10: 19.0 | Longest 10: 023 | Mean steps 10: 183.8 | Wins: 4036 | Win percentage: 63.5%\n","Epoch: 6370/10000 | Mean size 10: 15.7 | Longest 10: 027 | Mean steps 10: 133.6 | Wins: 4046 | Win percentage: 63.5%\n","Epoch: 6380/10000 | Mean size 10: 13.7 | Longest 10: 026 | Mean steps 10: 130.5 | Wins: 4056 | Win percentage: 63.6%\n","Epoch: 6390/10000 | Mean size 10: 15.6 | Longest 10: 024 | Mean steps 10: 150.0 | Wins: 4066 | Win percentage: 63.6%\n","Epoch: 6400/10000 | Mean size 10: 15.5 | Longest 10: 024 | Mean steps 10: 139.4 | Wins: 4076 | Win percentage: 63.7%\n","Epoch: 6410/10000 | Mean size 10: 17.1 | Longest 10: 027 | Mean steps 10: 178.2 | Wins: 4086 | Win percentage: 63.7%\n","Epoch: 6420/10000 | Mean size 10: 14.4 | Longest 10: 026 | Mean steps 10: 123.4 | Wins: 4096 | Win percentage: 63.8%\n","Epoch: 6430/10000 | Mean size 10: 14.4 | Longest 10: 025 | Mean steps 10: 115.7 | Wins: 4106 | Win percentage: 63.9%\n","Epoch: 6440/10000 | Mean size 10: 16.3 | Longest 10: 024 | Mean steps 10: 152.8 | Wins: 4116 | Win percentage: 63.9%\n","Epoch: 6450/10000 | Mean size 10: 15.4 | Longest 10: 023 | Mean steps 10: 132.8 | Wins: 4126 | Win percentage: 64.0%\n","Epoch: 6460/10000 | Mean size 10: 13.2 | Longest 10: 023 | Mean steps 10: 119.4 | Wins: 4136 | Win percentage: 64.0%\n","Epoch: 6470/10000 | Mean size 10: 14.3 | Longest 10: 021 | Mean steps 10: 122.9 | Wins: 4146 | Win percentage: 64.1%\n","Epoch: 6480/10000 | Mean size 10: 15.1 | Longest 10: 020 | Mean steps 10: 139.6 | Wins: 4156 | Win percentage: 64.1%\n","Epoch: 6490/10000 | Mean size 10: 16.4 | Longest 10: 026 | Mean steps 10: 172.6 | Wins: 4166 | Win percentage: 64.2%\n","Epoch: 6500/10000 | Mean size 10: 14.3 | Longest 10: 025 | Mean steps 10: 104.6 | Wins: 4176 | Win percentage: 64.2%\n","Epoch: 6510/10000 | Mean size 10: 15.5 | Longest 10: 030 | Mean steps 10: 149.8 | Wins: 4186 | Win percentage: 64.3%\n","Epoch: 6520/10000 | Mean size 10: 18.9 | Longest 10: 024 | Mean steps 10: 179.4 | Wins: 4196 | Win percentage: 64.4%\n","Epoch: 6530/10000 | Mean size 10: 17.5 | Longest 10: 027 | Mean steps 10: 177.7 | Wins: 4206 | Win percentage: 64.4%\n","Epoch: 6540/10000 | Mean size 10: 18.8 | Longest 10: 026 | Mean steps 10: 174.9 | Wins: 4216 | Win percentage: 64.5%\n","Epoch: 6550/10000 | Mean size 10: 16.9 | Longest 10: 024 | Mean steps 10: 149.7 | Wins: 4226 | Win percentage: 64.5%\n","Epoch: 6560/10000 | Mean size 10: 10.8 | Longest 10: 020 | Mean steps 10: 80.8 | Wins: 4236 | Win percentage: 64.6%\n","Epoch: 6570/10000 | Mean size 10: 14.6 | Longest 10: 023 | Mean steps 10: 125.0 | Wins: 4245 | Win percentage: 64.6%\n","Epoch: 6580/10000 | Mean size 10: 16.9 | Longest 10: 024 | Mean steps 10: 153.0 | Wins: 4255 | Win percentage: 64.7%\n","Epoch: 6590/10000 | Mean size 10: 15.8 | Longest 10: 026 | Mean steps 10: 130.3 | Wins: 4265 | Win percentage: 64.7%\n","Epoch: 6600/10000 | Mean size 10: 15.8 | Longest 10: 025 | Mean steps 10: 134.0 | Wins: 4275 | Win percentage: 64.8%\n","Epoch: 6610/10000 | Mean size 10: 17.0 | Longest 10: 023 | Mean steps 10: 152.7 | Wins: 4285 | Win percentage: 64.8%\n","Epoch: 6620/10000 | Mean size 10: 16.7 | Longest 10: 022 | Mean steps 10: 162.6 | Wins: 4295 | Win percentage: 64.9%\n","Epoch: 6630/10000 | Mean size 10: 18.4 | Longest 10: 029 | Mean steps 10: 188.6 | Wins: 4305 | Win percentage: 64.9%\n","Epoch: 6640/10000 | Mean size 10: 16.5 | Longest 10: 026 | Mean steps 10: 139.6 | Wins: 4315 | Win percentage: 65.0%\n","Epoch: 6650/10000 | Mean size 10: 12.4 | Longest 10: 019 | Mean steps 10: 96.0 | Wins: 4325 | Win percentage: 65.0%\n","Epoch: 6660/10000 | Mean size 10: 18.2 | Longest 10: 027 | Mean steps 10: 181.0 | Wins: 4335 | Win percentage: 65.1%\n","Epoch: 6670/10000 | Mean size 10: 17.5 | Longest 10: 025 | Mean steps 10: 175.0 | Wins: 4345 | Win percentage: 65.1%\n","Epoch: 6680/10000 | Mean size 10: 17.4 | Longest 10: 023 | Mean steps 10: 150.0 | Wins: 4355 | Win percentage: 65.2%\n","Epoch: 6690/10000 | Mean size 10: 15.1 | Longest 10: 023 | Mean steps 10: 123.9 | Wins: 4365 | Win percentage: 65.2%\n","Epoch: 6700/10000 | Mean size 10: 16.9 | Longest 10: 027 | Mean steps 10: 146.7 | Wins: 4375 | Win percentage: 65.3%\n","Epoch: 6710/10000 | Mean size 10: 14.4 | Longest 10: 025 | Mean steps 10: 124.0 | Wins: 4385 | Win percentage: 65.4%\n","Epoch: 6720/10000 | Mean size 10: 17.9 | Longest 10: 029 | Mean steps 10: 152.4 | Wins: 4395 | Win percentage: 65.4%\n","Epoch: 6730/10000 | Mean size 10: 17.5 | Longest 10: 028 | Mean steps 10: 169.2 | Wins: 4405 | Win percentage: 65.5%\n","Epoch: 6740/10000 | Mean size 10: 16.3 | Longest 10: 023 | Mean steps 10: 147.7 | Wins: 4415 | Win percentage: 65.5%\n","Epoch: 6750/10000 | Mean size 10: 13.3 | Longest 10: 021 | Mean steps 10: 99.3 | Wins: 4425 | Win percentage: 65.6%\n","Epoch: 6760/10000 | Mean size 10: 16.0 | Longest 10: 030 | Mean steps 10: 143.6 | Wins: 4435 | Win percentage: 65.6%\n","Epoch: 6770/10000 | Mean size 10: 17.9 | Longest 10: 026 | Mean steps 10: 162.3 | Wins: 4445 | Win percentage: 65.7%\n","Epoch: 6780/10000 | Mean size 10: 13.5 | Longest 10: 026 | Mean steps 10: 107.4 | Wins: 4455 | Win percentage: 65.7%\n","Epoch: 6790/10000 | Mean size 10: 14.8 | Longest 10: 021 | Mean steps 10: 125.0 | Wins: 4465 | Win percentage: 65.8%\n","Epoch: 6800/10000 | Mean size 10: 18.4 | Longest 10: 034 | Mean steps 10: 165.3 | Wins: 4475 | Win percentage: 65.8%\n","Epoch: 6810/10000 | Mean size 10: 15.4 | Longest 10: 028 | Mean steps 10: 141.8 | Wins: 4485 | Win percentage: 65.9%\n","Epoch: 6820/10000 | Mean size 10: 18.6 | Longest 10: 031 | Mean steps 10: 173.7 | Wins: 4495 | Win percentage: 65.9%\n","Epoch: 6830/10000 | Mean size 10: 15.0 | Longest 10: 019 | Mean steps 10: 119.9 | Wins: 4505 | Win percentage: 66.0%\n","Epoch: 6840/10000 | Mean size 10: 19.2 | Longest 10: 030 | Mean steps 10: 178.3 | Wins: 4515 | Win percentage: 66.0%\n","Epoch: 6850/10000 | Mean size 10: 16.0 | Longest 10: 025 | Mean steps 10: 140.8 | Wins: 4525 | Win percentage: 66.1%\n","Epoch: 6860/10000 | Mean size 10: 14.3 | Longest 10: 026 | Mean steps 10: 136.4 | Wins: 4535 | Win percentage: 66.1%\n","Epoch: 6870/10000 | Mean size 10: 10.8 | Longest 10: 016 | Mean steps 10: 75.7 | Wins: 4545 | Win percentage: 66.2%\n","Epoch: 6880/10000 | Mean size 10: 16.1 | Longest 10: 024 | Mean steps 10: 147.7 | Wins: 4555 | Win percentage: 66.2%\n","Epoch: 6890/10000 | Mean size 10: 18.5 | Longest 10: 028 | Mean steps 10: 185.9 | Wins: 4565 | Win percentage: 66.3%\n","Epoch: 6900/10000 | Mean size 10: 20.0 | Longest 10: 027 | Mean steps 10: 183.6 | Wins: 4575 | Win percentage: 66.3%\n","Epoch: 6910/10000 | Mean size 10: 19.7 | Longest 10: 029 | Mean steps 10: 181.2 | Wins: 4585 | Win percentage: 66.4%\n","Epoch: 6920/10000 | Mean size 10: 14.1 | Longest 10: 024 | Mean steps 10: 120.2 | Wins: 4595 | Win percentage: 66.4%\n","Epoch: 6930/10000 | Mean size 10: 17.0 | Longest 10: 025 | Mean steps 10: 137.0 | Wins: 4605 | Win percentage: 66.5%\n","Epoch: 6940/10000 | Mean size 10: 13.1 | Longest 10: 027 | Mean steps 10: 102.9 | Wins: 4615 | Win percentage: 66.5%\n","Epoch: 6950/10000 | Mean size 10: 16.2 | Longest 10: 026 | Mean steps 10: 150.4 | Wins: 4625 | Win percentage: 66.5%\n","Epoch: 6960/10000 | Mean size 10: 20.0 | Longest 10: 029 | Mean steps 10: 173.1 | Wins: 4635 | Win percentage: 66.6%\n","Epoch: 6970/10000 | Mean size 10: 14.6 | Longest 10: 022 | Mean steps 10: 121.1 | Wins: 4645 | Win percentage: 66.6%\n","Epoch: 6980/10000 | Mean size 10: 16.8 | Longest 10: 027 | Mean steps 10: 129.2 | Wins: 4655 | Win percentage: 66.7%\n","Epoch: 6990/10000 | Mean size 10: 18.9 | Longest 10: 026 | Mean steps 10: 153.7 | Wins: 4665 | Win percentage: 66.7%\n","Epoch: 7000/10000 | Mean size 10: 15.9 | Longest 10: 028 | Mean steps 10: 138.8 | Wins: 4675 | Win percentage: 66.8%\n","Epoch: 7010/10000 | Mean size 10: 16.9 | Longest 10: 027 | Mean steps 10: 167.0 | Wins: 4685 | Win percentage: 66.8%\n","Epoch: 7020/10000 | Mean size 10: 17.2 | Longest 10: 031 | Mean steps 10: 155.3 | Wins: 4695 | Win percentage: 66.9%\n","Epoch: 7030/10000 | Mean size 10: 19.2 | Longest 10: 027 | Mean steps 10: 178.1 | Wins: 4705 | Win percentage: 66.9%\n","Epoch: 7040/10000 | Mean size 10: 17.5 | Longest 10: 025 | Mean steps 10: 149.2 | Wins: 4715 | Win percentage: 67.0%\n","Epoch: 7050/10000 | Mean size 10: 12.9 | Longest 10: 022 | Mean steps 10: 104.6 | Wins: 4725 | Win percentage: 67.0%\n","Epoch: 7060/10000 | Mean size 10: 19.9 | Longest 10: 027 | Mean steps 10: 191.6 | Wins: 4735 | Win percentage: 67.1%\n","Epoch: 7070/10000 | Mean size 10: 14.2 | Longest 10: 025 | Mean steps 10: 117.5 | Wins: 4745 | Win percentage: 67.1%\n","Epoch: 7080/10000 | Mean size 10: 19.0 | Longest 10: 028 | Mean steps 10: 173.2 | Wins: 4755 | Win percentage: 67.2%\n","Epoch: 7090/10000 | Mean size 10: 16.0 | Longest 10: 028 | Mean steps 10: 139.4 | Wins: 4765 | Win percentage: 67.2%\n","Epoch: 7100/10000 | Mean size 10: 15.5 | Longest 10: 027 | Mean steps 10: 112.7 | Wins: 4775 | Win percentage: 67.3%\n","Epoch: 7110/10000 | Mean size 10: 16.6 | Longest 10: 031 | Mean steps 10: 135.9 | Wins: 4785 | Win percentage: 67.3%\n","Epoch: 7120/10000 | Mean size 10: 13.8 | Longest 10: 024 | Mean steps 10: 110.0 | Wins: 4795 | Win percentage: 67.3%\n","Epoch: 7130/10000 | Mean size 10: 18.0 | Longest 10: 025 | Mean steps 10: 160.5 | Wins: 4805 | Win percentage: 67.4%\n","Epoch: 7140/10000 | Mean size 10: 17.3 | Longest 10: 024 | Mean steps 10: 133.6 | Wins: 4815 | Win percentage: 67.4%\n","Epoch: 7150/10000 | Mean size 10: 14.1 | Longest 10: 021 | Mean steps 10: 116.3 | Wins: 4825 | Win percentage: 67.5%\n","Epoch: 7160/10000 | Mean size 10: 20.3 | Longest 10: 029 | Mean steps 10: 205.4 | Wins: 4835 | Win percentage: 67.5%\n","Epoch: 7170/10000 | Mean size 10: 16.7 | Longest 10: 023 | Mean steps 10: 134.1 | Wins: 4845 | Win percentage: 67.6%\n","Epoch: 7180/10000 | Mean size 10: 18.6 | Longest 10: 025 | Mean steps 10: 154.4 | Wins: 4855 | Win percentage: 67.6%\n","Epoch: 7190/10000 | Mean size 10: 10.1 | Longest 10: 015 | Mean steps 10: 64.1 | Wins: 4865 | Win percentage: 67.7%\n","Epoch: 7200/10000 | Mean size 10: 16.8 | Longest 10: 027 | Mean steps 10: 143.5 | Wins: 4875 | Win percentage: 67.7%\n","Epoch: 7210/10000 | Mean size 10: 20.0 | Longest 10: 029 | Mean steps 10: 182.4 | Wins: 4885 | Win percentage: 67.8%\n","Epoch: 7220/10000 | Mean size 10: 15.2 | Longest 10: 030 | Mean steps 10: 125.8 | Wins: 4895 | Win percentage: 67.8%\n","Epoch: 7230/10000 | Mean size 10: 19.1 | Longest 10: 028 | Mean steps 10: 170.2 | Wins: 4905 | Win percentage: 67.8%\n","Epoch: 7240/10000 | Mean size 10: 17.6 | Longest 10: 028 | Mean steps 10: 171.0 | Wins: 4915 | Win percentage: 67.9%\n","Epoch: 7250/10000 | Mean size 10: 17.6 | Longest 10: 029 | Mean steps 10: 151.1 | Wins: 4925 | Win percentage: 67.9%\n","Epoch: 7260/10000 | Mean size 10: 15.7 | Longest 10: 029 | Mean steps 10: 120.8 | Wins: 4935 | Win percentage: 68.0%\n","Epoch: 7270/10000 | Mean size 10: 17.6 | Longest 10: 033 | Mean steps 10: 146.6 | Wins: 4945 | Win percentage: 68.0%\n","Epoch: 7280/10000 | Mean size 10: 18.7 | Longest 10: 030 | Mean steps 10: 156.1 | Wins: 4955 | Win percentage: 68.1%\n","Epoch: 7290/10000 | Mean size 10: 12.9 | Longest 10: 017 | Mean steps 10: 99.7 | Wins: 4965 | Win percentage: 68.1%\n","Epoch: 7300/10000 | Mean size 10: 17.9 | Longest 10: 027 | Mean steps 10: 159.0 | Wins: 4975 | Win percentage: 68.2%\n","Epoch: 7310/10000 | Mean size 10: 17.3 | Longest 10: 027 | Mean steps 10: 143.6 | Wins: 4985 | Win percentage: 68.2%\n","Epoch: 7320/10000 | Mean size 10: 16.8 | Longest 10: 030 | Mean steps 10: 138.1 | Wins: 4995 | Win percentage: 68.2%\n","Epoch: 7330/10000 | Mean size 10: 17.8 | Longest 10: 031 | Mean steps 10: 164.2 | Wins: 5005 | Win percentage: 68.3%\n","Epoch: 7340/10000 | Mean size 10: 14.8 | Longest 10: 027 | Mean steps 10: 110.7 | Wins: 5015 | Win percentage: 68.3%\n","Epoch: 7350/10000 | Mean size 10: 18.2 | Longest 10: 030 | Mean steps 10: 153.7 | Wins: 5025 | Win percentage: 68.4%\n","Epoch: 7360/10000 | Mean size 10: 20.7 | Longest 10: 034 | Mean steps 10: 168.4 | Wins: 5035 | Win percentage: 68.4%\n","Epoch: 7370/10000 | Mean size 10: 17.0 | Longest 10: 025 | Mean steps 10: 155.1 | Wins: 5045 | Win percentage: 68.5%\n","Epoch: 7380/10000 | Mean size 10: 14.5 | Longest 10: 025 | Mean steps 10: 123.1 | Wins: 5055 | Win percentage: 68.5%\n","Epoch: 7390/10000 | Mean size 10: 17.4 | Longest 10: 027 | Mean steps 10: 138.2 | Wins: 5065 | Win percentage: 68.5%\n","Epoch: 7400/10000 | Mean size 10: 16.9 | Longest 10: 024 | Mean steps 10: 132.1 | Wins: 5075 | Win percentage: 68.6%\n","Epoch: 7410/10000 | Mean size 10: 15.7 | Longest 10: 026 | Mean steps 10: 132.9 | Wins: 5085 | Win percentage: 68.6%\n","Epoch: 7420/10000 | Mean size 10: 21.1 | Longest 10: 027 | Mean steps 10: 181.5 | Wins: 5095 | Win percentage: 68.7%\n","Epoch: 7430/10000 | Mean size 10: 18.2 | Longest 10: 027 | Mean steps 10: 142.8 | Wins: 5105 | Win percentage: 68.7%\n","Epoch: 7440/10000 | Mean size 10: 16.1 | Longest 10: 025 | Mean steps 10: 147.8 | Wins: 5115 | Win percentage: 68.8%\n","Epoch: 7450/10000 | Mean size 10: 17.0 | Longest 10: 023 | Mean steps 10: 133.9 | Wins: 5125 | Win percentage: 68.8%\n","Epoch: 7460/10000 | Mean size 10: 19.1 | Longest 10: 031 | Mean steps 10: 176.5 | Wins: 5135 | Win percentage: 68.8%\n","Epoch: 7470/10000 | Mean size 10: 21.1 | Longest 10: 029 | Mean steps 10: 181.9 | Wins: 5145 | Win percentage: 68.9%\n","Epoch: 7480/10000 | Mean size 10: 18.6 | Longest 10: 028 | Mean steps 10: 157.1 | Wins: 5155 | Win percentage: 68.9%\n","Epoch: 7490/10000 | Mean size 10: 20.0 | Longest 10: 029 | Mean steps 10: 175.4 | Wins: 5165 | Win percentage: 69.0%\n","Epoch: 7500/10000 | Mean size 10: 19.8 | Longest 10: 036 | Mean steps 10: 192.7 | Wins: 5175 | Win percentage: 69.0%\n","Epoch: 7510/10000 | Mean size 10: 18.1 | Longest 10: 025 | Mean steps 10: 145.9 | Wins: 5185 | Win percentage: 69.0%\n","Epoch: 7520/10000 | Mean size 10: 13.7 | Longest 10: 023 | Mean steps 10: 109.3 | Wins: 5195 | Win percentage: 69.1%\n","Epoch: 7530/10000 | Mean size 10: 17.2 | Longest 10: 025 | Mean steps 10: 149.5 | Wins: 5205 | Win percentage: 69.1%\n","Epoch: 7540/10000 | Mean size 10: 22.2 | Longest 10: 031 | Mean steps 10: 205.9 | Wins: 5215 | Win percentage: 69.2%\n","Epoch: 7550/10000 | Mean size 10: 20.2 | Longest 10: 032 | Mean steps 10: 187.4 | Wins: 5225 | Win percentage: 69.2%\n","Epoch: 7560/10000 | Mean size 10: 17.4 | Longest 10: 026 | Mean steps 10: 132.2 | Wins: 5235 | Win percentage: 69.2%\n","Epoch: 7570/10000 | Mean size 10: 17.6 | Longest 10: 027 | Mean steps 10: 153.2 | Wins: 5245 | Win percentage: 69.3%\n","Epoch: 7580/10000 | Mean size 10: 19.1 | Longest 10: 029 | Mean steps 10: 163.9 | Wins: 5255 | Win percentage: 69.3%\n","Epoch: 7590/10000 | Mean size 10: 14.1 | Longest 10: 024 | Mean steps 10: 102.2 | Wins: 5265 | Win percentage: 69.4%\n","Epoch: 7600/10000 | Mean size 10: 15.4 | Longest 10: 030 | Mean steps 10: 125.4 | Wins: 5275 | Win percentage: 69.4%\n","Epoch: 7610/10000 | Mean size 10: 18.0 | Longest 10: 030 | Mean steps 10: 142.1 | Wins: 5285 | Win percentage: 69.4%\n","Epoch: 7620/10000 | Mean size 10: 20.2 | Longest 10: 032 | Mean steps 10: 183.9 | Wins: 5295 | Win percentage: 69.5%\n","Epoch: 7630/10000 | Mean size 10: 23.4 | Longest 10: 030 | Mean steps 10: 215.7 | Wins: 5305 | Win percentage: 69.5%\n","Epoch: 7640/10000 | Mean size 10: 20.1 | Longest 10: 027 | Mean steps 10: 178.4 | Wins: 5315 | Win percentage: 69.6%\n","Epoch: 7650/10000 | Mean size 10: 15.8 | Longest 10: 024 | Mean steps 10: 129.8 | Wins: 5325 | Win percentage: 69.6%\n","Epoch: 7660/10000 | Mean size 10: 18.2 | Longest 10: 026 | Mean steps 10: 150.2 | Wins: 5334 | Win percentage: 69.6%\n","Epoch: 7670/10000 | Mean size 10: 17.6 | Longest 10: 028 | Mean steps 10: 137.9 | Wins: 5344 | Win percentage: 69.7%\n","Epoch: 7680/10000 | Mean size 10: 17.3 | Longest 10: 026 | Mean steps 10: 142.9 | Wins: 5354 | Win percentage: 69.7%\n","Epoch: 7690/10000 | Mean size 10: 20.0 | Longest 10: 028 | Mean steps 10: 162.6 | Wins: 5364 | Win percentage: 69.8%\n","Epoch: 7700/10000 | Mean size 10: 17.1 | Longest 10: 033 | Mean steps 10: 137.4 | Wins: 5374 | Win percentage: 69.8%\n","Epoch: 7710/10000 | Mean size 10: 13.4 | Longest 10: 027 | Mean steps 10: 103.8 | Wins: 5384 | Win percentage: 69.8%\n","Epoch: 7720/10000 | Mean size 10: 19.6 | Longest 10: 031 | Mean steps 10: 152.8 | Wins: 5394 | Win percentage: 69.9%\n","Epoch: 7730/10000 | Mean size 10: 14.8 | Longest 10: 027 | Mean steps 10: 112.3 | Wins: 5404 | Win percentage: 69.9%\n","Epoch: 7740/10000 | Mean size 10: 17.3 | Longest 10: 032 | Mean steps 10: 143.0 | Wins: 5414 | Win percentage: 69.9%\n","Epoch: 7750/10000 | Mean size 10: 14.9 | Longest 10: 033 | Mean steps 10: 109.1 | Wins: 5424 | Win percentage: 70.0%\n","Epoch: 7760/10000 | Mean size 10: 20.5 | Longest 10: 033 | Mean steps 10: 167.7 | Wins: 5434 | Win percentage: 70.0%\n","Epoch: 7770/10000 | Mean size 10: 16.7 | Longest 10: 024 | Mean steps 10: 133.1 | Wins: 5444 | Win percentage: 70.1%\n","Epoch: 7780/10000 | Mean size 10: 19.0 | Longest 10: 031 | Mean steps 10: 157.4 | Wins: 5454 | Win percentage: 70.1%\n","Epoch: 7790/10000 | Mean size 10: 12.8 | Longest 10: 024 | Mean steps 10: 94.0 | Wins: 5464 | Win percentage: 70.1%\n","Epoch: 7800/10000 | Mean size 10: 20.2 | Longest 10: 032 | Mean steps 10: 159.6 | Wins: 5474 | Win percentage: 70.2%\n","Epoch: 7810/10000 | Mean size 10: 15.7 | Longest 10: 026 | Mean steps 10: 119.4 | Wins: 5484 | Win percentage: 70.2%\n","Epoch: 7820/10000 | Mean size 10: 17.4 | Longest 10: 034 | Mean steps 10: 147.0 | Wins: 5494 | Win percentage: 70.3%\n","Epoch: 7830/10000 | Mean size 10: 18.7 | Longest 10: 027 | Mean steps 10: 160.4 | Wins: 5504 | Win percentage: 70.3%\n","Epoch: 7840/10000 | Mean size 10: 19.2 | Longest 10: 034 | Mean steps 10: 167.9 | Wins: 5514 | Win percentage: 70.3%\n","Epoch: 7850/10000 | Mean size 10: 22.0 | Longest 10: 032 | Mean steps 10: 197.3 | Wins: 5524 | Win percentage: 70.4%\n","Epoch: 7860/10000 | Mean size 10: 17.1 | Longest 10: 025 | Mean steps 10: 135.2 | Wins: 5534 | Win percentage: 70.4%\n","Epoch: 7870/10000 | Mean size 10: 19.0 | Longest 10: 029 | Mean steps 10: 160.3 | Wins: 5544 | Win percentage: 70.4%\n","Epoch: 7880/10000 | Mean size 10: 15.8 | Longest 10: 024 | Mean steps 10: 114.5 | Wins: 5554 | Win percentage: 70.5%\n","Epoch: 7890/10000 | Mean size 10: 16.0 | Longest 10: 031 | Mean steps 10: 130.9 | Wins: 5564 | Win percentage: 70.5%\n","Epoch: 7900/10000 | Mean size 10: 18.3 | Longest 10: 028 | Mean steps 10: 150.1 | Wins: 5574 | Win percentage: 70.6%\n","Epoch: 7910/10000 | Mean size 10: 17.1 | Longest 10: 024 | Mean steps 10: 131.9 | Wins: 5584 | Win percentage: 70.6%\n","Epoch: 7920/10000 | Mean size 10: 18.1 | Longest 10: 034 | Mean steps 10: 142.2 | Wins: 5594 | Win percentage: 70.6%\n","Epoch: 7930/10000 | Mean size 10: 15.2 | Longest 10: 023 | Mean steps 10: 107.8 | Wins: 5604 | Win percentage: 70.7%\n","Epoch: 7940/10000 | Mean size 10: 14.0 | Longest 10: 024 | Mean steps 10: 112.6 | Wins: 5614 | Win percentage: 70.7%\n","Epoch: 7950/10000 | Mean size 10: 18.3 | Longest 10: 031 | Mean steps 10: 163.0 | Wins: 5624 | Win percentage: 70.7%\n","Epoch: 7960/10000 | Mean size 10: 16.9 | Longest 10: 026 | Mean steps 10: 124.3 | Wins: 5634 | Win percentage: 70.8%\n","Epoch: 7970/10000 | Mean size 10: 18.7 | Longest 10: 030 | Mean steps 10: 158.3 | Wins: 5644 | Win percentage: 70.8%\n","Epoch: 7980/10000 | Mean size 10: 19.3 | Longest 10: 026 | Mean steps 10: 155.2 | Wins: 5654 | Win percentage: 70.9%\n","Epoch: 7990/10000 | Mean size 10: 17.0 | Longest 10: 032 | Mean steps 10: 132.0 | Wins: 5664 | Win percentage: 70.9%\n","Epoch: 8000/10000 | Mean size 10: 15.5 | Longest 10: 028 | Mean steps 10: 112.0 | Wins: 5674 | Win percentage: 70.9%\n","Epoch: 8010/10000 | Mean size 10: 20.6 | Longest 10: 030 | Mean steps 10: 168.3 | Wins: 5684 | Win percentage: 71.0%\n","Epoch: 8020/10000 | Mean size 10: 13.1 | Longest 10: 020 | Mean steps 10: 88.4 | Wins: 5694 | Win percentage: 71.0%\n","Epoch: 8030/10000 | Mean size 10: 18.0 | Longest 10: 027 | Mean steps 10: 144.2 | Wins: 5704 | Win percentage: 71.0%\n","Epoch: 8040/10000 | Mean size 10: 19.4 | Longest 10: 029 | Mean steps 10: 152.1 | Wins: 5714 | Win percentage: 71.1%\n","Epoch: 8050/10000 | Mean size 10: 15.5 | Longest 10: 025 | Mean steps 10: 108.0 | Wins: 5724 | Win percentage: 71.1%\n","Epoch: 8060/10000 | Mean size 10: 20.4 | Longest 10: 028 | Mean steps 10: 206.6 | Wins: 5734 | Win percentage: 71.1%\n","Epoch: 8070/10000 | Mean size 10: 16.2 | Longest 10: 027 | Mean steps 10: 130.9 | Wins: 5744 | Win percentage: 71.2%\n","Epoch: 8080/10000 | Mean size 10: 19.6 | Longest 10: 031 | Mean steps 10: 184.7 | Wins: 5754 | Win percentage: 71.2%\n","Epoch: 8090/10000 | Mean size 10: 15.3 | Longest 10: 030 | Mean steps 10: 112.0 | Wins: 5764 | Win percentage: 71.2%\n","Epoch: 8100/10000 | Mean size 10: 16.1 | Longest 10: 026 | Mean steps 10: 121.1 | Wins: 5774 | Win percentage: 71.3%\n","Epoch: 8110/10000 | Mean size 10: 18.3 | Longest 10: 032 | Mean steps 10: 158.4 | Wins: 5784 | Win percentage: 71.3%\n","Epoch: 8120/10000 | Mean size 10: 21.6 | Longest 10: 034 | Mean steps 10: 191.0 | Wins: 5794 | Win percentage: 71.4%\n","Epoch: 8130/10000 | Mean size 10: 14.5 | Longest 10: 032 | Mean steps 10: 109.8 | Wins: 5804 | Win percentage: 71.4%\n","Epoch: 8140/10000 | Mean size 10: 18.4 | Longest 10: 028 | Mean steps 10: 152.3 | Wins: 5814 | Win percentage: 71.4%\n","Epoch: 8150/10000 | Mean size 10: 21.6 | Longest 10: 031 | Mean steps 10: 177.0 | Wins: 5824 | Win percentage: 71.5%\n","Epoch: 8160/10000 | Mean size 10: 20.5 | Longest 10: 034 | Mean steps 10: 162.1 | Wins: 5834 | Win percentage: 71.5%\n","Epoch: 8170/10000 | Mean size 10: 22.0 | Longest 10: 033 | Mean steps 10: 200.1 | Wins: 5844 | Win percentage: 71.5%\n","Epoch: 8180/10000 | Mean size 10: 13.8 | Longest 10: 029 | Mean steps 10: 98.1 | Wins: 5853 | Win percentage: 71.6%\n","Epoch: 8190/10000 | Mean size 10: 18.0 | Longest 10: 024 | Mean steps 10: 135.8 | Wins: 5863 | Win percentage: 71.6%\n","Epoch: 8200/10000 | Mean size 10: 16.1 | Longest 10: 025 | Mean steps 10: 121.9 | Wins: 5873 | Win percentage: 71.6%\n","Epoch: 8210/10000 | Mean size 10: 16.8 | Longest 10: 029 | Mean steps 10: 142.5 | Wins: 5883 | Win percentage: 71.7%\n","Epoch: 8220/10000 | Mean size 10: 16.4 | Longest 10: 033 | Mean steps 10: 140.4 | Wins: 5893 | Win percentage: 71.7%\n","Epoch: 8230/10000 | Mean size 10: 21.2 | Longest 10: 031 | Mean steps 10: 195.7 | Wins: 5903 | Win percentage: 71.7%\n","Epoch: 8240/10000 | Mean size 10: 18.6 | Longest 10: 030 | Mean steps 10: 167.4 | Wins: 5913 | Win percentage: 71.8%\n","Epoch: 8250/10000 | Mean size 10: 17.3 | Longest 10: 026 | Mean steps 10: 141.1 | Wins: 5923 | Win percentage: 71.8%\n","Epoch: 8260/10000 | Mean size 10: 18.0 | Longest 10: 030 | Mean steps 10: 148.7 | Wins: 5933 | Win percentage: 71.8%\n","Epoch: 8270/10000 | Mean size 10: 14.1 | Longest 10: 027 | Mean steps 10: 101.6 | Wins: 5943 | Win percentage: 71.9%\n","Epoch: 8280/10000 | Mean size 10: 14.6 | Longest 10: 025 | Mean steps 10: 111.3 | Wins: 5953 | Win percentage: 71.9%\n","Epoch: 8290/10000 | Mean size 10: 15.9 | Longest 10: 022 | Mean steps 10: 127.6 | Wins: 5963 | Win percentage: 71.9%\n","Epoch: 8300/10000 | Mean size 10: 17.7 | Longest 10: 027 | Mean steps 10: 141.1 | Wins: 5973 | Win percentage: 72.0%\n","Epoch: 8310/10000 | Mean size 10: 19.3 | Longest 10: 032 | Mean steps 10: 158.5 | Wins: 5983 | Win percentage: 72.0%\n","Epoch: 8320/10000 | Mean size 10: 13.6 | Longest 10: 026 | Mean steps 10: 102.4 | Wins: 5993 | Win percentage: 72.0%\n","Epoch: 8330/10000 | Mean size 10: 20.5 | Longest 10: 029 | Mean steps 10: 179.4 | Wins: 6003 | Win percentage: 72.1%\n","Epoch: 8340/10000 | Mean size 10: 19.3 | Longest 10: 028 | Mean steps 10: 165.5 | Wins: 6013 | Win percentage: 72.1%\n","Epoch: 8350/10000 | Mean size 10: 15.2 | Longest 10: 029 | Mean steps 10: 113.2 | Wins: 6023 | Win percentage: 72.1%\n","Epoch: 8360/10000 | Mean size 10: 15.3 | Longest 10: 025 | Mean steps 10: 107.4 | Wins: 6033 | Win percentage: 72.2%\n","Epoch: 8370/10000 | Mean size 10: 16.7 | Longest 10: 032 | Mean steps 10: 130.9 | Wins: 6043 | Win percentage: 72.2%\n","Epoch: 8380/10000 | Mean size 10: 17.1 | Longest 10: 034 | Mean steps 10: 144.1 | Wins: 6053 | Win percentage: 72.2%\n","Epoch: 8390/10000 | Mean size 10: 16.8 | Longest 10: 026 | Mean steps 10: 129.0 | Wins: 6063 | Win percentage: 72.3%\n","Epoch: 8400/10000 | Mean size 10: 16.6 | Longest 10: 029 | Mean steps 10: 133.0 | Wins: 6073 | Win percentage: 72.3%\n","Epoch: 8410/10000 | Mean size 10: 21.5 | Longest 10: 033 | Mean steps 10: 197.8 | Wins: 6083 | Win percentage: 72.3%\n","Epoch: 8420/10000 | Mean size 10: 13.9 | Longest 10: 025 | Mean steps 10: 95.8 | Wins: 6093 | Win percentage: 72.4%\n","Epoch: 8430/10000 | Mean size 10: 17.0 | Longest 10: 031 | Mean steps 10: 137.4 | Wins: 6103 | Win percentage: 72.4%\n","Epoch: 8440/10000 | Mean size 10: 17.7 | Longest 10: 029 | Mean steps 10: 147.6 | Wins: 6113 | Win percentage: 72.4%\n","Epoch: 8450/10000 | Mean size 10: 15.5 | Longest 10: 027 | Mean steps 10: 120.5 | Wins: 6123 | Win percentage: 72.5%\n","Epoch: 8460/10000 | Mean size 10: 13.9 | Longest 10: 025 | Mean steps 10: 98.5 | Wins: 6133 | Win percentage: 72.5%\n","Epoch: 8470/10000 | Mean size 10: 18.6 | Longest 10: 028 | Mean steps 10: 145.6 | Wins: 6143 | Win percentage: 72.5%\n","Epoch: 8480/10000 | Mean size 10: 15.4 | Longest 10: 029 | Mean steps 10: 121.7 | Wins: 6153 | Win percentage: 72.6%\n","Epoch: 8490/10000 | Mean size 10: 16.6 | Longest 10: 026 | Mean steps 10: 126.4 | Wins: 6163 | Win percentage: 72.6%\n","Epoch: 8500/10000 | Mean size 10: 19.2 | Longest 10: 034 | Mean steps 10: 157.1 | Wins: 6173 | Win percentage: 72.6%\n","Epoch: 8510/10000 | Mean size 10: 18.9 | Longest 10: 030 | Mean steps 10: 159.9 | Wins: 6183 | Win percentage: 72.7%\n","Epoch: 8520/10000 | Mean size 10: 21.3 | Longest 10: 033 | Mean steps 10: 186.7 | Wins: 6193 | Win percentage: 72.7%\n","Epoch: 8530/10000 | Mean size 10: 18.3 | Longest 10: 030 | Mean steps 10: 159.0 | Wins: 6203 | Win percentage: 72.7%\n","Epoch: 8540/10000 | Mean size 10: 18.6 | Longest 10: 024 | Mean steps 10: 155.2 | Wins: 6213 | Win percentage: 72.8%\n","Epoch: 8550/10000 | Mean size 10: 15.4 | Longest 10: 030 | Mean steps 10: 121.1 | Wins: 6223 | Win percentage: 72.8%\n","Epoch: 8560/10000 | Mean size 10: 19.2 | Longest 10: 033 | Mean steps 10: 155.7 | Wins: 6233 | Win percentage: 72.8%\n","Epoch: 8570/10000 | Mean size 10: 18.3 | Longest 10: 032 | Mean steps 10: 152.4 | Wins: 6243 | Win percentage: 72.8%\n","Epoch: 8580/10000 | Mean size 10: 12.6 | Longest 10: 020 | Mean steps 10: 91.4 | Wins: 6253 | Win percentage: 72.9%\n","Epoch: 8590/10000 | Mean size 10: 9.8 | Longest 10: 020 | Mean steps 10: 60.4 | Wins: 6263 | Win percentage: 72.9%\n","Epoch: 8600/10000 | Mean size 10: 19.7 | Longest 10: 034 | Mean steps 10: 167.9 | Wins: 6273 | Win percentage: 72.9%\n","Epoch: 8610/10000 | Mean size 10: 19.2 | Longest 10: 028 | Mean steps 10: 152.7 | Wins: 6283 | Win percentage: 73.0%\n","Epoch: 8620/10000 | Mean size 10: 17.9 | Longest 10: 027 | Mean steps 10: 135.3 | Wins: 6293 | Win percentage: 73.0%\n","Epoch: 8630/10000 | Mean size 10: 18.3 | Longest 10: 031 | Mean steps 10: 161.1 | Wins: 6303 | Win percentage: 73.0%\n","Epoch: 8640/10000 | Mean size 10: 18.3 | Longest 10: 031 | Mean steps 10: 159.0 | Wins: 6313 | Win percentage: 73.1%\n","Epoch: 8650/10000 | Mean size 10: 14.2 | Longest 10: 022 | Mean steps 10: 103.5 | Wins: 6323 | Win percentage: 73.1%\n","Epoch: 8660/10000 | Mean size 10: 15.1 | Longest 10: 026 | Mean steps 10: 114.6 | Wins: 6333 | Win percentage: 73.1%\n","Epoch: 8670/10000 | Mean size 10: 18.0 | Longest 10: 028 | Mean steps 10: 148.1 | Wins: 6343 | Win percentage: 73.2%\n","Epoch: 8680/10000 | Mean size 10: 14.6 | Longest 10: 025 | Mean steps 10: 108.3 | Wins: 6353 | Win percentage: 73.2%\n","Epoch: 8690/10000 | Mean size 10: 19.9 | Longest 10: 029 | Mean steps 10: 159.5 | Wins: 6363 | Win percentage: 73.2%\n","Epoch: 8700/10000 | Mean size 10: 20.0 | Longest 10: 032 | Mean steps 10: 162.5 | Wins: 6373 | Win percentage: 73.3%\n","Epoch: 8710/10000 | Mean size 10: 14.8 | Longest 10: 031 | Mean steps 10: 105.1 | Wins: 6383 | Win percentage: 73.3%\n","Epoch: 8720/10000 | Mean size 10: 17.8 | Longest 10: 030 | Mean steps 10: 154.0 | Wins: 6393 | Win percentage: 73.3%\n","Epoch: 8730/10000 | Mean size 10: 18.9 | Longest 10: 030 | Mean steps 10: 147.4 | Wins: 6403 | Win percentage: 73.3%\n","Epoch: 8740/10000 | Mean size 10: 16.7 | Longest 10: 027 | Mean steps 10: 129.9 | Wins: 6413 | Win percentage: 73.4%\n","Epoch: 8750/10000 | Mean size 10: 14.6 | Longest 10: 026 | Mean steps 10: 97.5 | Wins: 6423 | Win percentage: 73.4%\n","Epoch: 8760/10000 | Mean size 10: 17.8 | Longest 10: 024 | Mean steps 10: 135.7 | Wins: 6433 | Win percentage: 73.4%\n","Epoch: 8770/10000 | Mean size 10: 17.4 | Longest 10: 027 | Mean steps 10: 136.4 | Wins: 6443 | Win percentage: 73.5%\n","Epoch: 8780/10000 | Mean size 10: 18.1 | Longest 10: 033 | Mean steps 10: 134.2 | Wins: 6453 | Win percentage: 73.5%\n","Epoch: 8790/10000 | Mean size 10: 15.8 | Longest 10: 028 | Mean steps 10: 126.7 | Wins: 6463 | Win percentage: 73.5%\n","Epoch: 8800/10000 | Mean size 10: 20.2 | Longest 10: 029 | Mean steps 10: 168.2 | Wins: 6473 | Win percentage: 73.6%\n","Epoch: 8810/10000 | Mean size 10: 21.7 | Longest 10: 029 | Mean steps 10: 178.4 | Wins: 6483 | Win percentage: 73.6%\n","Epoch: 8820/10000 | Mean size 10: 18.4 | Longest 10: 035 | Mean steps 10: 148.7 | Wins: 6492 | Win percentage: 73.6%\n","Epoch: 8830/10000 | Mean size 10: 17.9 | Longest 10: 026 | Mean steps 10: 139.3 | Wins: 6502 | Win percentage: 73.6%\n","Epoch: 8840/10000 | Mean size 10: 13.1 | Longest 10: 021 | Mean steps 10: 86.9 | Wins: 6512 | Win percentage: 73.7%\n","Epoch: 8850/10000 | Mean size 10: 14.9 | Longest 10: 030 | Mean steps 10: 120.8 | Wins: 6522 | Win percentage: 73.7%\n","Epoch: 8860/10000 | Mean size 10: 15.9 | Longest 10: 030 | Mean steps 10: 119.9 | Wins: 6532 | Win percentage: 73.7%\n","Epoch: 8870/10000 | Mean size 10: 21.6 | Longest 10: 031 | Mean steps 10: 171.4 | Wins: 6542 | Win percentage: 73.8%\n","Epoch: 8880/10000 | Mean size 10: 21.1 | Longest 10: 032 | Mean steps 10: 169.6 | Wins: 6552 | Win percentage: 73.8%\n","Epoch: 8890/10000 | Mean size 10: 20.9 | Longest 10: 030 | Mean steps 10: 169.0 | Wins: 6562 | Win percentage: 73.8%\n","Epoch: 8900/10000 | Mean size 10: 21.5 | Longest 10: 033 | Mean steps 10: 172.3 | Wins: 6572 | Win percentage: 73.8%\n","Epoch: 8910/10000 | Mean size 10: 21.2 | Longest 10: 032 | Mean steps 10: 173.8 | Wins: 6582 | Win percentage: 73.9%\n","Epoch: 8920/10000 | Mean size 10: 22.0 | Longest 10: 030 | Mean steps 10: 190.9 | Wins: 6592 | Win percentage: 73.9%\n","Epoch: 8930/10000 | Mean size 10: 21.4 | Longest 10: 033 | Mean steps 10: 183.8 | Wins: 6602 | Win percentage: 73.9%\n","Epoch: 8940/10000 | Mean size 10: 17.7 | Longest 10: 024 | Mean steps 10: 137.2 | Wins: 6612 | Win percentage: 74.0%\n","Epoch: 8950/10000 | Mean size 10: 16.6 | Longest 10: 029 | Mean steps 10: 135.7 | Wins: 6622 | Win percentage: 74.0%\n","Epoch: 8960/10000 | Mean size 10: 14.5 | Longest 10: 028 | Mean steps 10: 104.3 | Wins: 6632 | Win percentage: 74.0%\n","Epoch: 8970/10000 | Mean size 10: 20.1 | Longest 10: 029 | Mean steps 10: 160.1 | Wins: 6642 | Win percentage: 74.0%\n","Epoch: 8980/10000 | Mean size 10: 21.9 | Longest 10: 027 | Mean steps 10: 179.9 | Wins: 6652 | Win percentage: 74.1%\n","Epoch: 8990/10000 | Mean size 10: 17.1 | Longest 10: 028 | Mean steps 10: 147.2 | Wins: 6662 | Win percentage: 74.1%\n","Epoch: 9000/10000 | Mean size 10: 16.1 | Longest 10: 030 | Mean steps 10: 123.7 | Wins: 6671 | Win percentage: 74.1%\n","Epoch: 9010/10000 | Mean size 10: 17.9 | Longest 10: 029 | Mean steps 10: 137.6 | Wins: 6681 | Win percentage: 74.2%\n","Epoch: 9020/10000 | Mean size 10: 20.5 | Longest 10: 032 | Mean steps 10: 168.2 | Wins: 6691 | Win percentage: 74.2%\n","Epoch: 9030/10000 | Mean size 10: 14.1 | Longest 10: 030 | Mean steps 10: 104.9 | Wins: 6701 | Win percentage: 74.2%\n","Epoch: 9040/10000 | Mean size 10: 22.0 | Longest 10: 030 | Mean steps 10: 184.1 | Wins: 6711 | Win percentage: 74.2%\n","Epoch: 9050/10000 | Mean size 10: 18.7 | Longest 10: 032 | Mean steps 10: 146.8 | Wins: 6721 | Win percentage: 74.3%\n","Epoch: 9060/10000 | Mean size 10: 19.8 | Longest 10: 031 | Mean steps 10: 156.8 | Wins: 6731 | Win percentage: 74.3%\n","Epoch: 9070/10000 | Mean size 10: 15.5 | Longest 10: 029 | Mean steps 10: 104.9 | Wins: 6741 | Win percentage: 74.3%\n","Epoch: 9080/10000 | Mean size 10: 16.9 | Longest 10: 028 | Mean steps 10: 131.2 | Wins: 6751 | Win percentage: 74.4%\n","Epoch: 9090/10000 | Mean size 10: 18.8 | Longest 10: 028 | Mean steps 10: 146.8 | Wins: 6761 | Win percentage: 74.4%\n","Epoch: 9100/10000 | Mean size 10: 20.1 | Longest 10: 029 | Mean steps 10: 168.2 | Wins: 6771 | Win percentage: 74.4%\n","Epoch: 9110/10000 | Mean size 10: 16.9 | Longest 10: 032 | Mean steps 10: 119.7 | Wins: 6781 | Win percentage: 74.4%\n","Epoch: 9120/10000 | Mean size 10: 18.0 | Longest 10: 027 | Mean steps 10: 140.0 | Wins: 6791 | Win percentage: 74.5%\n","Epoch: 9130/10000 | Mean size 10: 13.5 | Longest 10: 026 | Mean steps 10: 89.0 | Wins: 6801 | Win percentage: 74.5%\n","Epoch: 9140/10000 | Mean size 10: 16.3 | Longest 10: 022 | Mean steps 10: 112.8 | Wins: 6811 | Win percentage: 74.5%\n","Epoch: 9150/10000 | Mean size 10: 17.4 | Longest 10: 030 | Mean steps 10: 136.0 | Wins: 6821 | Win percentage: 74.5%\n","Epoch: 9160/10000 | Mean size 10: 20.9 | Longest 10: 031 | Mean steps 10: 167.7 | Wins: 6831 | Win percentage: 74.6%\n","Epoch: 9170/10000 | Mean size 10: 20.5 | Longest 10: 030 | Mean steps 10: 157.8 | Wins: 6841 | Win percentage: 74.6%\n","Epoch: 9180/10000 | Mean size 10: 19.8 | Longest 10: 032 | Mean steps 10: 158.3 | Wins: 6851 | Win percentage: 74.6%\n","Epoch: 9190/10000 | Mean size 10: 16.5 | Longest 10: 026 | Mean steps 10: 127.8 | Wins: 6861 | Win percentage: 74.7%\n","Epoch: 9200/10000 | Mean size 10: 18.2 | Longest 10: 025 | Mean steps 10: 138.0 | Wins: 6871 | Win percentage: 74.7%\n","Epoch: 9210/10000 | Mean size 10: 19.9 | Longest 10: 029 | Mean steps 10: 149.9 | Wins: 6881 | Win percentage: 74.7%\n","Epoch: 9220/10000 | Mean size 10: 18.5 | Longest 10: 032 | Mean steps 10: 152.4 | Wins: 6891 | Win percentage: 74.7%\n","Epoch: 9230/10000 | Mean size 10: 18.3 | Longest 10: 033 | Mean steps 10: 161.2 | Wins: 6900 | Win percentage: 74.8%\n","Epoch: 9240/10000 | Mean size 10: 17.7 | Longest 10: 029 | Mean steps 10: 134.8 | Wins: 6910 | Win percentage: 74.8%\n","Epoch: 9250/10000 | Mean size 10: 19.6 | Longest 10: 028 | Mean steps 10: 161.8 | Wins: 6919 | Win percentage: 74.8%\n","Epoch: 9260/10000 | Mean size 10: 17.6 | Longest 10: 028 | Mean steps 10: 134.8 | Wins: 6929 | Win percentage: 74.8%\n","Epoch: 9270/10000 | Mean size 10: 18.1 | Longest 10: 029 | Mean steps 10: 147.0 | Wins: 6939 | Win percentage: 74.9%\n","Epoch: 9280/10000 | Mean size 10: 17.1 | Longest 10: 024 | Mean steps 10: 121.8 | Wins: 6949 | Win percentage: 74.9%\n","Epoch: 9290/10000 | Mean size 10: 17.4 | Longest 10: 026 | Mean steps 10: 146.1 | Wins: 6959 | Win percentage: 74.9%\n","Epoch: 9300/10000 | Mean size 10: 18.6 | Longest 10: 030 | Mean steps 10: 150.4 | Wins: 6969 | Win percentage: 74.9%\n","Epoch: 9310/10000 | Mean size 10: 13.9 | Longest 10: 030 | Mean steps 10: 98.8 | Wins: 6979 | Win percentage: 75.0%\n","Epoch: 9320/10000 | Mean size 10: 13.6 | Longest 10: 024 | Mean steps 10: 92.6 | Wins: 6989 | Win percentage: 75.0%\n","Epoch: 9330/10000 | Mean size 10: 19.2 | Longest 10: 027 | Mean steps 10: 150.1 | Wins: 6999 | Win percentage: 75.0%\n","Epoch: 9340/10000 | Mean size 10: 16.8 | Longest 10: 028 | Mean steps 10: 129.7 | Wins: 7009 | Win percentage: 75.0%\n","Epoch: 9350/10000 | Mean size 10: 19.1 | Longest 10: 028 | Mean steps 10: 153.4 | Wins: 7019 | Win percentage: 75.1%\n","Epoch: 9360/10000 | Mean size 10: 21.0 | Longest 10: 037 | Mean steps 10: 171.4 | Wins: 7029 | Win percentage: 75.1%\n","Epoch: 9370/10000 | Mean size 10: 14.4 | Longest 10: 025 | Mean steps 10: 96.8 | Wins: 7039 | Win percentage: 75.1%\n","Epoch: 9380/10000 | Mean size 10: 19.4 | Longest 10: 026 | Mean steps 10: 155.6 | Wins: 7049 | Win percentage: 75.1%\n","Epoch: 9390/10000 | Mean size 10: 17.7 | Longest 10: 028 | Mean steps 10: 137.9 | Wins: 7059 | Win percentage: 75.2%\n","Epoch: 9400/10000 | Mean size 10: 17.4 | Longest 10: 035 | Mean steps 10: 135.6 | Wins: 7069 | Win percentage: 75.2%\n","Epoch: 9410/10000 | Mean size 10: 16.8 | Longest 10: 027 | Mean steps 10: 118.2 | Wins: 7079 | Win percentage: 75.2%\n","Epoch: 9420/10000 | Mean size 10: 14.7 | Longest 10: 027 | Mean steps 10: 117.1 | Wins: 7089 | Win percentage: 75.3%\n","Epoch: 9430/10000 | Mean size 10: 17.8 | Longest 10: 027 | Mean steps 10: 145.9 | Wins: 7099 | Win percentage: 75.3%\n","Epoch: 9440/10000 | Mean size 10: 17.3 | Longest 10: 027 | Mean steps 10: 135.9 | Wins: 7109 | Win percentage: 75.3%\n","Epoch: 9450/10000 | Mean size 10: 16.2 | Longest 10: 028 | Mean steps 10: 126.0 | Wins: 7119 | Win percentage: 75.3%\n","Epoch: 9460/10000 | Mean size 10: 18.7 | Longest 10: 035 | Mean steps 10: 150.7 | Wins: 7129 | Win percentage: 75.4%\n","Epoch: 9470/10000 | Mean size 10: 18.3 | Longest 10: 025 | Mean steps 10: 151.1 | Wins: 7139 | Win percentage: 75.4%\n","Epoch: 9480/10000 | Mean size 10: 19.1 | Longest 10: 028 | Mean steps 10: 156.6 | Wins: 7149 | Win percentage: 75.4%\n","Epoch: 9490/10000 | Mean size 10: 18.3 | Longest 10: 025 | Mean steps 10: 133.9 | Wins: 7159 | Win percentage: 75.4%\n","Epoch: 9500/10000 | Mean size 10: 15.8 | Longest 10: 029 | Mean steps 10: 117.7 | Wins: 7169 | Win percentage: 75.5%\n","Epoch: 9510/10000 | Mean size 10: 19.2 | Longest 10: 028 | Mean steps 10: 165.1 | Wins: 7179 | Win percentage: 75.5%\n","Epoch: 9520/10000 | Mean size 10: 17.7 | Longest 10: 027 | Mean steps 10: 132.2 | Wins: 7189 | Win percentage: 75.5%\n","Epoch: 9530/10000 | Mean size 10: 21.5 | Longest 10: 036 | Mean steps 10: 182.2 | Wins: 7199 | Win percentage: 75.5%\n","Epoch: 9540/10000 | Mean size 10: 18.9 | Longest 10: 032 | Mean steps 10: 142.1 | Wins: 7209 | Win percentage: 75.6%\n","Epoch: 9550/10000 | Mean size 10: 16.6 | Longest 10: 032 | Mean steps 10: 129.3 | Wins: 7219 | Win percentage: 75.6%\n","Epoch: 9560/10000 | Mean size 10: 18.9 | Longest 10: 036 | Mean steps 10: 142.4 | Wins: 7229 | Win percentage: 75.6%\n","Epoch: 9570/10000 | Mean size 10: 18.1 | Longest 10: 030 | Mean steps 10: 135.2 | Wins: 7239 | Win percentage: 75.6%\n","Epoch: 9580/10000 | Mean size 10: 18.2 | Longest 10: 031 | Mean steps 10: 162.8 | Wins: 7249 | Win percentage: 75.7%\n","Epoch: 9590/10000 | Mean size 10: 16.0 | Longest 10: 032 | Mean steps 10: 109.0 | Wins: 7259 | Win percentage: 75.7%\n","Epoch: 9600/10000 | Mean size 10: 18.4 | Longest 10: 031 | Mean steps 10: 149.2 | Wins: 7269 | Win percentage: 75.7%\n","Epoch: 9610/10000 | Mean size 10: 16.9 | Longest 10: 024 | Mean steps 10: 135.4 | Wins: 7279 | Win percentage: 75.7%\n","Epoch: 9620/10000 | Mean size 10: 18.9 | Longest 10: 033 | Mean steps 10: 143.0 | Wins: 7289 | Win percentage: 75.8%\n","Epoch: 9630/10000 | Mean size 10: 18.4 | Longest 10: 027 | Mean steps 10: 138.9 | Wins: 7299 | Win percentage: 75.8%\n","Epoch: 9640/10000 | Mean size 10: 15.0 | Longest 10: 025 | Mean steps 10: 108.6 | Wins: 7309 | Win percentage: 75.8%\n","Epoch: 9650/10000 | Mean size 10: 16.4 | Longest 10: 026 | Mean steps 10: 120.4 | Wins: 7318 | Win percentage: 75.8%\n","Epoch: 9660/10000 | Mean size 10: 14.7 | Longest 10: 028 | Mean steps 10: 107.3 | Wins: 7328 | Win percentage: 75.9%\n","Epoch: 9670/10000 | Mean size 10: 15.6 | Longest 10: 027 | Mean steps 10: 100.3 | Wins: 7338 | Win percentage: 75.9%\n","Epoch: 9680/10000 | Mean size 10: 20.5 | Longest 10: 033 | Mean steps 10: 161.2 | Wins: 7348 | Win percentage: 75.9%\n","Epoch: 9690/10000 | Mean size 10: 16.6 | Longest 10: 031 | Mean steps 10: 132.7 | Wins: 7358 | Win percentage: 75.9%\n","Epoch: 9700/10000 | Mean size 10: 16.7 | Longest 10: 028 | Mean steps 10: 117.6 | Wins: 7368 | Win percentage: 76.0%\n","Epoch: 9710/10000 | Mean size 10: 21.1 | Longest 10: 028 | Mean steps 10: 174.9 | Wins: 7378 | Win percentage: 76.0%\n","Epoch: 9720/10000 | Mean size 10: 19.0 | Longest 10: 036 | Mean steps 10: 160.0 | Wins: 7388 | Win percentage: 76.0%\n","Epoch: 9730/10000 | Mean size 10: 12.5 | Longest 10: 021 | Mean steps 10: 80.4 | Wins: 7398 | Win percentage: 76.0%\n","Epoch: 9740/10000 | Mean size 10: 20.2 | Longest 10: 033 | Mean steps 10: 181.4 | Wins: 7408 | Win percentage: 76.1%\n","Epoch: 9750/10000 | Mean size 10: 18.0 | Longest 10: 028 | Mean steps 10: 129.1 | Wins: 7418 | Win percentage: 76.1%\n","Epoch: 9760/10000 | Mean size 10: 16.9 | Longest 10: 027 | Mean steps 10: 133.9 | Wins: 7428 | Win percentage: 76.1%\n","Epoch: 9770/10000 | Mean size 10: 16.4 | Longest 10: 026 | Mean steps 10: 123.1 | Wins: 7438 | Win percentage: 76.1%\n","Epoch: 9780/10000 | Mean size 10: 16.6 | Longest 10: 035 | Mean steps 10: 133.5 | Wins: 7448 | Win percentage: 76.2%\n","Epoch: 9790/10000 | Mean size 10: 15.3 | Longest 10: 032 | Mean steps 10: 122.9 | Wins: 7458 | Win percentage: 76.2%\n","Epoch: 9800/10000 | Mean size 10: 17.6 | Longest 10: 026 | Mean steps 10: 128.7 | Wins: 7468 | Win percentage: 76.2%\n","Epoch: 9810/10000 | Mean size 10: 18.0 | Longest 10: 031 | Mean steps 10: 142.9 | Wins: 7478 | Win percentage: 76.2%\n","Epoch: 9820/10000 | Mean size 10: 15.8 | Longest 10: 025 | Mean steps 10: 122.8 | Wins: 7488 | Win percentage: 76.3%\n","Epoch: 9830/10000 | Mean size 10: 19.8 | Longest 10: 033 | Mean steps 10: 162.1 | Wins: 7498 | Win percentage: 76.3%\n","Epoch: 9840/10000 | Mean size 10: 20.6 | Longest 10: 034 | Mean steps 10: 162.0 | Wins: 7508 | Win percentage: 76.3%\n","Epoch: 9850/10000 | Mean size 10: 16.3 | Longest 10: 027 | Mean steps 10: 130.2 | Wins: 7518 | Win percentage: 76.3%\n","Epoch: 9860/10000 | Mean size 10: 17.5 | Longest 10: 029 | Mean steps 10: 125.9 | Wins: 7528 | Win percentage: 76.3%\n","Epoch: 9870/10000 | Mean size 10: 17.0 | Longest 10: 027 | Mean steps 10: 134.8 | Wins: 7538 | Win percentage: 76.4%\n","Epoch: 9880/10000 | Mean size 10: 18.5 | Longest 10: 031 | Mean steps 10: 137.1 | Wins: 7548 | Win percentage: 76.4%\n","Epoch: 9890/10000 | Mean size 10: 15.7 | Longest 10: 030 | Mean steps 10: 123.1 | Wins: 7558 | Win percentage: 76.4%\n","Epoch: 9900/10000 | Mean size 10: 15.9 | Longest 10: 024 | Mean steps 10: 107.8 | Wins: 7568 | Win percentage: 76.4%\n","Epoch: 9910/10000 | Mean size 10: 18.3 | Longest 10: 029 | Mean steps 10: 135.6 | Wins: 7577 | Win percentage: 76.5%\n","Epoch: 9920/10000 | Mean size 10: 17.8 | Longest 10: 029 | Mean steps 10: 139.0 | Wins: 7587 | Win percentage: 76.5%\n","Epoch: 9930/10000 | Mean size 10: 15.6 | Longest 10: 023 | Mean steps 10: 113.5 | Wins: 7597 | Win percentage: 76.5%\n","Epoch: 9940/10000 | Mean size 10: 12.8 | Longest 10: 023 | Mean steps 10: 96.7 | Wins: 7607 | Win percentage: 76.5%\n","Epoch: 9950/10000 | Mean size 10: 21.0 | Longest 10: 033 | Mean steps 10: 179.1 | Wins: 7617 | Win percentage: 76.6%\n","Epoch: 9960/10000 | Mean size 10: 18.4 | Longest 10: 031 | Mean steps 10: 149.1 | Wins: 7627 | Win percentage: 76.6%\n","Epoch: 9970/10000 | Mean size 10: 17.4 | Longest 10: 032 | Mean steps 10: 131.0 | Wins: 7637 | Win percentage: 76.6%\n","Epoch: 9980/10000 | Mean size 10: 15.3 | Longest 10: 025 | Mean steps 10: 113.8 | Wins: 7647 | Win percentage: 76.6%\n","Epoch: 9990/10000 | Mean size 10: 19.3 | Longest 10: 031 | Mean steps 10: 152.7 | Wins: 7657 | Win percentage: 76.6%\n","Epoch: 10000/10000 | Mean size 10: 18.1 | Longest 10: 029 | Mean steps 10: 127.4 | Wins: 7667 | Win percentage: 76.7%\n"],"name":"stdout"}]},{"metadata":{"id":"oEViwVmBsDjR","colab_type":"code","outputId":"82ff2241-1ef8-4b72-8959-18b78e313ee9","executionInfo":{"status":"ok","timestamp":1541745836647,"user_tz":120,"elapsed":2416094,"user":{"displayName":"Victor Neves","photoUrl":"https://lh5.googleusercontent.com/-zfn1KWpcK60/AAAAAAAAAAI/AAAAAAAAZas/oOSqICtc8i0/s64/photo.jpg","userId":"17687780415293205160"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["model.save('keras.h5')\n","\n","!zip -r model-epsgreedy-bench.zip keras.h5 \n","from google.colab import files\n","files.download('model-epsgreedy-bench.zip')\n","model = load_model('keras.h5', custom_objects={'clipped_error': clipped_error})\n","\n","board_size = 10\n","nb_frames = 4\n","nb_actions = 5\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = 1500000,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","agent.play(game, visual = False, nb_epoch = 10000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  adding: keras.h5 (deflated 42%)\n","Accuracy: 100.0 %\n","Mean size: 18.5717 | Biggest size: 43 | Smallest size: 4\n","Mean steps: 143.53480529785156 | Biggest step: 491.0 | Smallest step: 6.0\n","Mean rewards: -1.0 | Biggest reward: -1.0 | Smallest reward: -1.0\n"],"name":"stdout"}]},{"metadata":{"id":"TM6Bw0aiwVzQ","colab_type":"text"},"cell_type":"markdown","source":["# Testing what changed between the modules\n","Same as the benchmark, changed the memory file."]},{"metadata":{"id":"EQ__nvMAwHdh","colab_type":"code","outputId":"f4d99fcb-a25d-4d87-ce06-4b8c3e4c81f9","executionInfo":{"status":"ok","timestamp":1541686993124,"user_tz":120,"elapsed":8147732,"user":{"displayName":"Victor Neves","photoUrl":"https://lh5.googleusercontent.com/-zfn1KWpcK60/AAAAAAAAAAI/AAAAAAAAZas/oOSqICtc8i0/s64/photo.jpg","userId":"17687780415293205160"}},"colab":{"base_uri":"https://localhost:8080/","height":17017}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","\"\"\"SnakeGame: A simple and fun exploration, meant to be used by AI algorithms.\n","\"\"\"\n","\n","import sys # To close the window when the game is over\n","from os import environ, path # To center the game window the best possible\n","import random # Random numbers used for the food\n","import logging # Logging function for movements and errors\n","from itertools import tee # For the color gradient on snake\n","import numpy as np\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","# Actions, options and forbidden moves\n","options = {'QUIT': 0, 'PLAY': 1, 'BENCHMARK': 2, 'LEADERBOARDS': 3, 'MENU': 4, 'ADD_LEADERBOARDS': 5}\n","relative_actions = {'LEFT': 0, 'FORWARD': 1, 'RIGHT': 2}\n","actions = {'LEFT': 0, 'RIGHT': 1, 'UP': 2, 'DOWN': 3, 'IDLE': 4}\n","forbidden_moves = [(0, 1), (1, 0), (2, 3), (3, 2)]\n","\n","# Types of point in the board\n","point_type = {'EMPTY': 0, 'FOOD': 1, 'BODY': 2, 'HEAD': 3, 'DANGEROUS': 4}\n","\n","class GlobalVariables:\n","    \"\"\"Global variables to be used while drawing and moving the snake game.\n","\n","    Attributes:\n","        BLOCK_SIZE: The size in pixels of a block.\n","        HEAD_COLOR: Color of the head.\n","        BODY_COLOR: Color of the body.\n","        FOOD_COLOR: Color of the food.\n","        GAME_SPEED: Speed in ticks of the game. The higher the faster.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Initialize all global variables.\"\"\"\n","        self.BOARD_SIZE = 30\n","        self.BLOCK_SIZE = 20\n","        self.HEAD_COLOR = (0, 0, 0)\n","        self.BODY_COLOR = (0, 200, 0)\n","        self.FOOD_COLOR = (200, 0, 0)\n","        self.GAME_SPEED = 10\n","        self.BENCHMARK = 10\n","\n","        if self.BOARD_SIZE > 50:\n","            logger.warning('WARNING: BOARD IS TOO BIG, IT MAY RUN SLOWER.')\n","\n","class TextBlock:\n","    def __init__(self, text, pos, screen, scale = (1 / 12), type = \"text\"):\n","        self.type = type\n","        self.hovered = False\n","        self.text = text\n","        self.pos = pos\n","        self.screen = screen\n","        self.scale = scale\n","        self.set_rect()\n","        self.draw()\n","\n","    def draw(self):\n","        self.set_rend()\n","        self.screen.blit(self.rend, self.rect)\n","\n","    def set_rend(self):\n","        font = pygame.font.Font(resource_path(\"resources/fonts/freesansbold.ttf\"),\n","                                int((var.BOARD_SIZE * var.BLOCK_SIZE) * self.scale))\n","        self.rend = font.render(self.text, True, self.get_color(),\n","                                self.get_background())\n","\n","    def get_color(self):\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                return pygame.Color(42, 42, 42)\n","            else:\n","                return pygame.Color(152, 152, 152)\n","\n","        return pygame.Color(42, 42, 42)\n","\n","    def get_background(self):\n","        if self.type == \"menu\":\n","            if self.hovered:\n","                return pygame.Color(152, 152, 152)\n","\n","        return None\n","\n","    def set_rect(self):\n","        self.set_rend()\n","        self.rect = self.rend.get_rect()\n","        self.rect.center = self.pos\n","\n","\n","class Snake:\n","    \"\"\"Player (snake) class which initializes head, body and board.\n","\n","    The body attribute represents a list of positions of the body, which are in-\n","    cremented when moving/eating on the position [0]. The orientation represents\n","    where the snake is looking at (head) and collisions happen when any element\n","    is superposed with the head.\n","\n","    Attributes:\n","        head: The head of the snake, located according to the board size.\n","        body: Starts with 3 parts and grows when food is eaten.\n","        orientation: Current orientation where head is pointing.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"Inits Snake with 3 body parts (one is the head) and pointing right\"\"\"\n","        self.head = [int(var.BOARD_SIZE / 4), int(var.BOARD_SIZE / 4)]\n","        self.body = [[self.head[0], self.head[1]],\n","                     [self.head[0] - 1, self.head[1]],\n","                     [self.head[0] - 2, self.head[1]]]\n","        self.previous_action = 1\n","        self.length = 3\n","\n","    def move(self, action, food_pos):\n","        \"\"\"According to orientation, move 1 block. If the head is not positioned\n","        on food, pop a body part. Else (food), return without popping.\"\"\"\n","        if action == actions['IDLE']\\\n","            or (action, self.previous_action) in forbidden_moves:\n","            action = self.previous_action\n","        else:\n","            self.previous_action = action\n","\n","        if action == actions['LEFT']:\n","            self.head[0] -= 1\n","        elif action == actions['RIGHT']:\n","            self.head[0] += 1\n","        elif action == actions['UP']:\n","            self.head[1] -= 1\n","        elif action == actions['DOWN']:\n","            self.head[1] += 1\n","\n","        self.body.insert(0, list(self.head))\n","\n","        if self.head == food_pos:\n","            logger.info('EVENT: FOOD EATEN')\n","            self.length = len(self.body)\n","\n","            return True\n","        else:\n","            self.body.pop()\n","\n","            return False\n","\n","    def return_body(self):\n","        \"\"\"Return the whole body.\"\"\"\n","        return self.body\n","\n","\n","class FoodGenerator:\n","    \"\"\"Generate and keep track of food.\n","\n","    Attributes:\n","        pos: Current position of food.\n","        is_food_on_screen: Flag for existence of food.\n","    \"\"\"\n","    def __init__(self, body):\n","        \"\"\"Initialize a food piece and set existence flag.\"\"\"\n","        self.is_food_on_screen = False\n","        self.pos = self.generate_food(body)\n","\n","    def generate_food(self, body):\n","        \"\"\"Generate food and verify if it's on a valid place.\"\"\"\n","        if not self.is_food_on_screen:\n","            while True:\n","                food = [int((var.BOARD_SIZE - 1) * random.random()),\n","                        int((var.BOARD_SIZE - 1) * random.random())]\n","\n","                if food in body:\n","                    continue\n","                else:\n","                    self.pos = food\n","                    break\n","\n","            logger.info('EVENT: FOOD APPEARED')\n","            self.is_food_on_screen = True\n","\n","        return self.pos\n","\n","    def set_food_on_screen(self, bool_value):\n","        \"\"\"Set flag for existence (or not) of food.\"\"\"\n","        self.is_food_on_screen = bool_value\n","\n","\n","class Game:\n","    \"\"\"Hold the game window and functions.\n","\n","    Attributes:\n","        window: pygame window to show the game.\n","        fps: Define Clock and ticks in which the game will be displayed.\n","        snake: The actual snake who is going to be played.\n","        food_generator: Generator of food which responds to the snake.\n","        food_pos: Position of the food on the board.\n","        game_over: Flag for game_over.\n","    \"\"\"\n","    def __init__(self, player, board_size = 30, local_state = False, relative_pos = False):\n","        \"\"\"Initialize window, fps and score.\"\"\"\n","        var.BOARD_SIZE = board_size\n","        self.local_state = local_state\n","        self.relative_pos = relative_pos\n","        self.player = player\n","\n","        if player == \"ROBOT\":\n","            if self.relative_pos:\n","                self.nb_actions = 3\n","            else:\n","                self.nb_actions = 5\n","\n","            self.reset_game()\n","\n","    def reset_game(self):\n","        self.step = 0\n","        self.snake = Snake()\n","        self.food_generator = FoodGenerator(self.snake.body)\n","        self.food_pos = self.food_generator.pos\n","        self.scored = False\n","        self.game_over = False\n","\n","    def create_window(self):\n","        pygame.init()\n","\n","        flags = pygame.DOUBLEBUF\n","        self.window = pygame.display.set_mode((var.BOARD_SIZE * var.BLOCK_SIZE,\\\n","                                               var.BOARD_SIZE * var.BLOCK_SIZE),\n","                                               flags)\n","        self.window.set_alpha(None)\n","        self.fps = pygame.time.Clock()\n","\n","    def menu(self):\n","        pygame.display.set_caption(\"SNAKE GAME  | PLAY NOW!\")\n","\n","        img = pygame.image.load(resource_path(\"resources/images/snake_logo.png\"))\n","        img = pygame.transform.scale(img, (var.BOARD_SIZE * var.BLOCK_SIZE, int(var.BOARD_SIZE * var.BLOCK_SIZE / 3)))\n","\n","        self.screen_rect = self.window.get_rect()\n","        img_rect = img.get_rect()\n","        img_rect.center = self.screen_rect.center\n","\n","        menu_options = [TextBlock(' PLAY GAME ', (self.screen_rect.centerx,\n","                                                  4 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' BENCHMARK ', (self.screen_rect.centerx,\n","                                                  6 * self.screen_rect.centery / 10),\n","                                                  self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                     8 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 12), \"menu\"),\n","                        TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                             10 * self.screen_rect.centery / 10),\n","                                             self.window, (1 / 12), \"menu\")]\n","\n","        while True:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                option.draw()\n","\n","                if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                    option.hovered = True\n","\n","                    if option == menu_options[0]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['PLAY']\n","                    elif option == menu_options[1]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['BENCHMARK']\n","                    elif option == menu_options[2]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['LEADERBOARDS']\n","                    elif option == menu_options[3]:\n","                        for event in ev:\n","                            if event.type == pygame.MOUSEBUTTONUP:\n","                                return options['QUIT']\n","                else:\n","                    option.hovered = False\n","\n","            self.window.blit(img, img_rect.bottomleft)\n","            pygame.display.update()\n","\n","    def start_match(self):\n","        \"\"\"Create some wait time before the actual drawing of the game.\"\"\"\n","        for i in range(3):\n","            time = str(3 - i)\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            # Game starts in 3, 2, 1\n","            text = [TextBlock('Game starts in', (self.screen_rect.centerx,\n","                                                 4 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\"),\n","                    TextBlock(time, (self.screen_rect.centerx,\n","                                                 12 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 1.5), \"text\")]\n","\n","            for text_block in text:\n","                text_block.draw()\n","\n","            pygame.display.update()\n","            pygame.display.set_caption(\"SNAKE GAME  |  Game starts in \"\n","                                       + time + \" second(s) ...\")\n","\n","            pygame.time.wait(1000)\n","\n","        logger.info('EVENT: GAME START')\n","\n","    def start(self):\n","        \"\"\"Use menu to select the option/game mode.\"\"\"\n","        opt = self.menu()\n","        running = True\n","\n","        while running:\n","            if opt == options['QUIT']:\n","                pygame.quit()\n","                sys.exit()\n","            elif opt == options['PLAY']:\n","                self.select_speed()\n","                self.reset_game()\n","                self.start_match()\n","                score = self.single_player()\n","                opt = self.over(score)\n","            elif opt == options['BENCHMARK']:\n","                self.select_speed()\n","                score = []\n","\n","                for i in range(var.BENCHMARK):\n","                    self.reset_game()\n","                    self.start_match()\n","                    score.append(self.single_player())\n","\n","                opt = self.over(score)\n","            elif opt == options['LEADERBOARDS']:\n","                pass\n","            elif opt == options['ADD_LEADERBOARDS']:\n","                pass\n","            elif opt == options['MENU']:\n","                opt = self.menu()\n","\n","    def over(self, score):\n","        \"\"\"If collision with wall or body, end the game.\"\"\"\n","        menu_options = [None] * 5\n","\n","        menu_options[0] = TextBlock(' PLAY AGAIN ', (self.screen_rect.centerx,\n","                                                     4 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[1] = TextBlock(' GO TO MENU ', (self.screen_rect.centerx,\n","                                                     6 * self.screen_rect.centery / 10),\n","                                                     self.window, (1 / 15), \"menu\")\n","        menu_options[3] = TextBlock(' QUIT ', (self.screen_rect.centerx,\n","                                               10 * self.screen_rect.centery / 10),\n","                                               self.window, (1 / 15), \"menu\")\n","\n","        if isinstance(score, int):\n","            text_score = 'SCORE: ' + str(score)\n","\n","        else:\n","            text_score = 'MEAN SCORE: ' + str(sum(score) / var.BENCHMARK)\n","\n","            menu_options[2] = TextBlock(' ADD TO LEADERBOARDS ', (self.screen_rect.centerx,\n","                                                                  8 * self.screen_rect.centery / 10),\n","                                                                  self.window, (1 / 15), \"menu\")\n","\n","        pygame.display.set_caption(\"SNAKE GAME  | \" + text_score\n","                                   + \"  |  GAME OVER...\")\n","        logger.info('EVENT: GAME OVER | FINAL ' + text_score)\n","\n","        menu_options[4] = TextBlock(text_score, (self.screen_rect.centerx,\n","                                                 15 * self.screen_rect.centery / 10),\n","                                                 self.window, (1 / 10), \"text\")\n","\n","        while True:\n","            pygame.event.pump()\n","            ev = pygame.event.get()\n","\n","            # Game over screen\n","            self.window.fill(pygame.Color(225, 225, 225))\n","\n","            for option in menu_options:\n","                if option is not None:\n","                    option.draw()\n","\n","                    if option.rect.collidepoint(pygame.mouse.get_pos()):\n","                        option.hovered = True\n","\n","                        if option == menu_options[0]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['PLAY']\n","                        elif option == menu_options[1]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['MENU']\n","                        elif option == menu_options[2]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    return options['ADD_LEADERBOARDS']\n","                        elif option == menu_options[3]:\n","                            for event in ev:\n","                                if event.type == pygame.MOUSEBUTTONUP:\n","                                    pygame.quit()\n","                                    sys.exit()\n","                    else:\n","                        option.hovered = False\n","\n","            pygame.display.update()\n","\n","    def single_player(self):\n","        # The main loop, it pump key_presses and update the board every tick.\n","        previous_size = self.snake.length # Initial size of the snake\n","        current_size = previous_size # Initial size\n","        color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                   previous_size)\n","\n","        # Main loop, where the snake keeps going each tick. It generate food, check\n","        # collisions and draw.\n","        while True:\n","            action = self.handle_input()\n","\n","            if self.play(action):\n","                return current_size\n","\n","            self.draw(color_list)\n","            current_size = self.snake.length # Update the body size\n","\n","            if current_size > previous_size:\n","                color_list = self.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                           current_size)\n","\n","                previous_size = current_size\n","\n","    def check_collision(self):\n","        \"\"\"Check wether any collisions happened with the wall or body and re-\n","        turn.\"\"\"\n","        if self.snake.head[0] > (var.BOARD_SIZE - 1) or self.snake.head[0] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.snake.head[1] > (var.BOARD_SIZE - 1) or self.snake.head[1] < 0:\n","            logger.info('EVENT: WALL COLLISION')\n","\n","            return True\n","        elif self.snake.head in self.snake.body[1:]:\n","            logger.info('EVENT: BODY COLLISION')\n","\n","            return True\n","\n","        return False\n","\n","    def is_won(self):\n","        return self.snake.length > 3\n","\n","    def generate_food(self):\n","        return self.food_generator.generate_food(self.snake.body)\n","\n","    def handle_input(self):\n","        \"\"\"After getting current pressed keys, handle important cases.\"\"\"\n","        pygame.event.set_allowed([pygame.QUIT, pygame.KEYDOWN])\n","        keys = pygame.key.get_pressed()\n","        pygame.event.pump()\n","\n","        if keys[pygame.K_ESCAPE] or keys[pygame.K_q]:\n","            logger.info('ACTION: KEY PRESSED: ESCAPE or Q')\n","            self.over(self.snake.length - 3)\n","        elif keys[pygame.K_LEFT]:\n","            logger.info('ACTION: KEY PRESSED: LEFT')\n","            return actions['LEFT']\n","        elif keys[pygame.K_RIGHT]:\n","            logger.info('ACTION: KEY PRESSED: RIGHT')\n","            return actions['RIGHT']\n","        elif keys[pygame.K_UP]:\n","            logger.info('ACTION: KEY PRESSED: UP')\n","            return actions['UP']\n","        elif keys[pygame.K_DOWN]:\n","            logger.info('ACTION: KEY PRESSED: DOWN')\n","            return actions['DOWN']\n","        else:\n","            return self.snake.previous_action\n","\n","    def eval_local_safety(self, canvas, body):\n","        \"\"\"Evaluate the safety of the head's possible next movements.\"\"\"\n","        if (body[0][0] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0] + 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 0] = point_type['DANGEROUS']\n","        if (body[0][0] - 1) < 0 or ([body[0][0] - 1, body[0][1]]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 1] = point_type['DANGEROUS']\n","        if (body[0][1] - 1) < 0 or ([body[0][0], body[0][1] - 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 2] = point_type['DANGEROUS']\n","        if (body[0][1] + 1) > (var.BOARD_SIZE - 1)\\\n","            or ([body[0][0], body[0][1] + 1]) in body[1:]:\n","            canvas[var.BOARD_SIZE - 1, 3] = point_type['DANGEROUS']\n","\n","        return canvas\n","\n","    def state(self):\n","        \"\"\"Create a matrix of the current state of the game.\"\"\"\n","        body = self.snake.return_body()\n","        canvas = np.zeros((var.BOARD_SIZE, var.BOARD_SIZE))\n","\n","        for part in body:\n","            canvas[part[0], part[1]] = point_type['BODY']\n","\n","        canvas[body[0][0], body[0][1]] = point_type['HEAD']\n","\n","        if self.local_state:\n","            canvas = self.eval_local_safety(canvas, body)\n","\n","        canvas[self.food_pos[0], self.food_pos[1]] = point_type['FOOD']\n","\n","        return canvas\n","\n","    def relative_to_absolute(self, action):\n","        if action == relative_actions['FORWARD']:\n","            action = self.snake.previous_action\n","        elif action == relative_actions['LEFT']:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['LEFT']\n","            else:\n","                action = actions['RIGHT']\n","        else:\n","            if self.snake.previous_action == actions['LEFT']:\n","                action = actions['UP']\n","            elif self.snake.previous_action == actions['RIGHT']:\n","                action = actions['DOWN']\n","            elif self.snake.previous_action == actions['UP']:\n","                action = actions['RIGHT']\n","            else:\n","                action = actions['LEFT']\n","\n","        return action\n","\n","    def play(self, action):\n","        \"\"\"Move the snake to the direction, eat and check collision.\"\"\"\n","        self.scored = False\n","        self.step += 1\n","        self.food_pos = self.generate_food()\n","\n","        if self.relative_pos:\n","            action = self.relative_to_absolute(action)\n","\n","        if self.snake.move(action, self.food_pos):\n","            self.scored = True\n","            self.food_generator.set_food_on_screen(False)\n","\n","        if self.player == \"HUMAN\":\n","            if self.check_collision():\n","                return True\n","        elif self.check_collision() or self.step > 50 * self.snake.length:\n","            self.game_over = True\n","\n","    def get_reward(self):\n","        \"\"\"Return the current score. Can be used as the reward function.\"\"\"\n","        if self.game_over:\n","            return -1\n","        elif self.scored:\n","            return self.snake.length\n","\n","        return -0.005\n","\n","    def gradient(self, colors, steps, components = 3):\n","        \"\"\"Function to create RGB gradients given 2 colors and steps.\n","\n","        If component is changed to 4, it does the same to RGBA colors.\"\"\"\n","        def linear_gradient(start, finish, substeps):\n","            yield start\n","\n","            for i in range(1, substeps):\n","                yield tuple([(start[j] + (float(i) / (substeps-1)) * (finish[j]\\\n","                            - start[j])) for j in range(components)])\n","\n","        def pairs(seq):\n","            a, b = tee(seq)\n","            next(b, None)\n","\n","            return zip(a, b)\n","\n","        result = []\n","        substeps = int(float(steps) / (len(colors) - 1))\n","\n","        for a, b in pairs(colors):\n","            for c in linear_gradient(a, b, substeps):\n","                result.append(c)\n","\n","        return result\n","\n","    def draw(self, color_list):\n","        \"\"\"Draw the game, the snake and the food using pygame.\"\"\"\n","        self.window.fill(pygame.Color(225, 225, 225))\n","\n","        for part, color in zip(self.snake.body, color_list):\n","            pygame.draw.rect(self.window, color, pygame.Rect(part[0] *\\\n","                        var.BLOCK_SIZE, part[1] * var.BLOCK_SIZE, \\\n","                        var.BLOCK_SIZE, var.BLOCK_SIZE))\n","\n","        pygame.draw.rect(self.window, var.FOOD_COLOR,\\\n","                         pygame.Rect(self.food_pos[0] * var.BLOCK_SIZE,\\\n","                         self.food_pos[1] * var.BLOCK_SIZE, var.BLOCK_SIZE,\\\n","                         var.BLOCK_SIZE))\n","\n","        pygame.display.set_caption(\"SNAKE GAME  |  Score: \"\n","                                    + str(self.snake.length - 3))\n","        pygame.display.update()\n","        self.fps.tick(var.GAME_SPEED)\n","\n","def resource_path(relative_path):\n","    if hasattr(sys, '_MEIPASS'):\n","        return path.join(sys._MEIPASS, relative_path)\n","\n","    return path.join(path.dirname(path.realpath(__file__)), relative_path)\n","\n","var = GlobalVariables() # Initializing GlobalVariables\n","logger = logging.getLogger(__name__) # Setting logger\n","environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","\n","import numpy as np\n","from random import sample, uniform\n","from array import array  # Efficient numeric arrays\n","\n","class ExperienceReplay:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes\n","    ----------\n","    memory: list of experiences\n","        Memory list to insert experiences.\n","    memory_size: int, optional, default = 150000\n","        The ammount of experiences to be stored in the memory.\n","    input_shape: tuple of 3 * int\n","        The shape of the input which will be stored.\n","    \"\"\"\n","    def __init__(self, memory_size = 150000):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        self.memory.append(experience)\n","\n","        if self.memory_size > 0 and self.exp_size() > self.memory_size:\n","            self.memory.pop(0)\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\n","\n","        Return\n","        ----------\n","        batch: np.array of batch_size experiences\n","            The batched experiences from memory.\n","        IS_weights: np.array of batch_size of the weights\n","            As it's used only in PER, is an array of ones in this case.\n","        Indexes: list of batch_size * int\n","            As it's used only in PER, return None.\n","        \"\"\"\n","        IS_weights = np.ones((batch_size, ))\n","        batch = np.array(sample(self.memory, batch_size))\n","\n","        return batch, IS_weights, None\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        self.memory = []\n","\n","\n","class PrioritizedExperienceReplayNaive:\n","    \"\"\"The class that handles memory and experiences replay.\n","\n","    Attributes:\n","        memory: memory array to insert experiences.\n","        memory_size: the ammount of experiences to be stored in the memory.\n","        input_shape: the shape of the input which will be stored.\n","        batch_function: returns targets according to S.\n","        per: flag for PER usage.\n","        per_epsilon: used to replace \"0\" probabilities cases.\n","        per_alpha: how much prioritization to use.\n","        per_beta: importance sampling weights (IS_weights).\n","    \"\"\"\n","    def __init__(self, memory_size = 150000, alpha = 0.6, epsilon = 0.001,\n","                 beta = 0.4, nb_epoch = 10000, decay = 0.5):\n","        \"\"\"Initialize parameters and the memory array.\"\"\"\n","        self.memory_size = memory_size\n","        self.epsilon = epsilon\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.reset_memory() # Initiate the memory\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return self.exp\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def update(self, tree_indices, errors):\n","        \"\"\"Update a list of nodes, based on their errors.\"\"\"\n","        priorities = self.get_priority(errors)\n","\n","        for index, priority in zip(tree_indices, priorities):\n","            self.memory.update(index, priority)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        \"\"\"Remember SARS' experiences, with the game_over parameter (done).\"\"\"\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","\n","        max_priority = self.memory.max_leaf()\n","\n","        if max_priority == 0:\n","            max_priority = self.get_priority(0)\n","\n","        self.memory.insert(experience, max_priority)\n","        self.exp += 1\n","\n","    def get_samples(self, batch_size):\n","        \"\"\"Sample the memory according to PER flag.\"\"\"\n","        batch = [None] * batch_size\n","        IS_weights = np.zeros((batch_size, ))\n","        tree_indices = [0] * batch_size\n","\n","        memory_sum = self.memory.sum()\n","        len_seg = memory_sum / batch_size\n","        min_prob = self.memory.min_leaf() / memory_sum\n","\n","        for i in range(batch_size):\n","            val = uniform(len_seg * i, len_seg * (i + 1))\n","            tree_indices[i], priority, batch[i] = self.memory.retrieve(val)\n","            prob = priority / self.memory.sum()\n","            IS_weights[i] = np.power(prob / min_prob, -self.beta)\n","\n","        return np.array(batch), IS_weights, tree_indices\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = SumTree(self.memory_size)\n","        self.exp = 0\n","\n","\n","class PrioritizedExperienceReplay:\n","    def __init__(self, memory_size, nb_epoch = 10000, epsilon = 0.001,\n","                 alpha = 0.6, beta = 0.4, decay = 0.5):\n","        self.memory_size = memory_size\n","        self.alpha = alpha\n","        self.epsilon = epsilon\n","        self.beta = beta\n","        self.schedule = LinearSchedule(nb_epoch * decay, 1.0, beta)\n","        self.max_priority = 1.0\n","        self.reset_memory()\n","\n","    def exp_size(self):\n","        \"\"\"Returns how much memory is stored.\"\"\"\n","        return len(self.memory)\n","\n","    def remember(self, s, a, r, s_prime, game_over):\n","        if not hasattr(self, 'input_shape'):\n","            self.input_shape = s.shape[1:] # set attribute only once\n","\n","        experience = np.concatenate([s.flatten(),\n","                                     np.array(a).flatten(),\n","                                     np.array(r).flatten(),\n","                                     s_prime.flatten(),\n","                                     1 * np.array(game_over).flatten()])\n","        if self.exp_size() < self.memory_size:\n","            self.memory.append(experience)\n","            self.pos += 1\n","        else:\n","            self.memory[self.pos] = experience\n","            self.pos = (self.pos + 1) % self.memory_size\n","\n","        self._it_sum[self.pos] = self.max_priority ** self.alpha\n","        self._it_min[self.pos] = self.max_priority ** self.alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = array('i')\n","\n","        for _ in range(batch_size):\n","            mass = random.random() * self._it_sum.sum(0, self.exp_size() - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","\n","        return res\n","\n","    def get_priority(self, errors):\n","        \"\"\"Returns priority based on how much prioritization to use.\"\"\"\n","        return (errors + self.epsilon) ** self.alpha\n","\n","    def get_samples(self, batch_size):\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = array('f')\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * self.exp_size()) ** (-self.beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * self.exp_size()) ** (-self.beta)\n","            weights.append(weight / max_weight)\n","\n","        weights = np.array(weights, dtype=np.float32)\n","        samples = [self.memory[idx] for idx in idxes]\n","\n","        return np.array(samples), weights, idxes\n","\n","    def get_targets(self, target, model, batch_size, nb_actions, gamma = 0.9,\n","                    n_steps = 1):\n","        \"\"\"Function to sample, set batch function and use it for targets.\"\"\"\n","        if self.exp_size() < batch_size:\n","            return None\n","\n","        samples, IS_weights, tree_indices = self.get_samples(batch_size)\n","        input_dim = np.prod(self.input_shape) # Get the input shape, multiplied\n","\n","        S = samples[:, 0 : input_dim] # Seperate the states\n","        a = samples[:, input_dim] # Separate the actions\n","        r = samples[:, input_dim + 1] # Separate the rewards\n","        S_prime = samples[:, input_dim + 2 : 2 * input_dim + 2] # Next_actions\n","        game_over = samples[:, 2 * input_dim + 2] # Separate terminal flags\n","\n","        # Reshape the arrays to make them usable by the model.\n","        S = S.reshape((batch_size, ) + self.input_shape)\n","        S_prime = S_prime.reshape((batch_size, ) + self.input_shape)\n","\n","        X = np.concatenate([S, S_prime], axis = 0)\n","        Y = model.predict(X)\n","\n","        if target is not None: # Use Double DQN logic:\n","            Qsa = [None] * 64\n","            actions = np.argmax(Y[batch_size:], axis = 1)\n","            Y_target = target.predict(X[batch_size:])\n","\n","            for idx, target in enumerate(Y_target):\n","                Qsa[idx] = target[actions[idx]]\n","\n","            Qsa = np.array(Qsa)\n","        else:\n","            Qsa = np.max(Y[batch_size:], axis = 1)\n","\n","        # Where the action happened, replace with the Q values of S_prime\n","        targets = np.array(Y[:batch_size])\n","        value = r + (gamma ** n_steps) * (1 - game_over) * Qsa\n","        targets[range(batch_size), a.astype(int)] = value\n","\n","        errors = np.abs(value - Y[:batch_size].max(axis = 1)).clip(max = 1.)\n","        self.update_priorities(tree_indices, errors)\n","\n","        return S, targets, IS_weights\n","\n","    def update_priorities(self, idxes, errors):\n","        priorities = self.get_priority(errors)\n","\n","        for idx, priority in zip(idxes, priorities):\n","            self._it_sum[idx] = priority ** self.alpha\n","            self._it_min[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def reset_memory(self):\n","        \"\"\"Set the memory as a blank list.\"\"\"\n","        if self.memory_size <= 100:\n","            self.memory_size = 150000\n","\n","        self.memory = []\n","        self.pos = 0\n","\n","        it_capacity = 1\n","\n","        while it_capacity < self.memory_size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n","\n","\n","#!/usr/bin/env python\n","\n","\"\"\"dqn: First try to create an AI for SnakeGame. Is it good enough?\n","\n","This algorithm is a implementation of DQN, Double DQN logic (using a target\n","network to have fixed Q-targets), Dueling DQN logic (Q(s,a) = Advantage + Value),\n","PER (Prioritized Experience Replay, using Sum Trees) and Multi-step returns. You\n","can read more about these on https://goo.gl/MctLzp\n","\n","Implemented algorithms:\n","    * Simple DQN (with ExperienceReplay);\n","        Paper: https://arxiv.org/abs/1312.5602\n","    * Double DQN;\n","        Paper: https://arxiv.org/abs/1509.06461\n","    * Dueling DQN;\n","        Paper: https://arxiv.org/abs/1511.06581\n","    * DQN + PER;\n","        Paper: https://arxiv.org/abs/1511.05952\n","    * Multi-step returns.\n","        Paper: https://arxiv.org/pdf/1703.01327\n","\n","Arguments:\n","    --load FILE.h5: load a previously trained model in '.h5' format.\n","    --board_size INT: assign the size of the board, default = 10\n","    --nb_frames INT: assign the number of frames per stack, default = 4.\n","    --nb_actions INT: assign the number of actions possible, default = 5.\n","    --update_freq INT: assign how often, in epochs, to update the target,\n","      default = 500.\n","    --visual: select wheter or not to draw the game in pygame.\n","    --double: use a target network with double DQN logic.\n","    --dueling: use dueling network logic, Q(s,a) = A + V.\n","    --per: use Prioritized Experience Replay (based on Sum Trees).\n","    --local_state: Verify is possible next moves are dangerous (field expertise)\n","\"\"\"\n","\n","import numpy as np\n","from os import path, environ, sys\n","import random\n","\n","import inspect # Making relative imports from parallel folders possible\n","currentdir = path.dirname(path.abspath(inspect.getfile(inspect.currentframe())))\n","parentdir = path.dirname(currentdir)\n","sys.path.insert(0, parentdir)\n","\n","from keras.optimizers import RMSprop, Nadam\n","from keras.models import load_model, Sequential\n","from keras.layers import *\n","from keras import backend as K\n","K.set_image_dim_ordering('th')\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__version__ = \"1.0\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","__status__ = \"Production\"\n","\n","class Agent:\n","    \"\"\"Agent based in a simple DQN that can read states, remember and play.\n","\n","    Attributes:\n","    memory: memory used in the model. Input memory or ExperienceReplay.\n","    model: the input model, Conv2D in Keras.\n","    target: the target model, used to calculade the fixed Q-targets.\n","    nb_frames: ammount of frames for each sars.\n","    frames: the frames in each sars.\n","    per: flag for PER usage.\n","    \"\"\"\n","    def __init__(self, model, target, memory = None, memory_size = 150000,\n","                 nb_frames = 4, board_size = 10, per = False):\n","        \"\"\"Initialize the agent with given attributes.\"\"\"\n","        if memory:\n","            self.memory = memory\n","        else:\n","            self.memory = ExperienceReplay(memory_size = memory_size)\n","\n","        self.per = per\n","        self.model = model\n","        self.target = target\n","        self.nb_frames = nb_frames\n","        self.board_size = board_size\n","        self.frames = None\n","        self.target_updates = 0\n","\n","    def reset_memory(self):\n","        \"\"\"Reset memory if necessary.\"\"\"\n","        self.memory.reset_memory()\n","\n","    def get_game_data(self, game):\n","        \"\"\"Create a list with 4 frames and append/pop them each frame.\"\"\"\n","        if game.game_over:\n","            frame = np.zeros((self.board_size, self.board_size))\n","        else:\n","            frame = game.state()\n","\n","        if self.frames is None:\n","            self.frames = [frame] * self.nb_frames\n","        else:\n","            self.frames.append(frame)\n","            self.frames.pop(0)\n","\n","        return np.expand_dims(self.frames, 0)\n","\n","    def clear_frames(self):\n","        \"\"\"Reset frames to restart appending.\"\"\"\n","        self.frames = None\n","\n","    def update_target_model(self):\n","        \"\"\"Update the target model with the main model's weights.\"\"\"\n","        self.target_updates += 1\n","        self.target.set_weights(self.model.get_weights())\n","\n","    def print_metrics(self, epoch, nb_epoch, history_size, history_loss,\n","                      history_step, history_reward, policy, value, win_count,\n","                      verbose = 1):\n","        \"\"\"Function to print metrics of training steps.\"\"\"\n","        if verbose == 0:\n","            pass\n","        elif verbose == 1:\n","            text_epoch = ('Epoch: {:03d}/{:03d} | Mean size 10: {:.1f} | '\n","                           + 'Longest 10: {:03d} | Mean steps 10: {:.1f} | '\n","                           + 'Wins: {:d} | Win percentage: {:.1f}%')\n","            print(text_epoch.format(epoch + 1, nb_epoch,\n","                                    sum(history_size[-10:]) / 10,\n","                                    max(history_size[-10:]),\n","                                    sum(history_step[-10:]) / 10,\n","                                    win_count, 100 * win_count/(epoch + 1)))\n","        else:\n","            text_epoch = 'Epoch: {:03d}/{:03d}' # Print epoch info\n","            print(text_epoch.format(epoch + 1, nb_epoch))\n","\n","            # Print training performance\n","            text_train = ('\\t\\x1b[0;30;47m' + ' Training metrics ' + '\\x1b[0m'\n","                          + '\\tTotal loss: {:.4f} | Loss per step: {:.4f} | '\n","                          + 'Mean loss - 100 episodes: {:.4f}')\n","            print(text_perf.format(history_loss[-1],\n","                                   history_loss[-1] / history_step[-1],\n","                                   sum(history_loss[-100:]) / 100))\n","\n","            text_game = ('\\t\\x1b[0;30;47m' + ' Game metrics ' + '\\x1b[0m'\n","                         + '\\t\\tSize: {:d} | Ammount of steps: {:d} | '\n","                         + 'Steps per food eaten: {:.1f} | '\n","                         + 'Mean size - 100 episodes: {:.1f}')\n","            print(text_game.format(history_size[-1], history_step[-1],\n","                                   history_size[-1] / history_step[-1],\n","                                   sum(history_step[-100:]) / 100))\n","\n","            # Print policy metrics\n","            if policy == \"BoltzmannQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tBoltzmann Temperature: {:.2f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            elif policy == \"BoltzmannGumbelQPolicy\":\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tNumber of actions: {:.0f} | '\n","                               + 'Episode reward: {:.1f} | Wins: {:d} | '\n","                               + 'Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","            else:\n","                text_policy = ('\\t\\x1b[0;30;47m' + ' Policy metrics ' + '\\x1b[0m'\n","                               + '\\tEpsilon: {:.2f} | Episode reward: {:.1f} | '\n","                               + 'Wins: {:d} | Win percentage: {:.1f}%')\n","                print(text_policy.format(value, history_reward[-1], win_count,\n","                                         100 * win_count/(epoch + 1)))\n","\n","    def train_model(self, model, target, batch_size, gamma, nb_actions, epoch = 0):\n","        \"\"\"Function to train the model on a batch of the data. The optimization\n","        flag is used when we are not playing, just batching and optimizing.\"\"\"\n","        loss = 0.\n","\n","        batch = self.memory.get_targets(model = self.model,\n","                                        target = self.target,\n","                                        batch_size = batch_size,\n","                                        gamma = gamma,\n","                                        nb_actions = nb_actions)\n","\n","        if batch:\n","            inputs, targets, IS_weights = batch\n","\n","            if inputs is not None and targets is not None:\n","                loss = float(self.model.train_on_batch(inputs,\n","                                                       targets,\n","                                                       IS_weights))\n","\n","        return loss\n","\n","    def train(self, game, nb_epoch = 10000, batch_size = 64, gamma = 0.95,\n","              eps = [1., .01], temp = [1., 0.01], learning_rate = 0.5,\n","              observe = 0, update_target_freq = 0.001, optim_rounds = 1,\n","              policy = \"EpsGreedyQPolicy\", verbose = 1, n_steps = None):\n","        \"\"\"The main training function, loops the game, remember and choose best\n","        action given game state (frames).\"\"\"\n","        history_size = []\n","        history_step = []\n","        history_loss = []\n","        history_reward = []\n","\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp[0], temp[1], nb_epoch * learning_rate)\n","        if policy == \"BoltzmannGumbelQPolicy\":\n","            q_policy = BoltzmannGumbelQPolicy()\n","        else:\n","            q_policy = EpsGreedyQPolicy(eps[0], eps[1], nb_epoch * learning_rate)\n","\n","        nb_actions = game.nb_actions\n","        win_count = 0\n","\n","        for turn in range(optim_rounds):\n","            if turn > 0:\n","                for epoch in range(nb_epoch):\n","                    loss = self.train_model(model = self.model,\n","                                            epoch = epoch,\n","                                            target = self.target,\n","                                            batch_size = batch_size,\n","                                            gamma = gamma,\n","                                            nb_actions = nb_actions)\n","                    text_optim = ('Optimizer turn: {:2d} | Epoch: {:03d}/{:03d}'\n","                                  + '| Loss: {:.4f}')\n","                    print(text_optim.format(turn, epoch + 1, nb_epoch, loss))\n","            else:\n","                for epoch in range(nb_epoch):\n","                    loss = 0.\n","                    total_reward = 0.\n","                    if n_steps is not None:\n","                        n_step_buffer = []\n","                    game.reset_game()\n","                    self.clear_frames()\n","\n","                    S = self.get_game_data(game)\n","\n","                    while not game.game_over:\n","                        game.food_pos = game.generate_food()\n","                        action, value = q_policy.select_action(self.model,\n","                                                               S, epoch,\n","                                                               nb_actions)\n","\n","                        game.play(action)\n","\n","                        r = game.get_reward()\n","                        total_reward += r\n","                        if n_steps is not None:\n","                            n_step_buffer.append(r)\n","\n","                            if len(n_step_buffer) < n_steps:\n","                                R = r\n","                            else:\n","                                R = sum([n_step_buffer[i] * (gamma ** i)\\\n","                                        for i in range(n_steps)])\n","                        else:\n","                            R = r\n","\n","                        S_prime = self.get_game_data(game)\n","                        experience = [S, action, R, S_prime, game.game_over]\n","                        self.memory.remember(*experience) # Add to the memory\n","                        S = S_prime # Advance to the next state (stack of S)\n","\n","                        if epoch >= observe: # Get the batchs and train\n","                            loss += self.train_model(model = self.model,\n","                                                     target = self.target,\n","                                                     batch_size = batch_size,\n","                                                     gamma = gamma,\n","                                                     nb_actions = nb_actions)\n","\n","                    if game.is_won():\n","                        win_count += 1 # Counter for metric purposes\n","\n","                    if self.per: # Advance beta, used in PER\n","                        self.memory.per_beta = self.memory.schedule.value(epoch)\n","\n","                    if self.target is not None: # Update the target model\n","                        if epoch % update_target_freq == 0:\n","                            self.update_target_model()\n","\n","                    history_size.append(game.snake.length)\n","                    history_step.append(game.step)\n","                    history_loss.append(loss)\n","                    history_reward.append(total_reward)\n","\n","                    if (epoch + 1) % 10 == 0:\n","                        self.print_metrics(epoch, nb_epoch, history_size,\n","                                           history_loss, history_step,\n","                                           history_reward, policy, value,\n","                                           win_count, verbose)\n","\n","    def play(self, game, nb_epoch = 1000, eps = 0.01, temp = 0.01,\n","             visual = False, policy = \"GreedyQPolicy\"):\n","        \"\"\"Play the game with the trained agent. Can use the visual tag to draw\n","            in pygame.\"\"\"\n","        win_count = 0\n","        result_size = []\n","        result_step = []\n","        if policy == \"BoltzmannQPolicy\":\n","            q_policy = BoltzmannQPolicy(temp, temp, nb_epoch)\n","        elif policy == \"EpsGreedyQPolicy\":\n","            q_policy = EpsGreedyQPolicy(eps, eps, nb_epoch)\n","        else:\n","            q_policy = GreedyQPolicy()\n","\n","        for epoch in range(nb_epoch):\n","            game.reset_game()\n","            self.clear_frames()\n","            S = self.get_game_data(game)\n","\n","            if visual:\n","                game.create_window()\n","                # The main loop, it pump key_presses and update every tick.\n","                environ['SDL_VIDEO_CENTERED'] = '1' # Centering the window\n","                previous_size = game.snake.length # Initial size of the snake\n","                color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\\\n","                                               previous_size)\n","\n","            while not game.game_over:\n","                action, value = q_policy.select_action(self.model, S, epoch, nb_actions)\n","                game.play(action)\n","                current_size = game.snake.length # Update the body size\n","\n","                if visual:\n","                    game.draw(color_list)\n","\n","                    if current_size > previous_size:\n","                        color_list = game.gradient([(42, 42, 42), (152, 152, 152)],\n","                                                   game.snake.length)\n","\n","                        previous_size = current_size\n","\n","                S = self.get_game_data(game)\n","\n","                if game.game_over:\n","                    result_size.append(current_size)\n","                    result_step.append(game.step)\n","\n","            if game.is_won():\n","                win_count += 1\n","\n","        print(\"Accuracy: {} %\".format(100. * win_count / nb_epoch))\n","        print(\"Mean size: {} | Biggest size: {} | Smallest size: {}\"\\\n","              .format(np.mean(result_size), np.max(result_size),\n","                      np.min(result_size)))\n","        print(\"Mean steps: {} | Biggest step: {} | Smallest step: {}\"\\\n","              .format(np.mean(result_step), np.max(result_step),\\\n","                      np.min(result_step)))\n","\n","import random\n","import numpy as np\n","\n","class LinearSchedule(object):\n","    def __init__(self, schedule_timesteps, final_p, initial_p):\n","        \"\"\"Linear interpolation between initial_p and final_p over\n","        schedule_timesteps. After this many timesteps pass final_p is\n","        returned.\n","        Parameters\n","        ----------\n","        schedule_timesteps: int\n","            Number of timesteps for which to linearly anneal initial_p\n","            to final_p\n","        initial_p: float\n","            initial output value\n","        final_p: float\n","            final output value\n","        \"\"\"\n","        self.schedule_timesteps = schedule_timesteps\n","        self.final_p = final_p\n","        self.initial_p = initial_p\n","\n","    def value(self, t):\n","        \"\"\"See Schedule.value\"\"\"\n","        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n","        return self.initial_p + fraction * (self.final_p - self.initial_p)\n","\n","\n","class GreedyQPolicy:\n","    \"\"\"Implement the greedy policy\n","\n","    Greedy policy always takes current best action.\n","    \"\"\"\n","    def __init__(self):\n","        super(GreedyQPolicy, self).__init__()\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)\n","        action = int(np.argmax(q[0]))\n","\n","        return action, 0\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of GreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(GreedyQPolicy, self).get_config()\n","        return config\n","\n","\n","class EpsGreedyQPolicy:\n","    \"\"\"Implement the epsilon greedy policy\n","\n","    Eps Greedy policy either:\n","\n","    - takes a random action with probability epsilon\n","    - takes current best action with prob (1 - epsilon)\n","    \"\"\"\n","    def __init__(self, max_eps=1., min_eps = .01, nb_epoch = 10000):\n","        super(EpsGreedyQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_eps, max_eps)\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        rand = random.random()\n","        self.eps = self.schedule.value(epoch)\n","\n","        if rand < self.eps:\n","            action = int(nb_actions * rand)\n","        else:\n","            q = model.predict(state)\n","            action = int(np.argmax(q[0]))\n","\n","        return action, self.eps\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of EpsGreedyQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(EpsGreedyQPolicy, self).get_config()\n","        config['eps'] = self.eps\n","        return config\n","\n","\n","class BoltzmannQPolicy:\n","    \"\"\"Implement the Boltzmann Q Policy\n","    Boltzmann Q Policy builds a probability law on q values and returns\n","    an action selected randomly according to this law.\n","    \"\"\"\n","    def __init__(self, max_temp = 1., min_temp = .01, nb_epoch = 10000, clip = (-500., 500.)):\n","        super(BoltzmannQPolicy, self).__init__()\n","        self.schedule = LinearSchedule(nb_epoch, min_temp, max_temp)\n","        self.clip = clip\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        self.temp = self.schedule.value(epoch)\n","        arg = q / self.temp\n","\n","        exp_values = np.exp(arg - arg.max())\n","        probs = exp_values / exp_values.sum()\n","        action = np.random.choice(range(nb_actions), p = probs)\n","\n","        return action, self.temp\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannQPolicy, self).get_config()\n","        config['temp'] = self.temp\n","        config['clip'] = self.clip\n","        return config\n","\n","\n","class BoltzmannGumbelQPolicy:\n","    \"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n","    based on the paper Boltzmann Exploration Done Right\n","    (https://arxiv.org/pdf/1705.10257.pdf).\n","    BGE is invariant with respect to the mean of the rewards but not their\n","    variance. The parameter C, which defaults to 1, can be used to correct for\n","    this, and should be set to the least upper bound on the standard deviation\n","    of the rewards.\n","    BGE is only available for training, not testing. For testing purposes, you\n","    can achieve approximately the same result as BGE after training for N steps\n","    on K actions with parameter C by using the BoltzmannQPolicy and setting\n","    tau = C/sqrt(N/K).\"\"\"\n","\n","    def __init__(self, C = 1.0):\n","        super(BoltzmannGumbelQPolicy, self).__init__()\n","        self.C = C\n","        self.action_counts = None\n","\n","    def select_action(self, model, state, epoch, nb_actions):\n","        \"\"\"Return the selected action\n","        # Arguments\n","            q_values (np.ndarray): List of the estimations of Q for each action\n","        # Returns\n","            Selection action\n","        \"\"\"\n","        q = model.predict(state)[0]\n","        q = q.astype('float64')\n","\n","        # If we are starting training, we should reset the action_counts.\n","        # Otherwise, action_counts should already be initialized, since we\n","        # always do so when we begin training.\n","        if epoch == 0:\n","            self.action_counts = np.ones(q.shape)\n","\n","        beta = self.C/np.sqrt(self.action_counts)\n","        Z = np.random.gumbel(size = q.shape)\n","\n","        perturbation = beta * Z\n","        perturbed_q_values = q + perturbation\n","        action = np.argmax(perturbed_q_values)\n","\n","        self.action_counts[action] += 1\n","        return action, np.sum(self.action_counts)\n","\n","    def get_config(self):\n","        \"\"\"Return configurations of BoltzmannGumbelQPolicy\n","        # Returns\n","            Dict of config\n","        \"\"\"\n","        config = super(BoltzmannGumbelQPolicy, self).get_config()\n","        config['C'] = self.C\n","        return config\n","\n","#!/usr/bin/env python\n","\n","\"\"\"clipped_error: L1 for errors < clip_value else L2 error.\n","\n","Functions:\n","    huber_loss: Return L1 error if absolute error is less than clip_value, else\n","                return L2 error.\n","    clipped_error: Call huber_loss with default clip_value to 1.0.\n","\"\"\"\n","\n","import numpy as np\n","from keras import backend as K\n","import tensorflow as tf\n","\n","__author__ = \"Victor Neves\"\n","__license__ = \"MIT\"\n","__maintainer__ = \"Victor Neves\"\n","__email__ = \"victorneves478@gmail.com\"\n","\n","def huber_loss(y_true, y_pred, clip_value):\n","\t# Huber loss, see https://en.wikipedia.org/wiki/Huber_loss and\n","\t# https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n","\t# for details.\n","\tassert clip_value > 0.\n","\n","\tx = y_true - y_pred\n","\tif np.isinf(clip_value):\n","\t\t# Spacial case for infinity since Tensorflow does have problems\n","\t\t# if we compare `K.abs(x) < np.inf`.\n","\t\treturn .5 * K.square(x)\n","\n","\tcondition = K.abs(x) < clip_value\n","\tsquared_loss = .5 * K.square(x)\n","\tlinear_loss = clip_value * (K.abs(x) - .5 * clip_value)\n","\tif K.backend() == 'tensorflow':\n","\t\tif hasattr(tf, 'select'):\n","\t\t\treturn tf.select(condition, squared_loss, linear_loss)  # condition, true, false\n","\t\telse:\n","\t\t\treturn tf.where(condition, squared_loss, linear_loss)  # condition, true, false\n","\telif K.backend() == 'theano':\n","\t\tfrom theano import tensor as T\n","\t\treturn T.switch(condition, squared_loss, linear_loss)\n","\telse:\n","\t\traise RuntimeError('Unknown backend \"{}\".'.format(K.backend()))\n","\n","def clipped_error(y_true, y_pred):\n","\treturn K.mean(huber_loss(y_true, y_pred, clip_value = 1.), axis = -1)\n","\n","#def CNN1(optimizer, loss, stack, input_size, output_size):\n"," #   model = Sequential()\n","  #  model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (stack,\n","   #                                                                  input_size,\n","    #                                                                 input_size)))\n","#    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n"," #   model.add(Conv2D(128, (3, 3), activation = 'relu'))\n","  #  model.add(Conv2D(256, (3, 3), activation = 'relu'))\n","   # model.add(Flatten())\n","    #model.add(Dense(1024, activation = 'relu'))\n","    #model.add(Dense(output_size))\n","    #model.compile(optimizer = optimizer, loss = loss)\n","\n","    #return model\n","    \n","def CNN4(optimizer, loss, stack, input_size, output_size):\n","    \"\"\"From @Kaixhin implementation's of the Rainbow paper.\"\"\"\n","    model = Sequential()\n","    model.add(Conv2D(32, (4, 4), activation = 'relu', input_shape = (stack,\n","                                                                    input_size,\n","                                                                    input_size)))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Conv2D(64, (2, 2), activation = 'relu'))\n","    model.add(Flatten())\n","    model.add(Dense(3136, activation = 'relu'))\n","    model.add(Dense(output_size))\n","    model.compile(optimizer = optimizer, loss = loss)\n","\n","    return model\n","  \n","board_size = 10\n","nb_frames = 4\n","  \n","game = Game(player = \"ROBOT\", board_size = board_size,\n","                        local_state = True, relative_pos = False)\n","\n","model = CNN4(optimizer = RMSprop(), loss = clipped_error,\n","                            stack = nb_frames, input_size = board_size,\n","                            output_size = game.nb_actions)\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = -1,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","agent.train(game, batch_size = 64, nb_epoch = 10000, gamma = 0.95, policy = \"EpsGreedyQPolicy\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 010/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 11.8 | Wins: 2 | Win percentage: 20.0%\n","Epoch: 020/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.3 | Wins: 3 | Win percentage: 15.0%\n","Epoch: 030/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 7.5 | Wins: 3 | Win percentage: 10.0%\n","Epoch: 040/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 10.2 | Wins: 6 | Win percentage: 15.0%\n","Epoch: 050/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 9 | Win percentage: 18.0%\n","Epoch: 060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 10 | Win percentage: 16.7%\n","Epoch: 070/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 14.4 | Wins: 12 | Win percentage: 17.1%\n","Epoch: 080/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.6 | Wins: 12 | Win percentage: 15.0%\n","Epoch: 090/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.7 | Wins: 13 | Win percentage: 14.4%\n","Epoch: 100/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 12.2 | Wins: 13 | Win percentage: 13.0%\n","Epoch: 110/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 11.9 | Wins: 15 | Win percentage: 13.6%\n","Epoch: 120/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 16 | Win percentage: 13.3%\n","Epoch: 130/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.9 | Wins: 17 | Win percentage: 13.1%\n","Epoch: 140/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 19 | Win percentage: 13.6%\n","Epoch: 150/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 9.2 | Wins: 22 | Win percentage: 14.7%\n","Epoch: 160/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.3 | Wins: 22 | Win percentage: 13.8%\n","Epoch: 170/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.0 | Wins: 22 | Win percentage: 12.9%\n","Epoch: 180/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.7 | Wins: 22 | Win percentage: 12.2%\n","Epoch: 190/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.5 | Wins: 24 | Win percentage: 12.6%\n","Epoch: 200/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.8 | Wins: 25 | Win percentage: 12.5%\n","Epoch: 210/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.4 | Wins: 27 | Win percentage: 12.9%\n","Epoch: 220/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 20.6 | Wins: 28 | Win percentage: 12.7%\n","Epoch: 230/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.8 | Wins: 29 | Win percentage: 12.6%\n","Epoch: 240/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 9.4 | Wins: 29 | Win percentage: 12.1%\n","Epoch: 250/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.5 | Wins: 29 | Win percentage: 11.6%\n","Epoch: 260/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.8 | Wins: 29 | Win percentage: 11.2%\n","Epoch: 270/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.5 | Wins: 29 | Win percentage: 10.7%\n","Epoch: 280/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 30 | Win percentage: 10.7%\n","Epoch: 290/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 16.1 | Wins: 30 | Win percentage: 10.3%\n","Epoch: 300/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 34 | Win percentage: 11.3%\n","Epoch: 310/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.2 | Wins: 34 | Win percentage: 11.0%\n","Epoch: 320/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.0 | Wins: 35 | Win percentage: 10.9%\n","Epoch: 330/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 21.1 | Wins: 36 | Win percentage: 10.9%\n","Epoch: 340/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.8 | Wins: 37 | Win percentage: 10.9%\n","Epoch: 350/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 10.0 | Wins: 38 | Win percentage: 10.9%\n","Epoch: 360/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.4 | Wins: 41 | Win percentage: 11.4%\n","Epoch: 370/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 14.2 | Wins: 43 | Win percentage: 11.6%\n","Epoch: 380/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 45 | Win percentage: 11.8%\n","Epoch: 390/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.1 | Wins: 46 | Win percentage: 11.8%\n","Epoch: 400/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 13.0 | Wins: 48 | Win percentage: 12.0%\n","Epoch: 410/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.0 | Wins: 50 | Win percentage: 12.2%\n","Epoch: 420/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.2 | Wins: 53 | Win percentage: 12.6%\n","Epoch: 430/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 11.6 | Wins: 56 | Win percentage: 13.0%\n","Epoch: 440/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 60 | Win percentage: 13.6%\n","Epoch: 450/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.1 | Wins: 62 | Win percentage: 13.8%\n","Epoch: 460/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 7.3 | Wins: 63 | Win percentage: 13.7%\n","Epoch: 470/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.9 | Wins: 64 | Win percentage: 13.6%\n","Epoch: 480/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 21.5 | Wins: 65 | Win percentage: 13.5%\n","Epoch: 490/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 66 | Win percentage: 13.5%\n","Epoch: 500/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 15.7 | Wins: 66 | Win percentage: 13.2%\n","Epoch: 510/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 20.6 | Wins: 67 | Win percentage: 13.1%\n","Epoch: 520/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 17.9 | Wins: 67 | Win percentage: 12.9%\n","Epoch: 530/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 14.8 | Wins: 67 | Win percentage: 12.6%\n","Epoch: 540/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 13.6 | Wins: 69 | Win percentage: 12.8%\n","Epoch: 550/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 9.9 | Wins: 71 | Win percentage: 12.9%\n","Epoch: 560/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.5 | Wins: 72 | Win percentage: 12.9%\n","Epoch: 570/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 74 | Win percentage: 13.0%\n","Epoch: 580/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 21.6 | Wins: 77 | Win percentage: 13.3%\n","Epoch: 590/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.9 | Wins: 78 | Win percentage: 13.2%\n","Epoch: 600/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.1 | Wins: 79 | Win percentage: 13.2%\n","Epoch: 610/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 10.6 | Wins: 83 | Win percentage: 13.6%\n","Epoch: 620/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 17.9 | Wins: 84 | Win percentage: 13.5%\n","Epoch: 630/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.7 | Wins: 85 | Win percentage: 13.5%\n","Epoch: 640/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.1 | Wins: 87 | Win percentage: 13.6%\n","Epoch: 650/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 18.3 | Wins: 91 | Win percentage: 14.0%\n","Epoch: 660/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 14.9 | Wins: 95 | Win percentage: 14.4%\n","Epoch: 670/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 9.4 | Wins: 96 | Win percentage: 14.3%\n","Epoch: 680/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.0 | Wins: 98 | Win percentage: 14.4%\n","Epoch: 690/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 17.5 | Wins: 101 | Win percentage: 14.6%\n","Epoch: 700/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.3 | Wins: 102 | Win percentage: 14.6%\n","Epoch: 710/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 8.9 | Wins: 103 | Win percentage: 14.5%\n","Epoch: 720/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.1 | Wins: 105 | Win percentage: 14.6%\n","Epoch: 730/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 9.2 | Wins: 109 | Win percentage: 14.9%\n","Epoch: 740/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.8 | Wins: 111 | Win percentage: 15.0%\n","Epoch: 750/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 112 | Win percentage: 14.9%\n","Epoch: 760/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 15.4 | Wins: 116 | Win percentage: 15.3%\n","Epoch: 770/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.8 | Wins: 118 | Win percentage: 15.3%\n","Epoch: 780/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.8 | Wins: 120 | Win percentage: 15.4%\n","Epoch: 790/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 10.9 | Wins: 120 | Win percentage: 15.2%\n","Epoch: 800/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.5 | Wins: 121 | Win percentage: 15.1%\n","Epoch: 810/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.9 | Wins: 123 | Win percentage: 15.2%\n","Epoch: 820/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 25.4 | Wins: 125 | Win percentage: 15.2%\n","Epoch: 830/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 11.2 | Wins: 125 | Win percentage: 15.1%\n","Epoch: 840/10000 | Mean size 10: 3.0 | Longest 10: 003 | Mean steps 10: 13.2 | Wins: 125 | Win percentage: 14.9%\n","Epoch: 850/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 129 | Win percentage: 15.2%\n","Epoch: 860/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 16.6 | Wins: 130 | Win percentage: 15.1%\n","Epoch: 870/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 23.8 | Wins: 135 | Win percentage: 15.5%\n","Epoch: 880/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 18.2 | Wins: 137 | Win percentage: 15.6%\n","Epoch: 890/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 13.0 | Wins: 139 | Win percentage: 15.6%\n","Epoch: 900/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 20.6 | Wins: 142 | Win percentage: 15.8%\n","Epoch: 910/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 9.9 | Wins: 143 | Win percentage: 15.7%\n","Epoch: 920/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 9.2 | Wins: 145 | Win percentage: 15.8%\n","Epoch: 930/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 19.1 | Wins: 149 | Win percentage: 16.0%\n","Epoch: 940/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.7 | Wins: 152 | Win percentage: 16.2%\n","Epoch: 950/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 12.3 | Wins: 153 | Win percentage: 16.1%\n","Epoch: 960/10000 | Mean size 10: 3.2 | Longest 10: 005 | Mean steps 10: 13.4 | Wins: 154 | Win percentage: 16.0%\n","Epoch: 970/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 23.7 | Wins: 157 | Win percentage: 16.2%\n","Epoch: 980/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 13.5 | Wins: 160 | Win percentage: 16.3%\n","Epoch: 990/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 163 | Win percentage: 16.5%\n","Epoch: 1000/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 29.4 | Wins: 168 | Win percentage: 16.8%\n","Epoch: 1010/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.8 | Wins: 170 | Win percentage: 16.8%\n","Epoch: 1020/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 9.2 | Wins: 172 | Win percentage: 16.9%\n","Epoch: 1030/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 15.1 | Wins: 173 | Win percentage: 16.8%\n","Epoch: 1040/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 24.3 | Wins: 175 | Win percentage: 16.8%\n","Epoch: 1050/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 23.8 | Wins: 178 | Win percentage: 17.0%\n","Epoch: 1060/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 20.8 | Wins: 182 | Win percentage: 17.2%\n","Epoch: 1070/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.2 | Wins: 184 | Win percentage: 17.2%\n","Epoch: 1080/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 19.1 | Wins: 188 | Win percentage: 17.4%\n","Epoch: 1090/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 18.7 | Wins: 190 | Win percentage: 17.4%\n","Epoch: 1100/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.5 | Wins: 192 | Win percentage: 17.5%\n","Epoch: 1110/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 30.4 | Wins: 194 | Win percentage: 17.5%\n","Epoch: 1120/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.9 | Wins: 196 | Win percentage: 17.5%\n","Epoch: 1130/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 19.1 | Wins: 199 | Win percentage: 17.6%\n","Epoch: 1140/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 27.3 | Wins: 201 | Win percentage: 17.6%\n","Epoch: 1150/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 22.2 | Wins: 203 | Win percentage: 17.7%\n","Epoch: 1160/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 26.5 | Wins: 205 | Win percentage: 17.7%\n","Epoch: 1170/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 20.0 | Wins: 209 | Win percentage: 17.9%\n","Epoch: 1180/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 29.4 | Wins: 214 | Win percentage: 18.1%\n","Epoch: 1190/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 17.9 | Wins: 218 | Win percentage: 18.3%\n","Epoch: 1200/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 13.9 | Wins: 221 | Win percentage: 18.4%\n","Epoch: 1210/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 19.9 | Wins: 224 | Win percentage: 18.5%\n","Epoch: 1220/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 23.1 | Wins: 226 | Win percentage: 18.5%\n","Epoch: 1230/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 11.8 | Wins: 228 | Win percentage: 18.5%\n","Epoch: 1240/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.0 | Wins: 231 | Win percentage: 18.6%\n","Epoch: 1250/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 21.2 | Wins: 234 | Win percentage: 18.7%\n","Epoch: 1260/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.6 | Wins: 237 | Win percentage: 18.8%\n","Epoch: 1270/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 18.2 | Wins: 240 | Win percentage: 18.9%\n","Epoch: 1280/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 21.8 | Wins: 245 | Win percentage: 19.1%\n","Epoch: 1290/10000 | Mean size 10: 3.4 | Longest 10: 006 | Mean steps 10: 16.4 | Wins: 247 | Win percentage: 19.1%\n","Epoch: 1300/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 23.1 | Wins: 251 | Win percentage: 19.3%\n","Epoch: 1310/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 25.7 | Wins: 256 | Win percentage: 19.5%\n","Epoch: 1320/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 17.7 | Wins: 260 | Win percentage: 19.7%\n","Epoch: 1330/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 19.4 | Wins: 264 | Win percentage: 19.8%\n","Epoch: 1340/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 12.0 | Wins: 266 | Win percentage: 19.9%\n","Epoch: 1350/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.9 | Wins: 268 | Win percentage: 19.9%\n","Epoch: 1360/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 20.1 | Wins: 272 | Win percentage: 20.0%\n","Epoch: 1370/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 13.5 | Wins: 275 | Win percentage: 20.1%\n","Epoch: 1380/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 24.0 | Wins: 280 | Win percentage: 20.3%\n","Epoch: 1390/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 15.9 | Wins: 282 | Win percentage: 20.3%\n","Epoch: 1400/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 34.2 | Wins: 287 | Win percentage: 20.5%\n","Epoch: 1410/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.5 | Wins: 290 | Win percentage: 20.6%\n","Epoch: 1420/10000 | Mean size 10: 3.3 | Longest 10: 006 | Mean steps 10: 18.5 | Wins: 291 | Win percentage: 20.5%\n","Epoch: 1430/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 23.6 | Wins: 295 | Win percentage: 20.6%\n","Epoch: 1440/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 8.3 | Wins: 298 | Win percentage: 20.7%\n","Epoch: 1450/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 20.9 | Wins: 300 | Win percentage: 20.7%\n","Epoch: 1460/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 26.9 | Wins: 306 | Win percentage: 21.0%\n","Epoch: 1470/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 15.5 | Wins: 309 | Win percentage: 21.0%\n","Epoch: 1480/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 31.2 | Wins: 312 | Win percentage: 21.1%\n","Epoch: 1490/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 14.2 | Wins: 313 | Win percentage: 21.0%\n","Epoch: 1500/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 32.7 | Wins: 317 | Win percentage: 21.1%\n","Epoch: 1510/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 14.3 | Wins: 321 | Win percentage: 21.3%\n","Epoch: 1520/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 15.8 | Wins: 324 | Win percentage: 21.3%\n","Epoch: 1530/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 14.5 | Wins: 326 | Win percentage: 21.3%\n","Epoch: 1540/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 14.1 | Wins: 329 | Win percentage: 21.4%\n","Epoch: 1550/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 24.4 | Wins: 334 | Win percentage: 21.5%\n","Epoch: 1560/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.7 | Wins: 337 | Win percentage: 21.6%\n","Epoch: 1570/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 32.8 | Wins: 340 | Win percentage: 21.7%\n","Epoch: 1580/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 18.4 | Wins: 343 | Win percentage: 21.7%\n","Epoch: 1590/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 15.6 | Wins: 347 | Win percentage: 21.8%\n","Epoch: 1600/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 37.9 | Wins: 353 | Win percentage: 22.1%\n","Epoch: 1610/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 18.5 | Wins: 358 | Win percentage: 22.2%\n","Epoch: 1620/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 16.3 | Wins: 362 | Win percentage: 22.3%\n","Epoch: 1630/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 13.2 | Wins: 365 | Win percentage: 22.4%\n","Epoch: 1640/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 20.6 | Wins: 368 | Win percentage: 22.4%\n","Epoch: 1650/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.6 | Wins: 370 | Win percentage: 22.4%\n","Epoch: 1660/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 20.1 | Wins: 373 | Win percentage: 22.5%\n","Epoch: 1670/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 25.4 | Wins: 379 | Win percentage: 22.7%\n","Epoch: 1680/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 32.1 | Wins: 383 | Win percentage: 22.8%\n","Epoch: 1690/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 17.8 | Wins: 385 | Win percentage: 22.8%\n","Epoch: 1700/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 18.5 | Wins: 388 | Win percentage: 22.8%\n","Epoch: 1710/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 18.1 | Wins: 392 | Win percentage: 22.9%\n","Epoch: 1720/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 19.9 | Wins: 397 | Win percentage: 23.1%\n","Epoch: 1730/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 26.4 | Wins: 401 | Win percentage: 23.2%\n","Epoch: 1740/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 15.0 | Wins: 404 | Win percentage: 23.2%\n","Epoch: 1750/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 16.8 | Wins: 409 | Win percentage: 23.4%\n","Epoch: 1760/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 18.0 | Wins: 413 | Win percentage: 23.5%\n","Epoch: 1770/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 21.5 | Wins: 416 | Win percentage: 23.5%\n","Epoch: 1780/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 17.0 | Wins: 420 | Win percentage: 23.6%\n","Epoch: 1790/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 17.9 | Wins: 423 | Win percentage: 23.6%\n","Epoch: 1800/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 19.5 | Wins: 426 | Win percentage: 23.7%\n","Epoch: 1810/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 17.3 | Wins: 429 | Win percentage: 23.7%\n","Epoch: 1820/10000 | Mean size 10: 3.7 | Longest 10: 004 | Mean steps 10: 18.9 | Wins: 436 | Win percentage: 24.0%\n","Epoch: 1830/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 24.3 | Wins: 440 | Win percentage: 24.0%\n","Epoch: 1840/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 11.1 | Wins: 445 | Win percentage: 24.2%\n","Epoch: 1850/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 23.1 | Wins: 450 | Win percentage: 24.3%\n","Epoch: 1860/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 14.8 | Wins: 452 | Win percentage: 24.3%\n","Epoch: 1870/10000 | Mean size 10: 4.1 | Longest 10: 008 | Mean steps 10: 37.2 | Wins: 457 | Win percentage: 24.4%\n","Epoch: 1880/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 25.5 | Wins: 462 | Win percentage: 24.6%\n","Epoch: 1890/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 23.6 | Wins: 467 | Win percentage: 24.7%\n","Epoch: 1900/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 21.0 | Wins: 472 | Win percentage: 24.8%\n","Epoch: 1910/10000 | Mean size 10: 3.3 | Longest 10: 005 | Mean steps 10: 18.6 | Wins: 474 | Win percentage: 24.8%\n","Epoch: 1920/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 16.4 | Wins: 477 | Win percentage: 24.8%\n","Epoch: 1930/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 21.9 | Wins: 483 | Win percentage: 25.0%\n","Epoch: 1940/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 27.5 | Wins: 487 | Win percentage: 25.1%\n","Epoch: 1950/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 30.5 | Wins: 493 | Win percentage: 25.3%\n","Epoch: 1960/10000 | Mean size 10: 3.2 | Longest 10: 004 | Mean steps 10: 16.7 | Wins: 495 | Win percentage: 25.3%\n","Epoch: 1970/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 25.1 | Wins: 499 | Win percentage: 25.3%\n","Epoch: 1980/10000 | Mean size 10: 3.6 | Longest 10: 006 | Mean steps 10: 20.3 | Wins: 502 | Win percentage: 25.4%\n","Epoch: 1990/10000 | Mean size 10: 3.6 | Longest 10: 004 | Mean steps 10: 21.4 | Wins: 508 | Win percentage: 25.5%\n","Epoch: 2000/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 20.8 | Wins: 513 | Win percentage: 25.6%\n","Epoch: 2010/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 21.8 | Wins: 520 | Win percentage: 25.9%\n","Epoch: 2020/10000 | Mean size 10: 3.6 | Longest 10: 004 | Mean steps 10: 19.6 | Wins: 526 | Win percentage: 26.0%\n","Epoch: 2030/10000 | Mean size 10: 4.3 | Longest 10: 007 | Mean steps 10: 30.1 | Wins: 533 | Win percentage: 26.3%\n","Epoch: 2040/10000 | Mean size 10: 3.4 | Longest 10: 004 | Mean steps 10: 12.0 | Wins: 537 | Win percentage: 26.3%\n","Epoch: 2050/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 23.7 | Wins: 541 | Win percentage: 26.4%\n","Epoch: 2060/10000 | Mean size 10: 3.1 | Longest 10: 004 | Mean steps 10: 11.7 | Wins: 542 | Win percentage: 26.3%\n","Epoch: 2070/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 20.4 | Wins: 546 | Win percentage: 26.4%\n","Epoch: 2080/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 27.0 | Wins: 550 | Win percentage: 26.4%\n","Epoch: 2090/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 27.9 | Wins: 554 | Win percentage: 26.5%\n","Epoch: 2100/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 31.4 | Wins: 561 | Win percentage: 26.7%\n","Epoch: 2110/10000 | Mean size 10: 4.1 | Longest 10: 008 | Mean steps 10: 27.1 | Wins: 567 | Win percentage: 26.9%\n","Epoch: 2120/10000 | Mean size 10: 4.0 | Longest 10: 007 | Mean steps 10: 17.9 | Wins: 572 | Win percentage: 27.0%\n","Epoch: 2130/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 32.2 | Wins: 579 | Win percentage: 27.2%\n","Epoch: 2140/10000 | Mean size 10: 3.3 | Longest 10: 004 | Mean steps 10: 11.9 | Wins: 582 | Win percentage: 27.2%\n","Epoch: 2150/10000 | Mean size 10: 3.8 | Longest 10: 006 | Mean steps 10: 29.6 | Wins: 587 | Win percentage: 27.3%\n","Epoch: 2160/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 18.9 | Wins: 592 | Win percentage: 27.4%\n","Epoch: 2170/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 21.0 | Wins: 599 | Win percentage: 27.6%\n","Epoch: 2180/10000 | Mean size 10: 4.4 | Longest 10: 009 | Mean steps 10: 45.1 | Wins: 605 | Win percentage: 27.8%\n","Epoch: 2190/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 17.0 | Wins: 610 | Win percentage: 27.9%\n","Epoch: 2200/10000 | Mean size 10: 4.5 | Longest 10: 006 | Mean steps 10: 30.8 | Wins: 619 | Win percentage: 28.1%\n","Epoch: 2210/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 16.1 | Wins: 623 | Win percentage: 28.2%\n","Epoch: 2220/10000 | Mean size 10: 3.9 | Longest 10: 005 | Mean steps 10: 35.9 | Wins: 630 | Win percentage: 28.4%\n","Epoch: 2230/10000 | Mean size 10: 3.5 | Longest 10: 006 | Mean steps 10: 24.3 | Wins: 633 | Win percentage: 28.4%\n","Epoch: 2240/10000 | Mean size 10: 4.2 | Longest 10: 005 | Mean steps 10: 39.1 | Wins: 640 | Win percentage: 28.6%\n","Epoch: 2250/10000 | Mean size 10: 3.9 | Longest 10: 005 | Mean steps 10: 24.6 | Wins: 646 | Win percentage: 28.7%\n","Epoch: 2260/10000 | Mean size 10: 4.0 | Longest 10: 007 | Mean steps 10: 26.7 | Wins: 653 | Win percentage: 28.9%\n","Epoch: 2270/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 27.9 | Wins: 659 | Win percentage: 29.0%\n","Epoch: 2280/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 18.0 | Wins: 662 | Win percentage: 29.0%\n","Epoch: 2290/10000 | Mean size 10: 4.4 | Longest 10: 006 | Mean steps 10: 32.0 | Wins: 670 | Win percentage: 29.3%\n","Epoch: 2300/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 18.9 | Wins: 675 | Win percentage: 29.3%\n","Epoch: 2310/10000 | Mean size 10: 3.4 | Longest 10: 005 | Mean steps 10: 17.9 | Wins: 678 | Win percentage: 29.4%\n","Epoch: 2320/10000 | Mean size 10: 4.1 | Longest 10: 005 | Mean steps 10: 23.8 | Wins: 686 | Win percentage: 29.6%\n","Epoch: 2330/10000 | Mean size 10: 3.7 | Longest 10: 006 | Mean steps 10: 29.0 | Wins: 689 | Win percentage: 29.6%\n","Epoch: 2340/10000 | Mean size 10: 3.9 | Longest 10: 005 | Mean steps 10: 16.8 | Wins: 696 | Win percentage: 29.7%\n","Epoch: 2350/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 23.9 | Wins: 701 | Win percentage: 29.8%\n","Epoch: 2360/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 24.2 | Wins: 707 | Win percentage: 30.0%\n","Epoch: 2370/10000 | Mean size 10: 3.6 | Longest 10: 006 | Mean steps 10: 15.8 | Wins: 711 | Win percentage: 30.0%\n","Epoch: 2380/10000 | Mean size 10: 3.5 | Longest 10: 004 | Mean steps 10: 19.1 | Wins: 716 | Win percentage: 30.1%\n","Epoch: 2390/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 20.0 | Wins: 720 | Win percentage: 30.1%\n","Epoch: 2400/10000 | Mean size 10: 3.6 | Longest 10: 005 | Mean steps 10: 19.1 | Wins: 724 | Win percentage: 30.2%\n","Epoch: 2410/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 34.1 | Wins: 729 | Win percentage: 30.2%\n","Epoch: 2420/10000 | Mean size 10: 4.1 | Longest 10: 007 | Mean steps 10: 30.2 | Wins: 735 | Win percentage: 30.4%\n","Epoch: 2430/10000 | Mean size 10: 4.3 | Longest 10: 005 | Mean steps 10: 27.1 | Wins: 744 | Win percentage: 30.6%\n","Epoch: 2440/10000 | Mean size 10: 4.3 | Longest 10: 006 | Mean steps 10: 34.4 | Wins: 752 | Win percentage: 30.8%\n","Epoch: 2450/10000 | Mean size 10: 4.2 | Longest 10: 005 | Mean steps 10: 18.8 | Wins: 761 | Win percentage: 31.1%\n","Epoch: 2460/10000 | Mean size 10: 3.6 | Longest 10: 004 | Mean steps 10: 21.1 | Wins: 767 | Win percentage: 31.2%\n","Epoch: 2470/10000 | Mean size 10: 4.6 | Longest 10: 007 | Mean steps 10: 29.3 | Wins: 775 | Win percentage: 31.4%\n","Epoch: 2480/10000 | Mean size 10: 4.5 | Longest 10: 007 | Mean steps 10: 29.0 | Wins: 782 | Win percentage: 31.5%\n","Epoch: 2490/10000 | Mean size 10: 4.2 | Longest 10: 007 | Mean steps 10: 24.6 | Wins: 789 | Win percentage: 31.7%\n","Epoch: 2500/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 22.8 | Wins: 795 | Win percentage: 31.8%\n","Epoch: 2510/10000 | Mean size 10: 4.2 | Longest 10: 007 | Mean steps 10: 29.6 | Wins: 801 | Win percentage: 31.9%\n","Epoch: 2520/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 19.0 | Wins: 806 | Win percentage: 32.0%\n","Epoch: 2530/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 20.7 | Wins: 812 | Win percentage: 32.1%\n","Epoch: 2540/10000 | Mean size 10: 4.3 | Longest 10: 006 | Mean steps 10: 21.7 | Wins: 818 | Win percentage: 32.2%\n","Epoch: 2550/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 27.8 | Wins: 826 | Win percentage: 32.4%\n","Epoch: 2560/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 19.4 | Wins: 833 | Win percentage: 32.5%\n","Epoch: 2570/10000 | Mean size 10: 4.6 | Longest 10: 008 | Mean steps 10: 35.7 | Wins: 839 | Win percentage: 32.6%\n","Epoch: 2580/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 19.5 | Wins: 847 | Win percentage: 32.8%\n","Epoch: 2590/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 25.6 | Wins: 853 | Win percentage: 32.9%\n","Epoch: 2600/10000 | Mean size 10: 3.9 | Longest 10: 005 | Mean steps 10: 22.9 | Wins: 860 | Win percentage: 33.1%\n","Epoch: 2610/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 27.1 | Wins: 868 | Win percentage: 33.3%\n","Epoch: 2620/10000 | Mean size 10: 4.2 | Longest 10: 006 | Mean steps 10: 25.2 | Wins: 874 | Win percentage: 33.4%\n","Epoch: 2630/10000 | Mean size 10: 4.1 | Longest 10: 005 | Mean steps 10: 24.6 | Wins: 881 | Win percentage: 33.5%\n","Epoch: 2640/10000 | Mean size 10: 4.1 | Longest 10: 005 | Mean steps 10: 22.4 | Wins: 889 | Win percentage: 33.7%\n","Epoch: 2650/10000 | Mean size 10: 4.5 | Longest 10: 007 | Mean steps 10: 35.2 | Wins: 897 | Win percentage: 33.8%\n","Epoch: 2660/10000 | Mean size 10: 4.4 | Longest 10: 006 | Mean steps 10: 24.4 | Wins: 905 | Win percentage: 34.0%\n","Epoch: 2670/10000 | Mean size 10: 4.0 | Longest 10: 007 | Mean steps 10: 17.8 | Wins: 910 | Win percentage: 34.1%\n","Epoch: 2680/10000 | Mean size 10: 3.5 | Longest 10: 005 | Mean steps 10: 20.3 | Wins: 914 | Win percentage: 34.1%\n","Epoch: 2690/10000 | Mean size 10: 4.8 | Longest 10: 007 | Mean steps 10: 35.0 | Wins: 924 | Win percentage: 34.3%\n","Epoch: 2700/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 25.6 | Wins: 932 | Win percentage: 34.5%\n","Epoch: 2710/10000 | Mean size 10: 4.2 | Longest 10: 005 | Mean steps 10: 25.5 | Wins: 939 | Win percentage: 34.6%\n","Epoch: 2720/10000 | Mean size 10: 3.7 | Longest 10: 005 | Mean steps 10: 24.2 | Wins: 944 | Win percentage: 34.7%\n","Epoch: 2730/10000 | Mean size 10: 4.4 | Longest 10: 007 | Mean steps 10: 36.4 | Wins: 951 | Win percentage: 34.8%\n","Epoch: 2740/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 34.7 | Wins: 959 | Win percentage: 35.0%\n","Epoch: 2750/10000 | Mean size 10: 4.2 | Longest 10: 005 | Mean steps 10: 21.5 | Wins: 968 | Win percentage: 35.2%\n","Epoch: 2760/10000 | Mean size 10: 4.8 | Longest 10: 007 | Mean steps 10: 32.0 | Wins: 977 | Win percentage: 35.4%\n","Epoch: 2770/10000 | Mean size 10: 4.8 | Longest 10: 008 | Mean steps 10: 32.9 | Wins: 986 | Win percentage: 35.6%\n","Epoch: 2780/10000 | Mean size 10: 4.5 | Longest 10: 007 | Mean steps 10: 26.2 | Wins: 993 | Win percentage: 35.7%\n","Epoch: 2790/10000 | Mean size 10: 4.8 | Longest 10: 006 | Mean steps 10: 27.1 | Wins: 1003 | Win percentage: 35.9%\n","Epoch: 2800/10000 | Mean size 10: 4.3 | Longest 10: 006 | Mean steps 10: 20.4 | Wins: 1012 | Win percentage: 36.1%\n","Epoch: 2810/10000 | Mean size 10: 3.9 | Longest 10: 006 | Mean steps 10: 28.2 | Wins: 1018 | Win percentage: 36.2%\n","Epoch: 2820/10000 | Mean size 10: 4.3 | Longest 10: 005 | Mean steps 10: 29.4 | Wins: 1026 | Win percentage: 36.4%\n","Epoch: 2830/10000 | Mean size 10: 3.8 | Longest 10: 005 | Mean steps 10: 14.3 | Wins: 1032 | Win percentage: 36.5%\n","Epoch: 2840/10000 | Mean size 10: 4.4 | Longest 10: 007 | Mean steps 10: 37.2 | Wins: 1039 | Win percentage: 36.6%\n","Epoch: 2850/10000 | Mean size 10: 4.5 | Longest 10: 007 | Mean steps 10: 25.9 | Wins: 1048 | Win percentage: 36.8%\n","Epoch: 2860/10000 | Mean size 10: 4.0 | Longest 10: 005 | Mean steps 10: 22.3 | Wins: 1056 | Win percentage: 36.9%\n","Epoch: 2870/10000 | Mean size 10: 4.1 | Longest 10: 006 | Mean steps 10: 14.2 | Wins: 1063 | Win percentage: 37.0%\n","Epoch: 2880/10000 | Mean size 10: 5.3 | Longest 10: 008 | Mean steps 10: 32.0 | Wins: 1073 | Win percentage: 37.3%\n","Epoch: 2890/10000 | Mean size 10: 4.2 | Longest 10: 005 | Mean steps 10: 27.8 | Wins: 1081 | Win percentage: 37.4%\n","Epoch: 2900/10000 | Mean size 10: 4.4 | Longest 10: 006 | Mean steps 10: 23.8 | Wins: 1089 | Win percentage: 37.6%\n","Epoch: 2910/10000 | Mean size 10: 4.9 | Longest 10: 007 | Mean steps 10: 29.2 | Wins: 1097 | Win percentage: 37.7%\n","Epoch: 2920/10000 | Mean size 10: 4.8 | Longest 10: 007 | Mean steps 10: 31.8 | Wins: 1103 | Win percentage: 37.8%\n","Epoch: 2930/10000 | Mean size 10: 4.3 | Longest 10: 008 | Mean steps 10: 25.8 | Wins: 1110 | Win percentage: 37.9%\n","Epoch: 2940/10000 | Mean size 10: 4.5 | Longest 10: 007 | Mean steps 10: 28.2 | Wins: 1118 | Win percentage: 38.0%\n","Epoch: 2950/10000 | Mean size 10: 4.0 | Longest 10: 006 | Mean steps 10: 23.2 | Wins: 1125 | Win percentage: 38.1%\n","Epoch: 2960/10000 | Mean size 10: 4.2 | Longest 10: 006 | Mean steps 10: 24.3 | Wins: 1132 | Win percentage: 38.2%\n","Epoch: 2970/10000 | Mean size 10: 4.5 | Longest 10: 006 | Mean steps 10: 48.8 | Wins: 1141 | Win percentage: 38.4%\n","Epoch: 2980/10000 | Mean size 10: 4.9 | Longest 10: 009 | Mean steps 10: 24.3 | Wins: 1150 | Win percentage: 38.6%\n","Epoch: 2990/10000 | Mean size 10: 5.6 | Longest 10: 010 | Mean steps 10: 47.5 | Wins: 1159 | Win percentage: 38.8%\n","Epoch: 3000/10000 | Mean size 10: 4.6 | Longest 10: 006 | Mean steps 10: 25.3 | Wins: 1167 | Win percentage: 38.9%\n","Epoch: 3010/10000 | Mean size 10: 4.8 | Longest 10: 006 | Mean steps 10: 35.3 | Wins: 1175 | Win percentage: 39.0%\n","Epoch: 3020/10000 | Mean size 10: 5.1 | Longest 10: 008 | Mean steps 10: 35.3 | Wins: 1184 | Win percentage: 39.2%\n","Epoch: 3030/10000 | Mean size 10: 4.6 | Longest 10: 006 | Mean steps 10: 30.2 | Wins: 1192 | Win percentage: 39.3%\n","Epoch: 3040/10000 | Mean size 10: 4.8 | Longest 10: 007 | Mean steps 10: 35.7 | Wins: 1200 | Win percentage: 39.5%\n","Epoch: 3050/10000 | Mean size 10: 4.3 | Longest 10: 006 | Mean steps 10: 21.6 | Wins: 1207 | Win percentage: 39.6%\n","Epoch: 3060/10000 | Mean size 10: 4.8 | Longest 10: 007 | Mean steps 10: 27.2 | Wins: 1216 | Win percentage: 39.7%\n","Epoch: 3070/10000 | Mean size 10: 3.9 | Longest 10: 007 | Mean steps 10: 21.0 | Wins: 1221 | Win percentage: 39.8%\n","Epoch: 3080/10000 | Mean size 10: 5.3 | Longest 10: 009 | Mean steps 10: 36.3 | Wins: 1230 | Win percentage: 39.9%\n","Epoch: 3090/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 26.6 | Wins: 1239 | Win percentage: 40.1%\n","Epoch: 3100/10000 | Mean size 10: 4.7 | Longest 10: 009 | Mean steps 10: 28.3 | Wins: 1248 | Win percentage: 40.3%\n","Epoch: 3110/10000 | Mean size 10: 4.6 | Longest 10: 010 | Mean steps 10: 36.1 | Wins: 1256 | Win percentage: 40.4%\n","Epoch: 3120/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 30.0 | Wins: 1265 | Win percentage: 40.5%\n","Epoch: 3130/10000 | Mean size 10: 4.9 | Longest 10: 006 | Mean steps 10: 36.8 | Wins: 1274 | Win percentage: 40.7%\n","Epoch: 3140/10000 | Mean size 10: 4.9 | Longest 10: 011 | Mean steps 10: 30.2 | Wins: 1282 | Win percentage: 40.8%\n","Epoch: 3150/10000 | Mean size 10: 4.3 | Longest 10: 006 | Mean steps 10: 19.8 | Wins: 1291 | Win percentage: 41.0%\n","Epoch: 3160/10000 | Mean size 10: 4.7 | Longest 10: 006 | Mean steps 10: 23.8 | Wins: 1299 | Win percentage: 41.1%\n","Epoch: 3170/10000 | Mean size 10: 5.5 | Longest 10: 008 | Mean steps 10: 33.6 | Wins: 1308 | Win percentage: 41.3%\n","Epoch: 3180/10000 | Mean size 10: 4.8 | Longest 10: 007 | Mean steps 10: 24.2 | Wins: 1316 | Win percentage: 41.4%\n","Epoch: 3190/10000 | Mean size 10: 5.2 | Longest 10: 007 | Mean steps 10: 32.3 | Wins: 1325 | Win percentage: 41.5%\n","Epoch: 3200/10000 | Mean size 10: 4.9 | Longest 10: 006 | Mean steps 10: 27.6 | Wins: 1334 | Win percentage: 41.7%\n","Epoch: 3210/10000 | Mean size 10: 5.0 | Longest 10: 007 | Mean steps 10: 27.9 | Wins: 1343 | Win percentage: 41.8%\n","Epoch: 3220/10000 | Mean size 10: 4.2 | Longest 10: 009 | Mean steps 10: 20.7 | Wins: 1348 | Win percentage: 41.9%\n","Epoch: 3230/10000 | Mean size 10: 5.2 | Longest 10: 008 | Mean steps 10: 41.3 | Wins: 1357 | Win percentage: 42.0%\n","Epoch: 3240/10000 | Mean size 10: 5.5 | Longest 10: 009 | Mean steps 10: 34.5 | Wins: 1367 | Win percentage: 42.2%\n","Epoch: 3250/10000 | Mean size 10: 4.9 | Longest 10: 008 | Mean steps 10: 30.3 | Wins: 1376 | Win percentage: 42.3%\n","Epoch: 3260/10000 | Mean size 10: 4.4 | Longest 10: 007 | Mean steps 10: 24.3 | Wins: 1384 | Win percentage: 42.5%\n","Epoch: 3270/10000 | Mean size 10: 5.2 | Longest 10: 009 | Mean steps 10: 31.3 | Wins: 1394 | Win percentage: 42.6%\n","Epoch: 3280/10000 | Mean size 10: 5.0 | Longest 10: 008 | Mean steps 10: 33.6 | Wins: 1402 | Win percentage: 42.7%\n","Epoch: 3290/10000 | Mean size 10: 4.4 | Longest 10: 005 | Mean steps 10: 23.2 | Wins: 1411 | Win percentage: 42.9%\n","Epoch: 3300/10000 | Mean size 10: 4.8 | Longest 10: 007 | Mean steps 10: 28.4 | Wins: 1418 | Win percentage: 43.0%\n","Epoch: 3310/10000 | Mean size 10: 5.3 | Longest 10: 008 | Mean steps 10: 34.0 | Wins: 1427 | Win percentage: 43.1%\n","Epoch: 3320/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 25.3 | Wins: 1435 | Win percentage: 43.2%\n","Epoch: 3330/10000 | Mean size 10: 4.5 | Longest 10: 007 | Mean steps 10: 21.4 | Wins: 1443 | Win percentage: 43.3%\n","Epoch: 3340/10000 | Mean size 10: 4.6 | Longest 10: 007 | Mean steps 10: 23.5 | Wins: 1452 | Win percentage: 43.5%\n","Epoch: 3350/10000 | Mean size 10: 5.3 | Longest 10: 009 | Mean steps 10: 36.2 | Wins: 1459 | Win percentage: 43.6%\n","Epoch: 3360/10000 | Mean size 10: 4.7 | Longest 10: 009 | Mean steps 10: 22.2 | Wins: 1466 | Win percentage: 43.6%\n","Epoch: 3370/10000 | Mean size 10: 5.4 | Longest 10: 008 | Mean steps 10: 36.6 | Wins: 1475 | Win percentage: 43.8%\n","Epoch: 3380/10000 | Mean size 10: 4.4 | Longest 10: 006 | Mean steps 10: 22.0 | Wins: 1483 | Win percentage: 43.9%\n","Epoch: 3390/10000 | Mean size 10: 5.1 | Longest 10: 008 | Mean steps 10: 30.5 | Wins: 1491 | Win percentage: 44.0%\n","Epoch: 3400/10000 | Mean size 10: 4.7 | Longest 10: 007 | Mean steps 10: 23.5 | Wins: 1499 | Win percentage: 44.1%\n","Epoch: 3410/10000 | Mean size 10: 4.5 | Longest 10: 006 | Mean steps 10: 27.7 | Wins: 1508 | Win percentage: 44.2%\n","Epoch: 3420/10000 | Mean size 10: 4.5 | Longest 10: 006 | Mean steps 10: 28.0 | Wins: 1517 | Win percentage: 44.4%\n","Epoch: 3430/10000 | Mean size 10: 5.3 | Longest 10: 007 | Mean steps 10: 33.3 | Wins: 1526 | Win percentage: 44.5%\n","Epoch: 3440/10000 | Mean size 10: 4.9 | Longest 10: 008 | Mean steps 10: 36.5 | Wins: 1534 | Win percentage: 44.6%\n","Epoch: 3450/10000 | Mean size 10: 4.8 | Longest 10: 008 | Mean steps 10: 17.6 | Wins: 1543 | Win percentage: 44.7%\n","Epoch: 3460/10000 | Mean size 10: 5.7 | Longest 10: 009 | Mean steps 10: 41.9 | Wins: 1553 | Win percentage: 44.9%\n","Epoch: 3470/10000 | Mean size 10: 5.8 | Longest 10: 009 | Mean steps 10: 34.1 | Wins: 1561 | Win percentage: 45.0%\n","Epoch: 3480/10000 | Mean size 10: 4.3 | Longest 10: 007 | Mean steps 10: 22.8 | Wins: 1567 | Win percentage: 45.0%\n","Epoch: 3490/10000 | Mean size 10: 6.2 | Longest 10: 012 | Mean steps 10: 43.3 | Wins: 1577 | Win percentage: 45.2%\n","Epoch: 3500/10000 | Mean size 10: 4.8 | Longest 10: 008 | Mean steps 10: 22.9 | Wins: 1586 | Win percentage: 45.3%\n","Epoch: 3510/10000 | Mean size 10: 5.4 | Longest 10: 009 | Mean steps 10: 33.1 | Wins: 1595 | Win percentage: 45.4%\n","Epoch: 3520/10000 | Mean size 10: 5.3 | Longest 10: 007 | Mean steps 10: 32.9 | Wins: 1603 | Win percentage: 45.5%\n","Epoch: 3530/10000 | Mean size 10: 5.7 | Longest 10: 008 | Mean steps 10: 32.9 | Wins: 1612 | Win percentage: 45.7%\n","Epoch: 3540/10000 | Mean size 10: 5.5 | Longest 10: 009 | Mean steps 10: 35.0 | Wins: 1620 | Win percentage: 45.8%\n","Epoch: 3550/10000 | Mean size 10: 4.6 | Longest 10: 008 | Mean steps 10: 21.6 | Wins: 1626 | Win percentage: 45.8%\n","Epoch: 3560/10000 | Mean size 10: 5.2 | Longest 10: 007 | Mean steps 10: 32.0 | Wins: 1635 | Win percentage: 45.9%\n","Epoch: 3570/10000 | Mean size 10: 6.2 | Longest 10: 011 | Mean steps 10: 45.6 | Wins: 1643 | Win percentage: 46.0%\n","Epoch: 3580/10000 | Mean size 10: 4.8 | Longest 10: 008 | Mean steps 10: 25.5 | Wins: 1651 | Win percentage: 46.1%\n","Epoch: 3590/10000 | Mean size 10: 5.2 | Longest 10: 008 | Mean steps 10: 33.5 | Wins: 1659 | Win percentage: 46.2%\n","Epoch: 3600/10000 | Mean size 10: 5.0 | Longest 10: 007 | Mean steps 10: 23.8 | Wins: 1668 | Win percentage: 46.3%\n","Epoch: 3610/10000 | Mean size 10: 6.1 | Longest 10: 009 | Mean steps 10: 43.9 | Wins: 1676 | Win percentage: 46.4%\n","Epoch: 3620/10000 | Mean size 10: 5.7 | Longest 10: 009 | Mean steps 10: 34.1 | Wins: 1686 | Win percentage: 46.6%\n","Epoch: 3630/10000 | Mean size 10: 6.2 | Longest 10: 011 | Mean steps 10: 39.3 | Wins: 1695 | Win percentage: 46.7%\n","Epoch: 3640/10000 | Mean size 10: 6.0 | Longest 10: 009 | Mean steps 10: 36.7 | Wins: 1705 | Win percentage: 46.8%\n","Epoch: 3650/10000 | Mean size 10: 4.8 | Longest 10: 009 | Mean steps 10: 22.6 | Wins: 1713 | Win percentage: 46.9%\n","Epoch: 3660/10000 | Mean size 10: 5.7 | Longest 10: 009 | Mean steps 10: 38.2 | Wins: 1721 | Win percentage: 47.0%\n","Epoch: 3670/10000 | Mean size 10: 5.6 | Longest 10: 007 | Mean steps 10: 32.1 | Wins: 1730 | Win percentage: 47.1%\n","Epoch: 3680/10000 | Mean size 10: 4.7 | Longest 10: 008 | Mean steps 10: 26.8 | Wins: 1737 | Win percentage: 47.2%\n","Epoch: 3690/10000 | Mean size 10: 5.3 | Longest 10: 008 | Mean steps 10: 32.3 | Wins: 1745 | Win percentage: 47.3%\n","Epoch: 3700/10000 | Mean size 10: 5.9 | Longest 10: 011 | Mean steps 10: 33.1 | Wins: 1755 | Win percentage: 47.4%\n","Epoch: 3710/10000 | Mean size 10: 5.6 | Longest 10: 010 | Mean steps 10: 39.1 | Wins: 1763 | Win percentage: 47.5%\n","Epoch: 3720/10000 | Mean size 10: 5.8 | Longest 10: 010 | Mean steps 10: 32.1 | Wins: 1771 | Win percentage: 47.6%\n","Epoch: 3730/10000 | Mean size 10: 5.5 | Longest 10: 010 | Mean steps 10: 39.3 | Wins: 1779 | Win percentage: 47.7%\n","Epoch: 3740/10000 | Mean size 10: 6.5 | Longest 10: 011 | Mean steps 10: 39.7 | Wins: 1789 | Win percentage: 47.8%\n","Epoch: 3750/10000 | Mean size 10: 6.9 | Longest 10: 013 | Mean steps 10: 53.0 | Wins: 1798 | Win percentage: 47.9%\n","Epoch: 3760/10000 | Mean size 10: 5.4 | Longest 10: 008 | Mean steps 10: 33.6 | Wins: 1807 | Win percentage: 48.1%\n","Epoch: 3770/10000 | Mean size 10: 5.3 | Longest 10: 008 | Mean steps 10: 31.9 | Wins: 1816 | Win percentage: 48.2%\n","Epoch: 3780/10000 | Mean size 10: 5.1 | Longest 10: 008 | Mean steps 10: 31.6 | Wins: 1824 | Win percentage: 48.3%\n","Epoch: 3790/10000 | Mean size 10: 5.6 | Longest 10: 014 | Mean steps 10: 37.3 | Wins: 1831 | Win percentage: 48.3%\n","Epoch: 3800/10000 | Mean size 10: 5.0 | Longest 10: 007 | Mean steps 10: 27.9 | Wins: 1840 | Win percentage: 48.4%\n","Epoch: 3810/10000 | Mean size 10: 5.2 | Longest 10: 007 | Mean steps 10: 32.2 | Wins: 1849 | Win percentage: 48.5%\n","Epoch: 3820/10000 | Mean size 10: 6.3 | Longest 10: 011 | Mean steps 10: 47.9 | Wins: 1858 | Win percentage: 48.6%\n","Epoch: 3830/10000 | Mean size 10: 6.1 | Longest 10: 007 | Mean steps 10: 42.1 | Wins: 1867 | Win percentage: 48.7%\n","Epoch: 3840/10000 | Mean size 10: 6.0 | Longest 10: 010 | Mean steps 10: 41.5 | Wins: 1877 | Win percentage: 48.9%\n","Epoch: 3850/10000 | Mean size 10: 5.9 | Longest 10: 010 | Mean steps 10: 38.3 | Wins: 1884 | Win percentage: 48.9%\n","Epoch: 3860/10000 | Mean size 10: 6.6 | Longest 10: 011 | Mean steps 10: 44.2 | Wins: 1893 | Win percentage: 49.0%\n","Epoch: 3870/10000 | Mean size 10: 6.3 | Longest 10: 011 | Mean steps 10: 48.0 | Wins: 1902 | Win percentage: 49.1%\n","Epoch: 3880/10000 | Mean size 10: 7.0 | Longest 10: 013 | Mean steps 10: 56.4 | Wins: 1911 | Win percentage: 49.3%\n","Epoch: 3890/10000 | Mean size 10: 5.8 | Longest 10: 011 | Mean steps 10: 46.7 | Wins: 1919 | Win percentage: 49.3%\n","Epoch: 3900/10000 | Mean size 10: 5.5 | Longest 10: 009 | Mean steps 10: 31.5 | Wins: 1929 | Win percentage: 49.5%\n","Epoch: 3910/10000 | Mean size 10: 6.0 | Longest 10: 010 | Mean steps 10: 31.1 | Wins: 1939 | Win percentage: 49.6%\n","Epoch: 3920/10000 | Mean size 10: 6.7 | Longest 10: 010 | Mean steps 10: 42.0 | Wins: 1948 | Win percentage: 49.7%\n","Epoch: 3930/10000 | Mean size 10: 7.2 | Longest 10: 011 | Mean steps 10: 51.8 | Wins: 1958 | Win percentage: 49.8%\n","Epoch: 3940/10000 | Mean size 10: 6.2 | Longest 10: 010 | Mean steps 10: 49.0 | Wins: 1968 | Win percentage: 49.9%\n","Epoch: 3950/10000 | Mean size 10: 6.0 | Longest 10: 009 | Mean steps 10: 36.9 | Wins: 1976 | Win percentage: 50.0%\n","Epoch: 3960/10000 | Mean size 10: 6.3 | Longest 10: 009 | Mean steps 10: 47.3 | Wins: 1986 | Win percentage: 50.2%\n","Epoch: 3970/10000 | Mean size 10: 7.2 | Longest 10: 012 | Mean steps 10: 56.9 | Wins: 1994 | Win percentage: 50.2%\n","Epoch: 3980/10000 | Mean size 10: 6.9 | Longest 10: 010 | Mean steps 10: 52.8 | Wins: 2004 | Win percentage: 50.4%\n","Epoch: 3990/10000 | Mean size 10: 6.5 | Longest 10: 010 | Mean steps 10: 45.4 | Wins: 2014 | Win percentage: 50.5%\n","Epoch: 4000/10000 | Mean size 10: 6.7 | Longest 10: 010 | Mean steps 10: 50.2 | Wins: 2024 | Win percentage: 50.6%\n","Epoch: 4010/10000 | Mean size 10: 6.5 | Longest 10: 011 | Mean steps 10: 49.3 | Wins: 2034 | Win percentage: 50.7%\n","Epoch: 4020/10000 | Mean size 10: 7.6 | Longest 10: 012 | Mean steps 10: 68.6 | Wins: 2044 | Win percentage: 50.8%\n","Epoch: 4030/10000 | Mean size 10: 6.8 | Longest 10: 010 | Mean steps 10: 49.1 | Wins: 2054 | Win percentage: 51.0%\n","Epoch: 4040/10000 | Mean size 10: 5.0 | Longest 10: 009 | Mean steps 10: 26.2 | Wins: 2061 | Win percentage: 51.0%\n","Epoch: 4050/10000 | Mean size 10: 6.4 | Longest 10: 012 | Mean steps 10: 41.2 | Wins: 2070 | Win percentage: 51.1%\n","Epoch: 4060/10000 | Mean size 10: 6.4 | Longest 10: 009 | Mean steps 10: 34.2 | Wins: 2078 | Win percentage: 51.2%\n","Epoch: 4070/10000 | Mean size 10: 6.9 | Longest 10: 010 | Mean steps 10: 43.4 | Wins: 2088 | Win percentage: 51.3%\n","Epoch: 4080/10000 | Mean size 10: 7.3 | Longest 10: 012 | Mean steps 10: 48.0 | Wins: 2098 | Win percentage: 51.4%\n","Epoch: 4090/10000 | Mean size 10: 5.4 | Longest 10: 010 | Mean steps 10: 30.8 | Wins: 2106 | Win percentage: 51.5%\n","Epoch: 4100/10000 | Mean size 10: 5.1 | Longest 10: 008 | Mean steps 10: 24.2 | Wins: 2115 | Win percentage: 51.6%\n","Epoch: 4110/10000 | Mean size 10: 5.5 | Longest 10: 013 | Mean steps 10: 29.6 | Wins: 2121 | Win percentage: 51.6%\n","Epoch: 4120/10000 | Mean size 10: 7.5 | Longest 10: 011 | Mean steps 10: 55.9 | Wins: 2130 | Win percentage: 51.7%\n","Epoch: 4130/10000 | Mean size 10: 5.2 | Longest 10: 008 | Mean steps 10: 24.3 | Wins: 2140 | Win percentage: 51.8%\n","Epoch: 4140/10000 | Mean size 10: 6.9 | Longest 10: 015 | Mean steps 10: 44.7 | Wins: 2149 | Win percentage: 51.9%\n","Epoch: 4150/10000 | Mean size 10: 6.3 | Longest 10: 009 | Mean steps 10: 43.7 | Wins: 2159 | Win percentage: 52.0%\n","Epoch: 4160/10000 | Mean size 10: 7.8 | Longest 10: 012 | Mean steps 10: 65.6 | Wins: 2168 | Win percentage: 52.1%\n","Epoch: 4170/10000 | Mean size 10: 6.5 | Longest 10: 011 | Mean steps 10: 43.9 | Wins: 2178 | Win percentage: 52.2%\n","Epoch: 4180/10000 | Mean size 10: 7.5 | Longest 10: 015 | Mean steps 10: 58.2 | Wins: 2187 | Win percentage: 52.3%\n","Epoch: 4190/10000 | Mean size 10: 5.6 | Longest 10: 009 | Mean steps 10: 27.2 | Wins: 2196 | Win percentage: 52.4%\n","Epoch: 4200/10000 | Mean size 10: 6.5 | Longest 10: 010 | Mean steps 10: 39.0 | Wins: 2206 | Win percentage: 52.5%\n","Epoch: 4210/10000 | Mean size 10: 6.1 | Longest 10: 009 | Mean steps 10: 37.6 | Wins: 2216 | Win percentage: 52.6%\n","Epoch: 4220/10000 | Mean size 10: 7.6 | Longest 10: 013 | Mean steps 10: 57.0 | Wins: 2225 | Win percentage: 52.7%\n","Epoch: 4230/10000 | Mean size 10: 6.1 | Longest 10: 011 | Mean steps 10: 35.4 | Wins: 2234 | Win percentage: 52.8%\n","Epoch: 4240/10000 | Mean size 10: 6.5 | Longest 10: 009 | Mean steps 10: 44.7 | Wins: 2244 | Win percentage: 52.9%\n","Epoch: 4250/10000 | Mean size 10: 7.3 | Longest 10: 011 | Mean steps 10: 46.6 | Wins: 2253 | Win percentage: 53.0%\n","Epoch: 4260/10000 | Mean size 10: 7.4 | Longest 10: 012 | Mean steps 10: 56.4 | Wins: 2262 | Win percentage: 53.1%\n","Epoch: 4270/10000 | Mean size 10: 8.2 | Longest 10: 013 | Mean steps 10: 52.6 | Wins: 2271 | Win percentage: 53.2%\n","Epoch: 4280/10000 | Mean size 10: 7.8 | Longest 10: 014 | Mean steps 10: 49.7 | Wins: 2281 | Win percentage: 53.3%\n","Epoch: 4290/10000 | Mean size 10: 8.9 | Longest 10: 016 | Mean steps 10: 63.0 | Wins: 2291 | Win percentage: 53.4%\n","Epoch: 4300/10000 | Mean size 10: 6.9 | Longest 10: 011 | Mean steps 10: 48.6 | Wins: 2301 | Win percentage: 53.5%\n","Epoch: 4310/10000 | Mean size 10: 9.3 | Longest 10: 013 | Mean steps 10: 81.5 | Wins: 2311 | Win percentage: 53.6%\n","Epoch: 4320/10000 | Mean size 10: 7.1 | Longest 10: 011 | Mean steps 10: 47.8 | Wins: 2321 | Win percentage: 53.7%\n","Epoch: 4330/10000 | Mean size 10: 7.0 | Longest 10: 011 | Mean steps 10: 48.6 | Wins: 2331 | Win percentage: 53.8%\n","Epoch: 4340/10000 | Mean size 10: 8.8 | Longest 10: 015 | Mean steps 10: 65.2 | Wins: 2340 | Win percentage: 53.9%\n","Epoch: 4350/10000 | Mean size 10: 7.8 | Longest 10: 012 | Mean steps 10: 48.5 | Wins: 2350 | Win percentage: 54.0%\n","Epoch: 4360/10000 | Mean size 10: 8.0 | Longest 10: 012 | Mean steps 10: 57.6 | Wins: 2360 | Win percentage: 54.1%\n","Epoch: 4370/10000 | Mean size 10: 7.3 | Longest 10: 015 | Mean steps 10: 54.2 | Wins: 2370 | Win percentage: 54.2%\n","Epoch: 4380/10000 | Mean size 10: 6.4 | Longest 10: 011 | Mean steps 10: 49.0 | Wins: 2380 | Win percentage: 54.3%\n","Epoch: 4390/10000 | Mean size 10: 5.6 | Longest 10: 009 | Mean steps 10: 28.7 | Wins: 2388 | Win percentage: 54.4%\n","Epoch: 4400/10000 | Mean size 10: 7.5 | Longest 10: 014 | Mean steps 10: 49.7 | Wins: 2398 | Win percentage: 54.5%\n","Epoch: 4410/10000 | Mean size 10: 7.9 | Longest 10: 010 | Mean steps 10: 59.7 | Wins: 2407 | Win percentage: 54.6%\n","Epoch: 4420/10000 | Mean size 10: 7.7 | Longest 10: 014 | Mean steps 10: 56.9 | Wins: 2417 | Win percentage: 54.7%\n","Epoch: 4430/10000 | Mean size 10: 7.7 | Longest 10: 010 | Mean steps 10: 58.2 | Wins: 2426 | Win percentage: 54.8%\n","Epoch: 4440/10000 | Mean size 10: 9.2 | Longest 10: 013 | Mean steps 10: 87.8 | Wins: 2436 | Win percentage: 54.9%\n","Epoch: 4450/10000 | Mean size 10: 8.7 | Longest 10: 015 | Mean steps 10: 70.0 | Wins: 2446 | Win percentage: 55.0%\n","Epoch: 4460/10000 | Mean size 10: 9.3 | Longest 10: 013 | Mean steps 10: 71.0 | Wins: 2456 | Win percentage: 55.1%\n","Epoch: 4470/10000 | Mean size 10: 8.9 | Longest 10: 015 | Mean steps 10: 73.6 | Wins: 2466 | Win percentage: 55.2%\n","Epoch: 4480/10000 | Mean size 10: 8.3 | Longest 10: 012 | Mean steps 10: 60.2 | Wins: 2476 | Win percentage: 55.3%\n","Epoch: 4490/10000 | Mean size 10: 6.3 | Longest 10: 009 | Mean steps 10: 38.1 | Wins: 2486 | Win percentage: 55.4%\n","Epoch: 4500/10000 | Mean size 10: 9.2 | Longest 10: 015 | Mean steps 10: 64.4 | Wins: 2496 | Win percentage: 55.5%\n","Epoch: 4510/10000 | Mean size 10: 8.3 | Longest 10: 013 | Mean steps 10: 64.5 | Wins: 2505 | Win percentage: 55.5%\n","Epoch: 4520/10000 | Mean size 10: 8.2 | Longest 10: 012 | Mean steps 10: 58.7 | Wins: 2515 | Win percentage: 55.6%\n","Epoch: 4530/10000 | Mean size 10: 8.9 | Longest 10: 019 | Mean steps 10: 70.2 | Wins: 2525 | Win percentage: 55.7%\n","Epoch: 4540/10000 | Mean size 10: 8.5 | Longest 10: 016 | Mean steps 10: 63.0 | Wins: 2535 | Win percentage: 55.8%\n","Epoch: 4550/10000 | Mean size 10: 9.1 | Longest 10: 012 | Mean steps 10: 69.1 | Wins: 2545 | Win percentage: 55.9%\n","Epoch: 4560/10000 | Mean size 10: 8.3 | Longest 10: 016 | Mean steps 10: 57.4 | Wins: 2555 | Win percentage: 56.0%\n","Epoch: 4570/10000 | Mean size 10: 10.4 | Longest 10: 017 | Mean steps 10: 96.4 | Wins: 2565 | Win percentage: 56.1%\n","Epoch: 4580/10000 | Mean size 10: 10.6 | Longest 10: 015 | Mean steps 10: 92.6 | Wins: 2575 | Win percentage: 56.2%\n","Epoch: 4590/10000 | Mean size 10: 8.8 | Longest 10: 014 | Mean steps 10: 70.5 | Wins: 2585 | Win percentage: 56.3%\n","Epoch: 4600/10000 | Mean size 10: 8.3 | Longest 10: 013 | Mean steps 10: 71.0 | Wins: 2594 | Win percentage: 56.4%\n","Epoch: 4610/10000 | Mean size 10: 9.3 | Longest 10: 020 | Mean steps 10: 80.2 | Wins: 2604 | Win percentage: 56.5%\n","Epoch: 4620/10000 | Mean size 10: 8.8 | Longest 10: 014 | Mean steps 10: 71.3 | Wins: 2614 | Win percentage: 56.6%\n","Epoch: 4630/10000 | Mean size 10: 7.2 | Longest 10: 014 | Mean steps 10: 47.2 | Wins: 2624 | Win percentage: 56.7%\n","Epoch: 4640/10000 | Mean size 10: 7.6 | Longest 10: 014 | Mean steps 10: 55.2 | Wins: 2634 | Win percentage: 56.8%\n","Epoch: 4650/10000 | Mean size 10: 10.3 | Longest 10: 017 | Mean steps 10: 77.3 | Wins: 2644 | Win percentage: 56.9%\n","Epoch: 4660/10000 | Mean size 10: 9.2 | Longest 10: 013 | Mean steps 10: 77.1 | Wins: 2654 | Win percentage: 57.0%\n","Epoch: 4670/10000 | Mean size 10: 8.0 | Longest 10: 014 | Mean steps 10: 59.7 | Wins: 2664 | Win percentage: 57.0%\n","Epoch: 4680/10000 | Mean size 10: 9.9 | Longest 10: 013 | Mean steps 10: 89.1 | Wins: 2674 | Win percentage: 57.1%\n","Epoch: 4690/10000 | Mean size 10: 9.9 | Longest 10: 019 | Mean steps 10: 89.3 | Wins: 2684 | Win percentage: 57.2%\n","Epoch: 4700/10000 | Mean size 10: 7.7 | Longest 10: 012 | Mean steps 10: 48.8 | Wins: 2693 | Win percentage: 57.3%\n","Epoch: 4710/10000 | Mean size 10: 7.4 | Longest 10: 012 | Mean steps 10: 51.0 | Wins: 2703 | Win percentage: 57.4%\n","Epoch: 4720/10000 | Mean size 10: 9.2 | Longest 10: 014 | Mean steps 10: 72.3 | Wins: 2713 | Win percentage: 57.5%\n","Epoch: 4730/10000 | Mean size 10: 12.1 | Longest 10: 020 | Mean steps 10: 123.8 | Wins: 2723 | Win percentage: 57.6%\n","Epoch: 4740/10000 | Mean size 10: 9.5 | Longest 10: 016 | Mean steps 10: 68.0 | Wins: 2733 | Win percentage: 57.7%\n","Epoch: 4750/10000 | Mean size 10: 9.5 | Longest 10: 014 | Mean steps 10: 72.9 | Wins: 2743 | Win percentage: 57.7%\n","Epoch: 4760/10000 | Mean size 10: 11.6 | Longest 10: 018 | Mean steps 10: 115.1 | Wins: 2753 | Win percentage: 57.8%\n","Epoch: 4770/10000 | Mean size 10: 7.6 | Longest 10: 013 | Mean steps 10: 55.5 | Wins: 2763 | Win percentage: 57.9%\n","Epoch: 4780/10000 | Mean size 10: 8.4 | Longest 10: 015 | Mean steps 10: 64.4 | Wins: 2773 | Win percentage: 58.0%\n","Epoch: 4790/10000 | Mean size 10: 8.5 | Longest 10: 012 | Mean steps 10: 57.8 | Wins: 2783 | Win percentage: 58.1%\n","Epoch: 4800/10000 | Mean size 10: 9.3 | Longest 10: 015 | Mean steps 10: 65.9 | Wins: 2793 | Win percentage: 58.2%\n","Epoch: 4810/10000 | Mean size 10: 9.8 | Longest 10: 015 | Mean steps 10: 75.4 | Wins: 2802 | Win percentage: 58.3%\n","Epoch: 4820/10000 | Mean size 10: 11.6 | Longest 10: 020 | Mean steps 10: 123.4 | Wins: 2812 | Win percentage: 58.3%\n","Epoch: 4830/10000 | Mean size 10: 9.2 | Longest 10: 015 | Mean steps 10: 66.0 | Wins: 2822 | Win percentage: 58.4%\n","Epoch: 4840/10000 | Mean size 10: 8.9 | Longest 10: 014 | Mean steps 10: 69.3 | Wins: 2832 | Win percentage: 58.5%\n","Epoch: 4850/10000 | Mean size 10: 11.8 | Longest 10: 017 | Mean steps 10: 121.5 | Wins: 2842 | Win percentage: 58.6%\n","Epoch: 4860/10000 | Mean size 10: 11.8 | Longest 10: 018 | Mean steps 10: 111.9 | Wins: 2851 | Win percentage: 58.7%\n","Epoch: 4870/10000 | Mean size 10: 11.5 | Longest 10: 020 | Mean steps 10: 102.3 | Wins: 2861 | Win percentage: 58.7%\n","Epoch: 4880/10000 | Mean size 10: 11.1 | Longest 10: 023 | Mean steps 10: 99.8 | Wins: 2871 | Win percentage: 58.8%\n","Epoch: 4890/10000 | Mean size 10: 10.3 | Longest 10: 016 | Mean steps 10: 75.3 | Wins: 2881 | Win percentage: 58.9%\n","Epoch: 4900/10000 | Mean size 10: 10.9 | Longest 10: 018 | Mean steps 10: 86.7 | Wins: 2891 | Win percentage: 59.0%\n","Epoch: 4910/10000 | Mean size 10: 13.1 | Longest 10: 017 | Mean steps 10: 119.2 | Wins: 2901 | Win percentage: 59.1%\n","Epoch: 4920/10000 | Mean size 10: 12.4 | Longest 10: 016 | Mean steps 10: 107.6 | Wins: 2911 | Win percentage: 59.2%\n","Epoch: 4930/10000 | Mean size 10: 14.2 | Longest 10: 018 | Mean steps 10: 148.0 | Wins: 2921 | Win percentage: 59.2%\n","Epoch: 4940/10000 | Mean size 10: 11.8 | Longest 10: 021 | Mean steps 10: 103.6 | Wins: 2930 | Win percentage: 59.3%\n","Epoch: 4950/10000 | Mean size 10: 13.9 | Longest 10: 022 | Mean steps 10: 126.9 | Wins: 2940 | Win percentage: 59.4%\n","Epoch: 4960/10000 | Mean size 10: 15.0 | Longest 10: 019 | Mean steps 10: 133.3 | Wins: 2950 | Win percentage: 59.5%\n","Epoch: 4970/10000 | Mean size 10: 14.0 | Longest 10: 023 | Mean steps 10: 128.5 | Wins: 2960 | Win percentage: 59.6%\n","Epoch: 4980/10000 | Mean size 10: 16.3 | Longest 10: 023 | Mean steps 10: 160.7 | Wins: 2970 | Win percentage: 59.6%\n","Epoch: 4990/10000 | Mean size 10: 12.2 | Longest 10: 018 | Mean steps 10: 98.0 | Wins: 2980 | Win percentage: 59.7%\n","Epoch: 5000/10000 | Mean size 10: 16.7 | Longest 10: 025 | Mean steps 10: 158.9 | Wins: 2990 | Win percentage: 59.8%\n","Epoch: 5010/10000 | Mean size 10: 13.4 | Longest 10: 020 | Mean steps 10: 116.5 | Wins: 3000 | Win percentage: 59.9%\n","Epoch: 5020/10000 | Mean size 10: 12.6 | Longest 10: 021 | Mean steps 10: 113.3 | Wins: 3010 | Win percentage: 60.0%\n","Epoch: 5030/10000 | Mean size 10: 13.4 | Longest 10: 022 | Mean steps 10: 119.8 | Wins: 3020 | Win percentage: 60.0%\n","Epoch: 5040/10000 | Mean size 10: 13.1 | Longest 10: 020 | Mean steps 10: 102.6 | Wins: 3030 | Win percentage: 60.1%\n","Epoch: 5050/10000 | Mean size 10: 13.3 | Longest 10: 023 | Mean steps 10: 114.0 | Wins: 3040 | Win percentage: 60.2%\n","Epoch: 5060/10000 | Mean size 10: 16.4 | Longest 10: 025 | Mean steps 10: 179.0 | Wins: 3050 | Win percentage: 60.3%\n","Epoch: 5070/10000 | Mean size 10: 14.2 | Longest 10: 028 | Mean steps 10: 142.6 | Wins: 3060 | Win percentage: 60.4%\n","Epoch: 5080/10000 | Mean size 10: 15.3 | Longest 10: 021 | Mean steps 10: 160.5 | Wins: 3070 | Win percentage: 60.4%\n","Epoch: 5090/10000 | Mean size 10: 11.8 | Longest 10: 017 | Mean steps 10: 97.1 | Wins: 3080 | Win percentage: 60.5%\n","Epoch: 5100/10000 | Mean size 10: 14.3 | Longest 10: 018 | Mean steps 10: 132.2 | Wins: 3090 | Win percentage: 60.6%\n","Epoch: 5110/10000 | Mean size 10: 16.2 | Longest 10: 022 | Mean steps 10: 175.2 | Wins: 3100 | Win percentage: 60.7%\n","Epoch: 5120/10000 | Mean size 10: 15.7 | Longest 10: 024 | Mean steps 10: 143.1 | Wins: 3110 | Win percentage: 60.7%\n","Epoch: 5130/10000 | Mean size 10: 11.9 | Longest 10: 017 | Mean steps 10: 97.3 | Wins: 3120 | Win percentage: 60.8%\n","Epoch: 5140/10000 | Mean size 10: 12.9 | Longest 10: 017 | Mean steps 10: 122.5 | Wins: 3130 | Win percentage: 60.9%\n","Epoch: 5150/10000 | Mean size 10: 15.3 | Longest 10: 020 | Mean steps 10: 148.4 | Wins: 3140 | Win percentage: 61.0%\n","Epoch: 5160/10000 | Mean size 10: 14.2 | Longest 10: 021 | Mean steps 10: 146.5 | Wins: 3150 | Win percentage: 61.0%\n","Epoch: 5170/10000 | Mean size 10: 15.1 | Longest 10: 024 | Mean steps 10: 145.9 | Wins: 3160 | Win percentage: 61.1%\n","Epoch: 5180/10000 | Mean size 10: 14.1 | Longest 10: 024 | Mean steps 10: 142.8 | Wins: 3170 | Win percentage: 61.2%\n","Epoch: 5190/10000 | Mean size 10: 13.4 | Longest 10: 021 | Mean steps 10: 125.3 | Wins: 3180 | Win percentage: 61.3%\n","Epoch: 5200/10000 | Mean size 10: 14.1 | Longest 10: 022 | Mean steps 10: 145.8 | Wins: 3190 | Win percentage: 61.3%\n","Epoch: 5210/10000 | Mean size 10: 14.4 | Longest 10: 023 | Mean steps 10: 119.1 | Wins: 3200 | Win percentage: 61.4%\n","Epoch: 5220/10000 | Mean size 10: 13.9 | Longest 10: 021 | Mean steps 10: 134.5 | Wins: 3210 | Win percentage: 61.5%\n","Epoch: 5230/10000 | Mean size 10: 11.1 | Longest 10: 018 | Mean steps 10: 92.2 | Wins: 3220 | Win percentage: 61.6%\n","Epoch: 5240/10000 | Mean size 10: 16.2 | Longest 10: 025 | Mean steps 10: 167.0 | Wins: 3230 | Win percentage: 61.6%\n","Epoch: 5250/10000 | Mean size 10: 15.4 | Longest 10: 024 | Mean steps 10: 138.7 | Wins: 3240 | Win percentage: 61.7%\n","Epoch: 5260/10000 | Mean size 10: 12.3 | Longest 10: 019 | Mean steps 10: 105.2 | Wins: 3250 | Win percentage: 61.8%\n","Epoch: 5270/10000 | Mean size 10: 15.2 | Longest 10: 026 | Mean steps 10: 143.0 | Wins: 3259 | Win percentage: 61.8%\n","Epoch: 5280/10000 | Mean size 10: 16.6 | Longest 10: 021 | Mean steps 10: 175.3 | Wins: 3269 | Win percentage: 61.9%\n","Epoch: 5290/10000 | Mean size 10: 15.4 | Longest 10: 022 | Mean steps 10: 157.3 | Wins: 3279 | Win percentage: 62.0%\n","Epoch: 5300/10000 | Mean size 10: 14.7 | Longest 10: 018 | Mean steps 10: 133.4 | Wins: 3289 | Win percentage: 62.1%\n","Epoch: 5310/10000 | Mean size 10: 13.3 | Longest 10: 018 | Mean steps 10: 107.5 | Wins: 3299 | Win percentage: 62.1%\n","Epoch: 5320/10000 | Mean size 10: 13.4 | Longest 10: 017 | Mean steps 10: 124.6 | Wins: 3309 | Win percentage: 62.2%\n","Epoch: 5330/10000 | Mean size 10: 16.7 | Longest 10: 028 | Mean steps 10: 173.9 | Wins: 3319 | Win percentage: 62.3%\n","Epoch: 5340/10000 | Mean size 10: 14.1 | Longest 10: 024 | Mean steps 10: 115.9 | Wins: 3329 | Win percentage: 62.3%\n","Epoch: 5350/10000 | Mean size 10: 14.7 | Longest 10: 020 | Mean steps 10: 142.5 | Wins: 3339 | Win percentage: 62.4%\n","Epoch: 5360/10000 | Mean size 10: 10.3 | Longest 10: 017 | Mean steps 10: 80.7 | Wins: 3349 | Win percentage: 62.5%\n","Epoch: 5370/10000 | Mean size 10: 14.8 | Longest 10: 022 | Mean steps 10: 148.6 | Wins: 3359 | Win percentage: 62.6%\n","Epoch: 5380/10000 | Mean size 10: 14.5 | Longest 10: 025 | Mean steps 10: 126.5 | Wins: 3369 | Win percentage: 62.6%\n","Epoch: 5390/10000 | Mean size 10: 16.0 | Longest 10: 021 | Mean steps 10: 152.0 | Wins: 3379 | Win percentage: 62.7%\n","Epoch: 5400/10000 | Mean size 10: 15.3 | Longest 10: 024 | Mean steps 10: 155.9 | Wins: 3389 | Win percentage: 62.8%\n","Epoch: 5410/10000 | Mean size 10: 15.8 | Longest 10: 020 | Mean steps 10: 143.3 | Wins: 3399 | Win percentage: 62.8%\n","Epoch: 5420/10000 | Mean size 10: 13.9 | Longest 10: 024 | Mean steps 10: 134.8 | Wins: 3409 | Win percentage: 62.9%\n","Epoch: 5430/10000 | Mean size 10: 14.1 | Longest 10: 020 | Mean steps 10: 122.4 | Wins: 3419 | Win percentage: 63.0%\n","Epoch: 5440/10000 | Mean size 10: 14.6 | Longest 10: 024 | Mean steps 10: 136.9 | Wins: 3429 | Win percentage: 63.0%\n","Epoch: 5450/10000 | Mean size 10: 12.6 | Longest 10: 022 | Mean steps 10: 107.6 | Wins: 3439 | Win percentage: 63.1%\n","Epoch: 5460/10000 | Mean size 10: 16.6 | Longest 10: 023 | Mean steps 10: 149.2 | Wins: 3449 | Win percentage: 63.2%\n","Epoch: 5470/10000 | Mean size 10: 14.3 | Longest 10: 025 | Mean steps 10: 126.3 | Wins: 3459 | Win percentage: 63.2%\n","Epoch: 5480/10000 | Mean size 10: 16.0 | Longest 10: 022 | Mean steps 10: 139.2 | Wins: 3469 | Win percentage: 63.3%\n","Epoch: 5490/10000 | Mean size 10: 19.1 | Longest 10: 029 | Mean steps 10: 188.4 | Wins: 3479 | Win percentage: 63.4%\n","Epoch: 5500/10000 | Mean size 10: 18.7 | Longest 10: 026 | Mean steps 10: 204.4 | Wins: 3489 | Win percentage: 63.4%\n","Epoch: 5510/10000 | Mean size 10: 15.2 | Longest 10: 021 | Mean steps 10: 146.9 | Wins: 3499 | Win percentage: 63.5%\n","Epoch: 5520/10000 | Mean size 10: 15.8 | Longest 10: 023 | Mean steps 10: 159.0 | Wins: 3509 | Win percentage: 63.6%\n","Epoch: 5530/10000 | Mean size 10: 16.3 | Longest 10: 024 | Mean steps 10: 182.4 | Wins: 3519 | Win percentage: 63.6%\n","Epoch: 5540/10000 | Mean size 10: 17.0 | Longest 10: 026 | Mean steps 10: 164.3 | Wins: 3529 | Win percentage: 63.7%\n","Epoch: 5550/10000 | Mean size 10: 16.8 | Longest 10: 022 | Mean steps 10: 156.2 | Wins: 3539 | Win percentage: 63.8%\n","Epoch: 5560/10000 | Mean size 10: 12.5 | Longest 10: 022 | Mean steps 10: 120.6 | Wins: 3549 | Win percentage: 63.8%\n","Epoch: 5570/10000 | Mean size 10: 15.5 | Longest 10: 022 | Mean steps 10: 154.0 | Wins: 3559 | Win percentage: 63.9%\n","Epoch: 5580/10000 | Mean size 10: 14.3 | Longest 10: 020 | Mean steps 10: 130.1 | Wins: 3569 | Win percentage: 64.0%\n","Epoch: 5590/10000 | Mean size 10: 13.3 | Longest 10: 022 | Mean steps 10: 111.0 | Wins: 3579 | Win percentage: 64.0%\n","Epoch: 5600/10000 | Mean size 10: 16.4 | Longest 10: 021 | Mean steps 10: 159.1 | Wins: 3589 | Win percentage: 64.1%\n","Epoch: 5610/10000 | Mean size 10: 14.2 | Longest 10: 022 | Mean steps 10: 121.2 | Wins: 3599 | Win percentage: 64.2%\n","Epoch: 5620/10000 | Mean size 10: 17.1 | Longest 10: 024 | Mean steps 10: 162.8 | Wins: 3609 | Win percentage: 64.2%\n","Epoch: 5630/10000 | Mean size 10: 15.8 | Longest 10: 023 | Mean steps 10: 144.3 | Wins: 3619 | Win percentage: 64.3%\n","Epoch: 5640/10000 | Mean size 10: 15.8 | Longest 10: 026 | Mean steps 10: 144.4 | Wins: 3629 | Win percentage: 64.3%\n","Epoch: 5650/10000 | Mean size 10: 16.0 | Longest 10: 025 | Mean steps 10: 152.0 | Wins: 3639 | Win percentage: 64.4%\n","Epoch: 5660/10000 | Mean size 10: 15.9 | Longest 10: 027 | Mean steps 10: 148.4 | Wins: 3649 | Win percentage: 64.5%\n","Epoch: 5670/10000 | Mean size 10: 17.5 | Longest 10: 024 | Mean steps 10: 152.8 | Wins: 3659 | Win percentage: 64.5%\n","Epoch: 5680/10000 | Mean size 10: 19.3 | Longest 10: 027 | Mean steps 10: 192.6 | Wins: 3669 | Win percentage: 64.6%\n","Epoch: 5690/10000 | Mean size 10: 17.6 | Longest 10: 026 | Mean steps 10: 171.1 | Wins: 3679 | Win percentage: 64.7%\n","Epoch: 5700/10000 | Mean size 10: 17.0 | Longest 10: 023 | Mean steps 10: 147.4 | Wins: 3689 | Win percentage: 64.7%\n","Epoch: 5710/10000 | Mean size 10: 17.0 | Longest 10: 024 | Mean steps 10: 159.6 | Wins: 3699 | Win percentage: 64.8%\n","Epoch: 5720/10000 | Mean size 10: 15.7 | Longest 10: 026 | Mean steps 10: 132.3 | Wins: 3709 | Win percentage: 64.8%\n","Epoch: 5730/10000 | Mean size 10: 15.7 | Longest 10: 023 | Mean steps 10: 148.7 | Wins: 3719 | Win percentage: 64.9%\n","Epoch: 5740/10000 | Mean size 10: 18.3 | Longest 10: 023 | Mean steps 10: 158.1 | Wins: 3729 | Win percentage: 65.0%\n","Epoch: 5750/10000 | Mean size 10: 14.6 | Longest 10: 023 | Mean steps 10: 107.4 | Wins: 3739 | Win percentage: 65.0%\n","Epoch: 5760/10000 | Mean size 10: 13.9 | Longest 10: 022 | Mean steps 10: 119.5 | Wins: 3749 | Win percentage: 65.1%\n","Epoch: 5770/10000 | Mean size 10: 15.1 | Longest 10: 022 | Mean steps 10: 151.0 | Wins: 3759 | Win percentage: 65.1%\n","Epoch: 5780/10000 | Mean size 10: 16.6 | Longest 10: 030 | Mean steps 10: 142.9 | Wins: 3769 | Win percentage: 65.2%\n","Epoch: 5790/10000 | Mean size 10: 12.8 | Longest 10: 023 | Mean steps 10: 107.3 | Wins: 3779 | Win percentage: 65.3%\n","Epoch: 5800/10000 | Mean size 10: 12.5 | Longest 10: 020 | Mean steps 10: 93.6 | Wins: 3789 | Win percentage: 65.3%\n","Epoch: 5810/10000 | Mean size 10: 15.4 | Longest 10: 026 | Mean steps 10: 134.9 | Wins: 3799 | Win percentage: 65.4%\n","Epoch: 5820/10000 | Mean size 10: 16.6 | Longest 10: 023 | Mean steps 10: 141.5 | Wins: 3809 | Win percentage: 65.4%\n","Epoch: 5830/10000 | Mean size 10: 16.8 | Longest 10: 028 | Mean steps 10: 156.4 | Wins: 3819 | Win percentage: 65.5%\n","Epoch: 5840/10000 | Mean size 10: 15.5 | Longest 10: 025 | Mean steps 10: 143.8 | Wins: 3829 | Win percentage: 65.6%\n","Epoch: 5850/10000 | Mean size 10: 17.2 | Longest 10: 027 | Mean steps 10: 166.8 | Wins: 3839 | Win percentage: 65.6%\n","Epoch: 5860/10000 | Mean size 10: 15.0 | Longest 10: 021 | Mean steps 10: 127.2 | Wins: 3849 | Win percentage: 65.7%\n","Epoch: 5870/10000 | Mean size 10: 16.0 | Longest 10: 023 | Mean steps 10: 142.2 | Wins: 3859 | Win percentage: 65.7%\n","Epoch: 5880/10000 | Mean size 10: 17.5 | Longest 10: 026 | Mean steps 10: 144.9 | Wins: 3869 | Win percentage: 65.8%\n","Epoch: 5890/10000 | Mean size 10: 16.6 | Longest 10: 027 | Mean steps 10: 166.1 | Wins: 3879 | Win percentage: 65.9%\n","Epoch: 5900/10000 | Mean size 10: 15.8 | Longest 10: 025 | Mean steps 10: 118.1 | Wins: 3889 | Win percentage: 65.9%\n","Epoch: 5910/10000 | Mean size 10: 12.1 | Longest 10: 021 | Mean steps 10: 88.9 | Wins: 3899 | Win percentage: 66.0%\n","Epoch: 5920/10000 | Mean size 10: 12.4 | Longest 10: 023 | Mean steps 10: 92.6 | Wins: 3909 | Win percentage: 66.0%\n","Epoch: 5930/10000 | Mean size 10: 17.5 | Longest 10: 025 | Mean steps 10: 158.0 | Wins: 3919 | Win percentage: 66.1%\n","Epoch: 5940/10000 | Mean size 10: 18.3 | Longest 10: 025 | Mean steps 10: 169.7 | Wins: 3929 | Win percentage: 66.1%\n","Epoch: 5950/10000 | Mean size 10: 16.2 | Longest 10: 030 | Mean steps 10: 143.2 | Wins: 3939 | Win percentage: 66.2%\n","Epoch: 5960/10000 | Mean size 10: 17.0 | Longest 10: 025 | Mean steps 10: 154.7 | Wins: 3949 | Win percentage: 66.3%\n","Epoch: 5970/10000 | Mean size 10: 16.9 | Longest 10: 022 | Mean steps 10: 148.9 | Wins: 3959 | Win percentage: 66.3%\n","Epoch: 5980/10000 | Mean size 10: 13.6 | Longest 10: 024 | Mean steps 10: 115.7 | Wins: 3969 | Win percentage: 66.4%\n","Epoch: 5990/10000 | Mean size 10: 15.7 | Longest 10: 025 | Mean steps 10: 138.4 | Wins: 3979 | Win percentage: 66.4%\n","Epoch: 6000/10000 | Mean size 10: 17.3 | Longest 10: 026 | Mean steps 10: 145.4 | Wins: 3989 | Win percentage: 66.5%\n","Epoch: 6010/10000 | Mean size 10: 13.2 | Longest 10: 025 | Mean steps 10: 103.1 | Wins: 3999 | Win percentage: 66.5%\n","Epoch: 6020/10000 | Mean size 10: 14.2 | Longest 10: 026 | Mean steps 10: 119.4 | Wins: 4009 | Win percentage: 66.6%\n","Epoch: 6030/10000 | Mean size 10: 14.9 | Longest 10: 026 | Mean steps 10: 125.9 | Wins: 4019 | Win percentage: 66.7%\n","Epoch: 6040/10000 | Mean size 10: 14.1 | Longest 10: 020 | Mean steps 10: 115.6 | Wins: 4029 | Win percentage: 66.7%\n","Epoch: 6050/10000 | Mean size 10: 17.2 | Longest 10: 029 | Mean steps 10: 156.1 | Wins: 4039 | Win percentage: 66.8%\n","Epoch: 6060/10000 | Mean size 10: 18.3 | Longest 10: 024 | Mean steps 10: 155.3 | Wins: 4049 | Win percentage: 66.8%\n","Epoch: 6070/10000 | Mean size 10: 15.4 | Longest 10: 028 | Mean steps 10: 125.8 | Wins: 4059 | Win percentage: 66.9%\n","Epoch: 6080/10000 | Mean size 10: 16.1 | Longest 10: 028 | Mean steps 10: 138.3 | Wins: 4069 | Win percentage: 66.9%\n","Epoch: 6090/10000 | Mean size 10: 17.0 | Longest 10: 027 | Mean steps 10: 154.8 | Wins: 4079 | Win percentage: 67.0%\n","Epoch: 6100/10000 | Mean size 10: 15.4 | Longest 10: 028 | Mean steps 10: 133.3 | Wins: 4089 | Win percentage: 67.0%\n","Epoch: 6110/10000 | Mean size 10: 13.8 | Longest 10: 023 | Mean steps 10: 111.2 | Wins: 4099 | Win percentage: 67.1%\n","Epoch: 6120/10000 | Mean size 10: 15.6 | Longest 10: 027 | Mean steps 10: 133.5 | Wins: 4109 | Win percentage: 67.1%\n","Epoch: 6130/10000 | Mean size 10: 11.8 | Longest 10: 024 | Mean steps 10: 88.5 | Wins: 4119 | Win percentage: 67.2%\n","Epoch: 6140/10000 | Mean size 10: 16.6 | Longest 10: 025 | Mean steps 10: 138.7 | Wins: 4129 | Win percentage: 67.2%\n","Epoch: 6150/10000 | Mean size 10: 17.2 | Longest 10: 026 | Mean steps 10: 152.0 | Wins: 4139 | Win percentage: 67.3%\n","Epoch: 6160/10000 | Mean size 10: 15.8 | Longest 10: 022 | Mean steps 10: 137.6 | Wins: 4149 | Win percentage: 67.4%\n","Epoch: 6170/10000 | Mean size 10: 16.2 | Longest 10: 021 | Mean steps 10: 118.1 | Wins: 4159 | Win percentage: 67.4%\n","Epoch: 6180/10000 | Mean size 10: 18.5 | Longest 10: 025 | Mean steps 10: 165.4 | Wins: 4169 | Win percentage: 67.5%\n","Epoch: 6190/10000 | Mean size 10: 16.8 | Longest 10: 025 | Mean steps 10: 142.3 | Wins: 4179 | Win percentage: 67.5%\n","Epoch: 6200/10000 | Mean size 10: 17.1 | Longest 10: 029 | Mean steps 10: 142.4 | Wins: 4189 | Win percentage: 67.6%\n","Epoch: 6210/10000 | Mean size 10: 15.4 | Longest 10: 026 | Mean steps 10: 119.2 | Wins: 4199 | Win percentage: 67.6%\n","Epoch: 6220/10000 | Mean size 10: 14.7 | Longest 10: 022 | Mean steps 10: 114.4 | Wins: 4209 | Win percentage: 67.7%\n","Epoch: 6230/10000 | Mean size 10: 15.8 | Longest 10: 025 | Mean steps 10: 127.2 | Wins: 4219 | Win percentage: 67.7%\n","Epoch: 6240/10000 | Mean size 10: 19.8 | Longest 10: 030 | Mean steps 10: 194.6 | Wins: 4229 | Win percentage: 67.8%\n","Epoch: 6250/10000 | Mean size 10: 15.8 | Longest 10: 025 | Mean steps 10: 132.9 | Wins: 4239 | Win percentage: 67.8%\n","Epoch: 6260/10000 | Mean size 10: 19.2 | Longest 10: 025 | Mean steps 10: 168.8 | Wins: 4249 | Win percentage: 67.9%\n","Epoch: 6270/10000 | Mean size 10: 18.5 | Longest 10: 028 | Mean steps 10: 152.7 | Wins: 4259 | Win percentage: 67.9%\n","Epoch: 6280/10000 | Mean size 10: 15.3 | Longest 10: 022 | Mean steps 10: 133.6 | Wins: 4269 | Win percentage: 68.0%\n","Epoch: 6290/10000 | Mean size 10: 16.1 | Longest 10: 022 | Mean steps 10: 126.7 | Wins: 4279 | Win percentage: 68.0%\n","Epoch: 6300/10000 | Mean size 10: 15.8 | Longest 10: 026 | Mean steps 10: 134.5 | Wins: 4289 | Win percentage: 68.1%\n","Epoch: 6310/10000 | Mean size 10: 16.0 | Longest 10: 022 | Mean steps 10: 131.9 | Wins: 4299 | Win percentage: 68.1%\n","Epoch: 6320/10000 | Mean size 10: 14.7 | Longest 10: 023 | Mean steps 10: 128.6 | Wins: 4309 | Win percentage: 68.2%\n","Epoch: 6330/10000 | Mean size 10: 14.8 | Longest 10: 027 | Mean steps 10: 115.0 | Wins: 4319 | Win percentage: 68.2%\n","Epoch: 6340/10000 | Mean size 10: 17.5 | Longest 10: 026 | Mean steps 10: 152.4 | Wins: 4329 | Win percentage: 68.3%\n","Epoch: 6350/10000 | Mean size 10: 17.6 | Longest 10: 026 | Mean steps 10: 143.6 | Wins: 4339 | Win percentage: 68.3%\n","Epoch: 6360/10000 | Mean size 10: 18.3 | Longest 10: 029 | Mean steps 10: 160.9 | Wins: 4349 | Win percentage: 68.4%\n","Epoch: 6370/10000 | Mean size 10: 15.7 | Longest 10: 024 | Mean steps 10: 133.4 | Wins: 4359 | Win percentage: 68.4%\n","Epoch: 6380/10000 | Mean size 10: 18.4 | Longest 10: 031 | Mean steps 10: 169.5 | Wins: 4369 | Win percentage: 68.5%\n","Epoch: 6390/10000 | Mean size 10: 15.8 | Longest 10: 026 | Mean steps 10: 128.2 | Wins: 4379 | Win percentage: 68.5%\n","Epoch: 6400/10000 | Mean size 10: 15.8 | Longest 10: 024 | Mean steps 10: 120.0 | Wins: 4389 | Win percentage: 68.6%\n","Epoch: 6410/10000 | Mean size 10: 18.4 | Longest 10: 026 | Mean steps 10: 145.8 | Wins: 4399 | Win percentage: 68.6%\n","Epoch: 6420/10000 | Mean size 10: 17.2 | Longest 10: 033 | Mean steps 10: 127.0 | Wins: 4409 | Win percentage: 68.7%\n","Epoch: 6430/10000 | Mean size 10: 16.7 | Longest 10: 027 | Mean steps 10: 147.6 | Wins: 4419 | Win percentage: 68.7%\n","Epoch: 6440/10000 | Mean size 10: 18.4 | Longest 10: 029 | Mean steps 10: 169.2 | Wins: 4429 | Win percentage: 68.8%\n","Epoch: 6450/10000 | Mean size 10: 18.2 | Longest 10: 026 | Mean steps 10: 158.0 | Wins: 4439 | Win percentage: 68.8%\n","Epoch: 6460/10000 | Mean size 10: 15.2 | Longest 10: 025 | Mean steps 10: 123.6 | Wins: 4449 | Win percentage: 68.9%\n","Epoch: 6470/10000 | Mean size 10: 16.2 | Longest 10: 027 | Mean steps 10: 131.6 | Wins: 4459 | Win percentage: 68.9%\n","Epoch: 6480/10000 | Mean size 10: 16.4 | Longest 10: 024 | Mean steps 10: 133.1 | Wins: 4469 | Win percentage: 69.0%\n","Epoch: 6490/10000 | Mean size 10: 17.9 | Longest 10: 027 | Mean steps 10: 147.5 | Wins: 4479 | Win percentage: 69.0%\n","Epoch: 6500/10000 | Mean size 10: 16.4 | Longest 10: 024 | Mean steps 10: 115.4 | Wins: 4489 | Win percentage: 69.1%\n","Epoch: 6510/10000 | Mean size 10: 15.6 | Longest 10: 031 | Mean steps 10: 115.3 | Wins: 4499 | Win percentage: 69.1%\n","Epoch: 6520/10000 | Mean size 10: 17.9 | Longest 10: 035 | Mean steps 10: 147.0 | Wins: 4509 | Win percentage: 69.2%\n","Epoch: 6530/10000 | Mean size 10: 16.9 | Longest 10: 026 | Mean steps 10: 127.5 | Wins: 4519 | Win percentage: 69.2%\n","Epoch: 6540/10000 | Mean size 10: 18.6 | Longest 10: 035 | Mean steps 10: 155.9 | Wins: 4529 | Win percentage: 69.3%\n","Epoch: 6550/10000 | Mean size 10: 16.2 | Longest 10: 027 | Mean steps 10: 129.9 | Wins: 4539 | Win percentage: 69.3%\n","Epoch: 6560/10000 | Mean size 10: 15.2 | Longest 10: 027 | Mean steps 10: 117.6 | Wins: 4549 | Win percentage: 69.3%\n","Epoch: 6570/10000 | Mean size 10: 18.1 | Longest 10: 030 | Mean steps 10: 149.5 | Wins: 4559 | Win percentage: 69.4%\n","Epoch: 6580/10000 | Mean size 10: 19.9 | Longest 10: 030 | Mean steps 10: 169.3 | Wins: 4569 | Win percentage: 69.4%\n","Epoch: 6590/10000 | Mean size 10: 13.8 | Longest 10: 024 | Mean steps 10: 105.9 | Wins: 4579 | Win percentage: 69.5%\n","Epoch: 6600/10000 | Mean size 10: 17.0 | Longest 10: 024 | Mean steps 10: 144.9 | Wins: 4589 | Win percentage: 69.5%\n","Epoch: 6610/10000 | Mean size 10: 18.0 | Longest 10: 027 | Mean steps 10: 152.1 | Wins: 4599 | Win percentage: 69.6%\n","Epoch: 6620/10000 | Mean size 10: 12.8 | Longest 10: 018 | Mean steps 10: 85.7 | Wins: 4609 | Win percentage: 69.6%\n","Epoch: 6630/10000 | Mean size 10: 15.6 | Longest 10: 027 | Mean steps 10: 119.3 | Wins: 4619 | Win percentage: 69.7%\n","Epoch: 6640/10000 | Mean size 10: 16.9 | Longest 10: 029 | Mean steps 10: 134.9 | Wins: 4629 | Win percentage: 69.7%\n","Epoch: 6650/10000 | Mean size 10: 15.7 | Longest 10: 026 | Mean steps 10: 119.1 | Wins: 4639 | Win percentage: 69.8%\n","Epoch: 6660/10000 | Mean size 10: 17.0 | Longest 10: 023 | Mean steps 10: 130.8 | Wins: 4649 | Win percentage: 69.8%\n","Epoch: 6670/10000 | Mean size 10: 14.2 | Longest 10: 024 | Mean steps 10: 108.2 | Wins: 4659 | Win percentage: 69.9%\n","Epoch: 6680/10000 | Mean size 10: 19.3 | Longest 10: 026 | Mean steps 10: 156.0 | Wins: 4669 | Win percentage: 69.9%\n","Epoch: 6690/10000 | Mean size 10: 16.8 | Longest 10: 025 | Mean steps 10: 132.8 | Wins: 4679 | Win percentage: 69.9%\n","Epoch: 6700/10000 | Mean size 10: 18.4 | Longest 10: 029 | Mean steps 10: 156.3 | Wins: 4689 | Win percentage: 70.0%\n","Epoch: 6710/10000 | Mean size 10: 16.3 | Longest 10: 025 | Mean steps 10: 127.1 | Wins: 4699 | Win percentage: 70.0%\n","Epoch: 6720/10000 | Mean size 10: 18.4 | Longest 10: 024 | Mean steps 10: 160.2 | Wins: 4709 | Win percentage: 70.1%\n","Epoch: 6730/10000 | Mean size 10: 16.8 | Longest 10: 024 | Mean steps 10: 114.8 | Wins: 4719 | Win percentage: 70.1%\n","Epoch: 6740/10000 | Mean size 10: 15.0 | Longest 10: 019 | Mean steps 10: 117.0 | Wins: 4729 | Win percentage: 70.2%\n","Epoch: 6750/10000 | Mean size 10: 19.1 | Longest 10: 025 | Mean steps 10: 154.4 | Wins: 4739 | Win percentage: 70.2%\n","Epoch: 6760/10000 | Mean size 10: 17.1 | Longest 10: 026 | Mean steps 10: 123.5 | Wins: 4749 | Win percentage: 70.3%\n","Epoch: 6770/10000 | Mean size 10: 16.5 | Longest 10: 030 | Mean steps 10: 125.2 | Wins: 4758 | Win percentage: 70.3%\n","Epoch: 6780/10000 | Mean size 10: 18.2 | Longest 10: 028 | Mean steps 10: 149.8 | Wins: 4768 | Win percentage: 70.3%\n","Epoch: 6790/10000 | Mean size 10: 19.3 | Longest 10: 027 | Mean steps 10: 158.4 | Wins: 4778 | Win percentage: 70.4%\n","Epoch: 6800/10000 | Mean size 10: 14.2 | Longest 10: 024 | Mean steps 10: 101.9 | Wins: 4787 | Win percentage: 70.4%\n","Epoch: 6810/10000 | Mean size 10: 20.9 | Longest 10: 031 | Mean steps 10: 184.0 | Wins: 4797 | Win percentage: 70.4%\n","Epoch: 6820/10000 | Mean size 10: 15.0 | Longest 10: 032 | Mean steps 10: 110.6 | Wins: 4807 | Win percentage: 70.5%\n","Epoch: 6830/10000 | Mean size 10: 15.6 | Longest 10: 027 | Mean steps 10: 134.8 | Wins: 4817 | Win percentage: 70.5%\n","Epoch: 6840/10000 | Mean size 10: 16.8 | Longest 10: 025 | Mean steps 10: 128.4 | Wins: 4827 | Win percentage: 70.6%\n","Epoch: 6850/10000 | Mean size 10: 19.7 | Longest 10: 030 | Mean steps 10: 160.9 | Wins: 4837 | Win percentage: 70.6%\n","Epoch: 6860/10000 | Mean size 10: 18.1 | Longest 10: 035 | Mean steps 10: 146.8 | Wins: 4847 | Win percentage: 70.7%\n","Epoch: 6870/10000 | Mean size 10: 14.5 | Longest 10: 023 | Mean steps 10: 104.7 | Wins: 4857 | Win percentage: 70.7%\n","Epoch: 6880/10000 | Mean size 10: 16.0 | Longest 10: 028 | Mean steps 10: 120.3 | Wins: 4867 | Win percentage: 70.7%\n","Epoch: 6890/10000 | Mean size 10: 17.4 | Longest 10: 034 | Mean steps 10: 125.6 | Wins: 4877 | Win percentage: 70.8%\n","Epoch: 6900/10000 | Mean size 10: 16.1 | Longest 10: 023 | Mean steps 10: 117.9 | Wins: 4887 | Win percentage: 70.8%\n","Epoch: 6910/10000 | Mean size 10: 14.1 | Longest 10: 028 | Mean steps 10: 101.1 | Wins: 4897 | Win percentage: 70.9%\n","Epoch: 6920/10000 | Mean size 10: 19.4 | Longest 10: 026 | Mean steps 10: 161.4 | Wins: 4907 | Win percentage: 70.9%\n","Epoch: 6930/10000 | Mean size 10: 18.3 | Longest 10: 027 | Mean steps 10: 140.5 | Wins: 4917 | Win percentage: 71.0%\n","Epoch: 6940/10000 | Mean size 10: 16.3 | Longest 10: 024 | Mean steps 10: 127.3 | Wins: 4927 | Win percentage: 71.0%\n","Epoch: 6950/10000 | Mean size 10: 21.3 | Longest 10: 030 | Mean steps 10: 196.3 | Wins: 4937 | Win percentage: 71.0%\n","Epoch: 6960/10000 | Mean size 10: 16.0 | Longest 10: 026 | Mean steps 10: 131.9 | Wins: 4947 | Win percentage: 71.1%\n","Epoch: 6970/10000 | Mean size 10: 17.1 | Longest 10: 027 | Mean steps 10: 145.6 | Wins: 4957 | Win percentage: 71.1%\n","Epoch: 6980/10000 | Mean size 10: 16.7 | Longest 10: 022 | Mean steps 10: 128.2 | Wins: 4967 | Win percentage: 71.2%\n","Epoch: 6990/10000 | Mean size 10: 16.9 | Longest 10: 029 | Mean steps 10: 130.9 | Wins: 4977 | Win percentage: 71.2%\n","Epoch: 7000/10000 | Mean size 10: 17.5 | Longest 10: 026 | Mean steps 10: 132.5 | Wins: 4987 | Win percentage: 71.2%\n","Epoch: 7010/10000 | Mean size 10: 14.7 | Longest 10: 028 | Mean steps 10: 108.8 | Wins: 4997 | Win percentage: 71.3%\n","Epoch: 7020/10000 | Mean size 10: 19.4 | Longest 10: 028 | Mean steps 10: 161.3 | Wins: 5007 | Win percentage: 71.3%\n","Epoch: 7030/10000 | Mean size 10: 17.4 | Longest 10: 025 | Mean steps 10: 132.1 | Wins: 5017 | Win percentage: 71.4%\n","Epoch: 7040/10000 | Mean size 10: 16.7 | Longest 10: 037 | Mean steps 10: 125.9 | Wins: 5027 | Win percentage: 71.4%\n","Epoch: 7050/10000 | Mean size 10: 14.5 | Longest 10: 026 | Mean steps 10: 103.6 | Wins: 5037 | Win percentage: 71.4%\n","Epoch: 7060/10000 | Mean size 10: 19.7 | Longest 10: 027 | Mean steps 10: 154.4 | Wins: 5047 | Win percentage: 71.5%\n","Epoch: 7070/10000 | Mean size 10: 25.4 | Longest 10: 034 | Mean steps 10: 233.0 | Wins: 5057 | Win percentage: 71.5%\n","Epoch: 7080/10000 | Mean size 10: 18.7 | Longest 10: 027 | Mean steps 10: 145.4 | Wins: 5067 | Win percentage: 71.6%\n","Epoch: 7090/10000 | Mean size 10: 16.6 | Longest 10: 032 | Mean steps 10: 125.4 | Wins: 5077 | Win percentage: 71.6%\n","Epoch: 7100/10000 | Mean size 10: 16.0 | Longest 10: 025 | Mean steps 10: 124.4 | Wins: 5087 | Win percentage: 71.6%\n","Epoch: 7110/10000 | Mean size 10: 20.9 | Longest 10: 032 | Mean steps 10: 179.3 | Wins: 5097 | Win percentage: 71.7%\n","Epoch: 7120/10000 | Mean size 10: 17.5 | Longest 10: 030 | Mean steps 10: 138.8 | Wins: 5107 | Win percentage: 71.7%\n","Epoch: 7130/10000 | Mean size 10: 19.2 | Longest 10: 029 | Mean steps 10: 153.1 | Wins: 5117 | Win percentage: 71.8%\n","Epoch: 7140/10000 | Mean size 10: 19.0 | Longest 10: 034 | Mean steps 10: 150.1 | Wins: 5127 | Win percentage: 71.8%\n","Epoch: 7150/10000 | Mean size 10: 17.3 | Longest 10: 024 | Mean steps 10: 132.8 | Wins: 5137 | Win percentage: 71.8%\n","Epoch: 7160/10000 | Mean size 10: 19.3 | Longest 10: 025 | Mean steps 10: 152.7 | Wins: 5147 | Win percentage: 71.9%\n","Epoch: 7170/10000 | Mean size 10: 15.2 | Longest 10: 021 | Mean steps 10: 107.1 | Wins: 5157 | Win percentage: 71.9%\n","Epoch: 7180/10000 | Mean size 10: 17.5 | Longest 10: 029 | Mean steps 10: 144.5 | Wins: 5167 | Win percentage: 72.0%\n","Epoch: 7190/10000 | Mean size 10: 15.5 | Longest 10: 028 | Mean steps 10: 112.2 | Wins: 5177 | Win percentage: 72.0%\n","Epoch: 7200/10000 | Mean size 10: 18.6 | Longest 10: 030 | Mean steps 10: 144.4 | Wins: 5187 | Win percentage: 72.0%\n","Epoch: 7210/10000 | Mean size 10: 21.0 | Longest 10: 030 | Mean steps 10: 174.8 | Wins: 5197 | Win percentage: 72.1%\n","Epoch: 7220/10000 | Mean size 10: 17.2 | Longest 10: 030 | Mean steps 10: 134.3 | Wins: 5207 | Win percentage: 72.1%\n","Epoch: 7230/10000 | Mean size 10: 19.5 | Longest 10: 026 | Mean steps 10: 147.0 | Wins: 5217 | Win percentage: 72.2%\n","Epoch: 7240/10000 | Mean size 10: 15.7 | Longest 10: 028 | Mean steps 10: 115.4 | Wins: 5227 | Win percentage: 72.2%\n","Epoch: 7250/10000 | Mean size 10: 19.2 | Longest 10: 030 | Mean steps 10: 155.5 | Wins: 5237 | Win percentage: 72.2%\n","Epoch: 7260/10000 | Mean size 10: 18.2 | Longest 10: 025 | Mean steps 10: 142.1 | Wins: 5247 | Win percentage: 72.3%\n","Epoch: 7270/10000 | Mean size 10: 15.9 | Longest 10: 027 | Mean steps 10: 118.2 | Wins: 5257 | Win percentage: 72.3%\n","Epoch: 7280/10000 | Mean size 10: 17.3 | Longest 10: 029 | Mean steps 10: 116.3 | Wins: 5267 | Win percentage: 72.3%\n","Epoch: 7290/10000 | Mean size 10: 14.2 | Longest 10: 024 | Mean steps 10: 104.5 | Wins: 5276 | Win percentage: 72.4%\n","Epoch: 7300/10000 | Mean size 10: 16.4 | Longest 10: 023 | Mean steps 10: 122.4 | Wins: 5286 | Win percentage: 72.4%\n","Epoch: 7310/10000 | Mean size 10: 18.2 | Longest 10: 028 | Mean steps 10: 132.7 | Wins: 5296 | Win percentage: 72.4%\n","Epoch: 7320/10000 | Mean size 10: 14.4 | Longest 10: 024 | Mean steps 10: 114.7 | Wins: 5306 | Win percentage: 72.5%\n","Epoch: 7330/10000 | Mean size 10: 16.3 | Longest 10: 023 | Mean steps 10: 110.0 | Wins: 5316 | Win percentage: 72.5%\n","Epoch: 7340/10000 | Mean size 10: 17.0 | Longest 10: 031 | Mean steps 10: 136.6 | Wins: 5326 | Win percentage: 72.6%\n","Epoch: 7350/10000 | Mean size 10: 20.2 | Longest 10: 032 | Mean steps 10: 165.2 | Wins: 5336 | Win percentage: 72.6%\n","Epoch: 7360/10000 | Mean size 10: 20.0 | Longest 10: 027 | Mean steps 10: 167.6 | Wins: 5346 | Win percentage: 72.6%\n","Epoch: 7370/10000 | Mean size 10: 13.8 | Longest 10: 022 | Mean steps 10: 88.6 | Wins: 5356 | Win percentage: 72.7%\n","Epoch: 7380/10000 | Mean size 10: 15.4 | Longest 10: 028 | Mean steps 10: 110.0 | Wins: 5366 | Win percentage: 72.7%\n","Epoch: 7390/10000 | Mean size 10: 19.6 | Longest 10: 031 | Mean steps 10: 157.3 | Wins: 5376 | Win percentage: 72.7%\n","Epoch: 7400/10000 | Mean size 10: 17.9 | Longest 10: 027 | Mean steps 10: 130.8 | Wins: 5386 | Win percentage: 72.8%\n","Epoch: 7410/10000 | Mean size 10: 15.6 | Longest 10: 028 | Mean steps 10: 112.9 | Wins: 5395 | Win percentage: 72.8%\n","Epoch: 7420/10000 | Mean size 10: 14.6 | Longest 10: 023 | Mean steps 10: 102.1 | Wins: 5405 | Win percentage: 72.8%\n","Epoch: 7430/10000 | Mean size 10: 15.5 | Longest 10: 028 | Mean steps 10: 117.2 | Wins: 5415 | Win percentage: 72.9%\n","Epoch: 7440/10000 | Mean size 10: 19.8 | Longest 10: 031 | Mean steps 10: 148.4 | Wins: 5425 | Win percentage: 72.9%\n","Epoch: 7450/10000 | Mean size 10: 18.1 | Longest 10: 035 | Mean steps 10: 138.7 | Wins: 5435 | Win percentage: 73.0%\n","Epoch: 7460/10000 | Mean size 10: 17.3 | Longest 10: 033 | Mean steps 10: 129.2 | Wins: 5445 | Win percentage: 73.0%\n","Epoch: 7470/10000 | Mean size 10: 16.6 | Longest 10: 030 | Mean steps 10: 115.6 | Wins: 5455 | Win percentage: 73.0%\n","Epoch: 7480/10000 | Mean size 10: 17.8 | Longest 10: 030 | Mean steps 10: 141.5 | Wins: 5465 | Win percentage: 73.1%\n","Epoch: 7490/10000 | Mean size 10: 16.0 | Longest 10: 026 | Mean steps 10: 110.7 | Wins: 5475 | Win percentage: 73.1%\n","Epoch: 7500/10000 | Mean size 10: 18.3 | Longest 10: 030 | Mean steps 10: 141.9 | Wins: 5485 | Win percentage: 73.1%\n","Epoch: 7510/10000 | Mean size 10: 15.9 | Longest 10: 027 | Mean steps 10: 113.9 | Wins: 5495 | Win percentage: 73.2%\n","Epoch: 7520/10000 | Mean size 10: 18.0 | Longest 10: 027 | Mean steps 10: 142.9 | Wins: 5505 | Win percentage: 73.2%\n","Epoch: 7530/10000 | Mean size 10: 24.0 | Longest 10: 036 | Mean steps 10: 204.6 | Wins: 5515 | Win percentage: 73.2%\n","Epoch: 7540/10000 | Mean size 10: 17.2 | Longest 10: 026 | Mean steps 10: 122.1 | Wins: 5525 | Win percentage: 73.3%\n","Epoch: 7550/10000 | Mean size 10: 16.4 | Longest 10: 024 | Mean steps 10: 121.5 | Wins: 5535 | Win percentage: 73.3%\n","Epoch: 7560/10000 | Mean size 10: 23.0 | Longest 10: 031 | Mean steps 10: 189.6 | Wins: 5545 | Win percentage: 73.3%\n","Epoch: 7570/10000 | Mean size 10: 20.2 | Longest 10: 035 | Mean steps 10: 152.3 | Wins: 5555 | Win percentage: 73.4%\n","Epoch: 7580/10000 | Mean size 10: 18.3 | Longest 10: 034 | Mean steps 10: 137.7 | Wins: 5565 | Win percentage: 73.4%\n","Epoch: 7590/10000 | Mean size 10: 17.2 | Longest 10: 033 | Mean steps 10: 149.1 | Wins: 5575 | Win percentage: 73.5%\n","Epoch: 7600/10000 | Mean size 10: 19.6 | Longest 10: 029 | Mean steps 10: 149.7 | Wins: 5585 | Win percentage: 73.5%\n","Epoch: 7610/10000 | Mean size 10: 18.1 | Longest 10: 034 | Mean steps 10: 133.7 | Wins: 5595 | Win percentage: 73.5%\n","Epoch: 7620/10000 | Mean size 10: 19.2 | Longest 10: 031 | Mean steps 10: 145.5 | Wins: 5605 | Win percentage: 73.6%\n","Epoch: 7630/10000 | Mean size 10: 18.4 | Longest 10: 029 | Mean steps 10: 132.8 | Wins: 5615 | Win percentage: 73.6%\n","Epoch: 7640/10000 | Mean size 10: 18.2 | Longest 10: 032 | Mean steps 10: 139.5 | Wins: 5625 | Win percentage: 73.6%\n","Epoch: 7650/10000 | Mean size 10: 17.8 | Longest 10: 030 | Mean steps 10: 134.9 | Wins: 5635 | Win percentage: 73.7%\n","Epoch: 7660/10000 | Mean size 10: 16.9 | Longest 10: 026 | Mean steps 10: 127.1 | Wins: 5645 | Win percentage: 73.7%\n","Epoch: 7670/10000 | Mean size 10: 17.8 | Longest 10: 028 | Mean steps 10: 136.5 | Wins: 5655 | Win percentage: 73.7%\n","Epoch: 7680/10000 | Mean size 10: 13.0 | Longest 10: 027 | Mean steps 10: 94.9 | Wins: 5664 | Win percentage: 73.8%\n","Epoch: 7690/10000 | Mean size 10: 19.0 | Longest 10: 032 | Mean steps 10: 148.1 | Wins: 5674 | Win percentage: 73.8%\n","Epoch: 7700/10000 | Mean size 10: 17.8 | Longest 10: 025 | Mean steps 10: 124.9 | Wins: 5684 | Win percentage: 73.8%\n","Epoch: 7710/10000 | Mean size 10: 16.2 | Longest 10: 029 | Mean steps 10: 125.5 | Wins: 5694 | Win percentage: 73.9%\n","Epoch: 7720/10000 | Mean size 10: 19.4 | Longest 10: 027 | Mean steps 10: 142.3 | Wins: 5704 | Win percentage: 73.9%\n","Epoch: 7730/10000 | Mean size 10: 17.9 | Longest 10: 032 | Mean steps 10: 139.8 | Wins: 5714 | Win percentage: 73.9%\n","Epoch: 7740/10000 | Mean size 10: 14.3 | Longest 10: 031 | Mean steps 10: 104.3 | Wins: 5724 | Win percentage: 74.0%\n","Epoch: 7750/10000 | Mean size 10: 15.7 | Longest 10: 034 | Mean steps 10: 112.1 | Wins: 5734 | Win percentage: 74.0%\n","Epoch: 7760/10000 | Mean size 10: 20.0 | Longest 10: 031 | Mean steps 10: 158.8 | Wins: 5744 | Win percentage: 74.0%\n","Epoch: 7770/10000 | Mean size 10: 19.3 | Longest 10: 030 | Mean steps 10: 142.3 | Wins: 5754 | Win percentage: 74.1%\n","Epoch: 7780/10000 | Mean size 10: 15.5 | Longest 10: 028 | Mean steps 10: 106.7 | Wins: 5764 | Win percentage: 74.1%\n","Epoch: 7790/10000 | Mean size 10: 17.8 | Longest 10: 025 | Mean steps 10: 130.0 | Wins: 5774 | Win percentage: 74.1%\n","Epoch: 7800/10000 | Mean size 10: 16.0 | Longest 10: 026 | Mean steps 10: 105.5 | Wins: 5784 | Win percentage: 74.2%\n","Epoch: 7810/10000 | Mean size 10: 17.5 | Longest 10: 028 | Mean steps 10: 126.3 | Wins: 5794 | Win percentage: 74.2%\n","Epoch: 7820/10000 | Mean size 10: 18.0 | Longest 10: 031 | Mean steps 10: 140.8 | Wins: 5804 | Win percentage: 74.2%\n","Epoch: 7830/10000 | Mean size 10: 15.4 | Longest 10: 023 | Mean steps 10: 113.5 | Wins: 5814 | Win percentage: 74.3%\n","Epoch: 7840/10000 | Mean size 10: 13.6 | Longest 10: 024 | Mean steps 10: 94.5 | Wins: 5824 | Win percentage: 74.3%\n","Epoch: 7850/10000 | Mean size 10: 21.7 | Longest 10: 035 | Mean steps 10: 162.4 | Wins: 5834 | Win percentage: 74.3%\n","Epoch: 7860/10000 | Mean size 10: 19.3 | Longest 10: 031 | Mean steps 10: 149.3 | Wins: 5844 | Win percentage: 74.4%\n","Epoch: 7870/10000 | Mean size 10: 18.9 | Longest 10: 032 | Mean steps 10: 141.8 | Wins: 5854 | Win percentage: 74.4%\n","Epoch: 7880/10000 | Mean size 10: 15.7 | Longest 10: 025 | Mean steps 10: 112.3 | Wins: 5864 | Win percentage: 74.4%\n","Epoch: 7890/10000 | Mean size 10: 20.2 | Longest 10: 032 | Mean steps 10: 147.4 | Wins: 5874 | Win percentage: 74.4%\n","Epoch: 7900/10000 | Mean size 10: 18.3 | Longest 10: 029 | Mean steps 10: 138.9 | Wins: 5884 | Win percentage: 74.5%\n","Epoch: 7910/10000 | Mean size 10: 17.1 | Longest 10: 027 | Mean steps 10: 116.9 | Wins: 5894 | Win percentage: 74.5%\n","Epoch: 7920/10000 | Mean size 10: 16.7 | Longest 10: 035 | Mean steps 10: 126.9 | Wins: 5904 | Win percentage: 74.5%\n","Epoch: 7930/10000 | Mean size 10: 15.7 | Longest 10: 029 | Mean steps 10: 114.6 | Wins: 5914 | Win percentage: 74.6%\n","Epoch: 7940/10000 | Mean size 10: 18.8 | Longest 10: 036 | Mean steps 10: 143.0 | Wins: 5924 | Win percentage: 74.6%\n","Epoch: 7950/10000 | Mean size 10: 17.2 | Longest 10: 026 | Mean steps 10: 141.0 | Wins: 5934 | Win percentage: 74.6%\n","Epoch: 7960/10000 | Mean size 10: 16.8 | Longest 10: 030 | Mean steps 10: 124.1 | Wins: 5944 | Win percentage: 74.7%\n","Epoch: 7970/10000 | Mean size 10: 17.8 | Longest 10: 029 | Mean steps 10: 136.2 | Wins: 5954 | Win percentage: 74.7%\n","Epoch: 7980/10000 | Mean size 10: 20.7 | Longest 10: 029 | Mean steps 10: 159.0 | Wins: 5964 | Win percentage: 74.7%\n","Epoch: 7990/10000 | Mean size 10: 16.8 | Longest 10: 033 | Mean steps 10: 121.0 | Wins: 5974 | Win percentage: 74.8%\n","Epoch: 8000/10000 | Mean size 10: 21.7 | Longest 10: 032 | Mean steps 10: 172.5 | Wins: 5984 | Win percentage: 74.8%\n","Epoch: 8010/10000 | Mean size 10: 17.8 | Longest 10: 032 | Mean steps 10: 127.6 | Wins: 5994 | Win percentage: 74.8%\n","Epoch: 8020/10000 | Mean size 10: 15.6 | Longest 10: 033 | Mean steps 10: 128.5 | Wins: 6003 | Win percentage: 74.9%\n","Epoch: 8030/10000 | Mean size 10: 18.1 | Longest 10: 030 | Mean steps 10: 140.5 | Wins: 6013 | Win percentage: 74.9%\n","Epoch: 8040/10000 | Mean size 10: 16.0 | Longest 10: 031 | Mean steps 10: 118.8 | Wins: 6023 | Win percentage: 74.9%\n","Epoch: 8050/10000 | Mean size 10: 18.6 | Longest 10: 028 | Mean steps 10: 135.6 | Wins: 6033 | Win percentage: 74.9%\n","Epoch: 8060/10000 | Mean size 10: 18.7 | Longest 10: 029 | Mean steps 10: 143.4 | Wins: 6043 | Win percentage: 75.0%\n","Epoch: 8070/10000 | Mean size 10: 16.6 | Longest 10: 022 | Mean steps 10: 106.7 | Wins: 6053 | Win percentage: 75.0%\n","Epoch: 8080/10000 | Mean size 10: 12.0 | Longest 10: 019 | Mean steps 10: 75.6 | Wins: 6063 | Win percentage: 75.0%\n","Epoch: 8090/10000 | Mean size 10: 17.9 | Longest 10: 029 | Mean steps 10: 136.9 | Wins: 6073 | Win percentage: 75.1%\n","Epoch: 8100/10000 | Mean size 10: 17.6 | Longest 10: 027 | Mean steps 10: 139.0 | Wins: 6083 | Win percentage: 75.1%\n","Epoch: 8110/10000 | Mean size 10: 18.7 | Longest 10: 026 | Mean steps 10: 131.6 | Wins: 6093 | Win percentage: 75.1%\n","Epoch: 8120/10000 | Mean size 10: 17.5 | Longest 10: 032 | Mean steps 10: 124.6 | Wins: 6103 | Win percentage: 75.2%\n","Epoch: 8130/10000 | Mean size 10: 15.5 | Longest 10: 025 | Mean steps 10: 103.0 | Wins: 6113 | Win percentage: 75.2%\n","Epoch: 8140/10000 | Mean size 10: 20.4 | Longest 10: 025 | Mean steps 10: 155.0 | Wins: 6123 | Win percentage: 75.2%\n","Epoch: 8150/10000 | Mean size 10: 14.8 | Longest 10: 026 | Mean steps 10: 103.6 | Wins: 6133 | Win percentage: 75.3%\n","Epoch: 8160/10000 | Mean size 10: 17.2 | Longest 10: 029 | Mean steps 10: 131.4 | Wins: 6143 | Win percentage: 75.3%\n","Epoch: 8170/10000 | Mean size 10: 22.3 | Longest 10: 041 | Mean steps 10: 169.4 | Wins: 6153 | Win percentage: 75.3%\n","Epoch: 8180/10000 | Mean size 10: 16.0 | Longest 10: 031 | Mean steps 10: 120.0 | Wins: 6163 | Win percentage: 75.3%\n","Epoch: 8190/10000 | Mean size 10: 20.3 | Longest 10: 034 | Mean steps 10: 161.3 | Wins: 6173 | Win percentage: 75.4%\n","Epoch: 8200/10000 | Mean size 10: 17.5 | Longest 10: 026 | Mean steps 10: 119.9 | Wins: 6183 | Win percentage: 75.4%\n","Epoch: 8210/10000 | Mean size 10: 14.2 | Longest 10: 032 | Mean steps 10: 86.7 | Wins: 6193 | Win percentage: 75.4%\n","Epoch: 8220/10000 | Mean size 10: 20.5 | Longest 10: 028 | Mean steps 10: 151.2 | Wins: 6203 | Win percentage: 75.5%\n","Epoch: 8230/10000 | Mean size 10: 21.4 | Longest 10: 030 | Mean steps 10: 164.9 | Wins: 6213 | Win percentage: 75.5%\n","Epoch: 8240/10000 | Mean size 10: 18.3 | Longest 10: 024 | Mean steps 10: 140.9 | Wins: 6223 | Win percentage: 75.5%\n","Epoch: 8250/10000 | Mean size 10: 18.1 | Longest 10: 029 | Mean steps 10: 127.8 | Wins: 6233 | Win percentage: 75.6%\n","Epoch: 8260/10000 | Mean size 10: 18.5 | Longest 10: 029 | Mean steps 10: 122.6 | Wins: 6243 | Win percentage: 75.6%\n","Epoch: 8270/10000 | Mean size 10: 20.0 | Longest 10: 028 | Mean steps 10: 151.4 | Wins: 6253 | Win percentage: 75.6%\n","Epoch: 8280/10000 | Mean size 10: 18.9 | Longest 10: 026 | Mean steps 10: 136.3 | Wins: 6263 | Win percentage: 75.6%\n","Epoch: 8290/10000 | Mean size 10: 13.6 | Longest 10: 026 | Mean steps 10: 89.0 | Wins: 6273 | Win percentage: 75.7%\n","Epoch: 8300/10000 | Mean size 10: 19.8 | Longest 10: 026 | Mean steps 10: 142.0 | Wins: 6283 | Win percentage: 75.7%\n","Epoch: 8310/10000 | Mean size 10: 21.3 | Longest 10: 031 | Mean steps 10: 172.6 | Wins: 6293 | Win percentage: 75.7%\n","Epoch: 8320/10000 | Mean size 10: 19.6 | Longest 10: 028 | Mean steps 10: 160.0 | Wins: 6303 | Win percentage: 75.8%\n","Epoch: 8330/10000 | Mean size 10: 20.0 | Longest 10: 035 | Mean steps 10: 147.8 | Wins: 6313 | Win percentage: 75.8%\n","Epoch: 8340/10000 | Mean size 10: 16.3 | Longest 10: 024 | Mean steps 10: 117.5 | Wins: 6323 | Win percentage: 75.8%\n","Epoch: 8350/10000 | Mean size 10: 17.7 | Longest 10: 030 | Mean steps 10: 127.4 | Wins: 6333 | Win percentage: 75.8%\n","Epoch: 8360/10000 | Mean size 10: 20.0 | Longest 10: 027 | Mean steps 10: 153.7 | Wins: 6343 | Win percentage: 75.9%\n","Epoch: 8370/10000 | Mean size 10: 16.6 | Longest 10: 030 | Mean steps 10: 123.4 | Wins: 6353 | Win percentage: 75.9%\n","Epoch: 8380/10000 | Mean size 10: 15.6 | Longest 10: 022 | Mean steps 10: 108.1 | Wins: 6363 | Win percentage: 75.9%\n","Epoch: 8390/10000 | Mean size 10: 18.2 | Longest 10: 025 | Mean steps 10: 136.0 | Wins: 6373 | Win percentage: 76.0%\n","Epoch: 8400/10000 | Mean size 10: 16.7 | Longest 10: 026 | Mean steps 10: 123.4 | Wins: 6383 | Win percentage: 76.0%\n","Epoch: 8410/10000 | Mean size 10: 20.3 | Longest 10: 033 | Mean steps 10: 145.0 | Wins: 6393 | Win percentage: 76.0%\n","Epoch: 8420/10000 | Mean size 10: 19.0 | Longest 10: 028 | Mean steps 10: 138.8 | Wins: 6403 | Win percentage: 76.0%\n","Epoch: 8430/10000 | Mean size 10: 15.3 | Longest 10: 023 | Mean steps 10: 108.5 | Wins: 6413 | Win percentage: 76.1%\n","Epoch: 8440/10000 | Mean size 10: 15.2 | Longest 10: 040 | Mean steps 10: 110.3 | Wins: 6423 | Win percentage: 76.1%\n","Epoch: 8450/10000 | Mean size 10: 20.4 | Longest 10: 029 | Mean steps 10: 148.2 | Wins: 6433 | Win percentage: 76.1%\n","Epoch: 8460/10000 | Mean size 10: 22.3 | Longest 10: 036 | Mean steps 10: 171.1 | Wins: 6443 | Win percentage: 76.2%\n","Epoch: 8470/10000 | Mean size 10: 13.7 | Longest 10: 025 | Mean steps 10: 96.0 | Wins: 6453 | Win percentage: 76.2%\n","Epoch: 8480/10000 | Mean size 10: 14.5 | Longest 10: 023 | Mean steps 10: 102.5 | Wins: 6463 | Win percentage: 76.2%\n","Epoch: 8490/10000 | Mean size 10: 18.9 | Longest 10: 029 | Mean steps 10: 138.1 | Wins: 6473 | Win percentage: 76.2%\n","Epoch: 8500/10000 | Mean size 10: 20.0 | Longest 10: 034 | Mean steps 10: 146.3 | Wins: 6483 | Win percentage: 76.3%\n","Epoch: 8510/10000 | Mean size 10: 19.8 | Longest 10: 028 | Mean steps 10: 144.1 | Wins: 6493 | Win percentage: 76.3%\n","Epoch: 8520/10000 | Mean size 10: 19.5 | Longest 10: 030 | Mean steps 10: 148.3 | Wins: 6503 | Win percentage: 76.3%\n","Epoch: 8530/10000 | Mean size 10: 18.1 | Longest 10: 029 | Mean steps 10: 129.8 | Wins: 6513 | Win percentage: 76.4%\n","Epoch: 8540/10000 | Mean size 10: 15.9 | Longest 10: 023 | Mean steps 10: 100.1 | Wins: 6523 | Win percentage: 76.4%\n","Epoch: 8550/10000 | Mean size 10: 17.3 | Longest 10: 029 | Mean steps 10: 119.7 | Wins: 6533 | Win percentage: 76.4%\n","Epoch: 8560/10000 | Mean size 10: 17.7 | Longest 10: 022 | Mean steps 10: 124.4 | Wins: 6543 | Win percentage: 76.4%\n","Epoch: 8570/10000 | Mean size 10: 21.0 | Longest 10: 033 | Mean steps 10: 158.6 | Wins: 6553 | Win percentage: 76.5%\n","Epoch: 8580/10000 | Mean size 10: 17.2 | Longest 10: 022 | Mean steps 10: 126.2 | Wins: 6563 | Win percentage: 76.5%\n","Epoch: 8590/10000 | Mean size 10: 15.1 | Longest 10: 028 | Mean steps 10: 101.7 | Wins: 6573 | Win percentage: 76.5%\n","Epoch: 8600/10000 | Mean size 10: 18.5 | Longest 10: 028 | Mean steps 10: 123.3 | Wins: 6583 | Win percentage: 76.5%\n","Epoch: 8610/10000 | Mean size 10: 13.7 | Longest 10: 026 | Mean steps 10: 100.0 | Wins: 6593 | Win percentage: 76.6%\n","Epoch: 8620/10000 | Mean size 10: 14.8 | Longest 10: 023 | Mean steps 10: 100.4 | Wins: 6603 | Win percentage: 76.6%\n","Epoch: 8630/10000 | Mean size 10: 18.2 | Longest 10: 028 | Mean steps 10: 136.1 | Wins: 6613 | Win percentage: 76.6%\n","Epoch: 8640/10000 | Mean size 10: 19.0 | Longest 10: 029 | Mean steps 10: 143.7 | Wins: 6623 | Win percentage: 76.7%\n","Epoch: 8650/10000 | Mean size 10: 17.9 | Longest 10: 027 | Mean steps 10: 125.7 | Wins: 6633 | Win percentage: 76.7%\n","Epoch: 8660/10000 | Mean size 10: 17.7 | Longest 10: 026 | Mean steps 10: 138.3 | Wins: 6643 | Win percentage: 76.7%\n","Epoch: 8670/10000 | Mean size 10: 18.7 | Longest 10: 035 | Mean steps 10: 149.2 | Wins: 6653 | Win percentage: 76.7%\n","Epoch: 8680/10000 | Mean size 10: 20.3 | Longest 10: 027 | Mean steps 10: 150.5 | Wins: 6663 | Win percentage: 76.8%\n","Epoch: 8690/10000 | Mean size 10: 22.1 | Longest 10: 035 | Mean steps 10: 156.0 | Wins: 6673 | Win percentage: 76.8%\n","Epoch: 8700/10000 | Mean size 10: 18.7 | Longest 10: 029 | Mean steps 10: 149.3 | Wins: 6683 | Win percentage: 76.8%\n","Epoch: 8710/10000 | Mean size 10: 16.0 | Longest 10: 027 | Mean steps 10: 111.0 | Wins: 6693 | Win percentage: 76.8%\n","Epoch: 8720/10000 | Mean size 10: 21.2 | Longest 10: 030 | Mean steps 10: 155.1 | Wins: 6703 | Win percentage: 76.9%\n","Epoch: 8730/10000 | Mean size 10: 18.5 | Longest 10: 027 | Mean steps 10: 127.6 | Wins: 6713 | Win percentage: 76.9%\n","Epoch: 8740/10000 | Mean size 10: 16.2 | Longest 10: 033 | Mean steps 10: 109.4 | Wins: 6723 | Win percentage: 76.9%\n","Epoch: 8750/10000 | Mean size 10: 20.1 | Longest 10: 034 | Mean steps 10: 145.1 | Wins: 6733 | Win percentage: 76.9%\n","Epoch: 8760/10000 | Mean size 10: 18.7 | Longest 10: 027 | Mean steps 10: 130.8 | Wins: 6743 | Win percentage: 77.0%\n","Epoch: 8770/10000 | Mean size 10: 17.1 | Longest 10: 033 | Mean steps 10: 129.7 | Wins: 6753 | Win percentage: 77.0%\n","Epoch: 8780/10000 | Mean size 10: 19.8 | Longest 10: 034 | Mean steps 10: 142.9 | Wins: 6763 | Win percentage: 77.0%\n","Epoch: 8790/10000 | Mean size 10: 17.6 | Longest 10: 033 | Mean steps 10: 128.6 | Wins: 6773 | Win percentage: 77.1%\n","Epoch: 8800/10000 | Mean size 10: 18.8 | Longest 10: 030 | Mean steps 10: 133.5 | Wins: 6783 | Win percentage: 77.1%\n","Epoch: 8810/10000 | Mean size 10: 19.0 | Longest 10: 034 | Mean steps 10: 155.8 | Wins: 6792 | Win percentage: 77.1%\n","Epoch: 8820/10000 | Mean size 10: 17.1 | Longest 10: 028 | Mean steps 10: 129.8 | Wins: 6802 | Win percentage: 77.1%\n","Epoch: 8830/10000 | Mean size 10: 17.8 | Longest 10: 029 | Mean steps 10: 117.2 | Wins: 6812 | Win percentage: 77.1%\n","Epoch: 8840/10000 | Mean size 10: 17.1 | Longest 10: 029 | Mean steps 10: 122.6 | Wins: 6822 | Win percentage: 77.2%\n","Epoch: 8850/10000 | Mean size 10: 17.9 | Longest 10: 035 | Mean steps 10: 138.2 | Wins: 6832 | Win percentage: 77.2%\n","Epoch: 8860/10000 | Mean size 10: 15.2 | Longest 10: 026 | Mean steps 10: 99.6 | Wins: 6842 | Win percentage: 77.2%\n","Epoch: 8870/10000 | Mean size 10: 19.9 | Longest 10: 035 | Mean steps 10: 149.1 | Wins: 6852 | Win percentage: 77.2%\n","Epoch: 8880/10000 | Mean size 10: 20.6 | Longest 10: 034 | Mean steps 10: 147.5 | Wins: 6862 | Win percentage: 77.3%\n","Epoch: 8890/10000 | Mean size 10: 16.7 | Longest 10: 026 | Mean steps 10: 119.0 | Wins: 6872 | Win percentage: 77.3%\n","Epoch: 8900/10000 | Mean size 10: 21.4 | Longest 10: 031 | Mean steps 10: 165.0 | Wins: 6882 | Win percentage: 77.3%\n","Epoch: 8910/10000 | Mean size 10: 16.9 | Longest 10: 029 | Mean steps 10: 119.3 | Wins: 6892 | Win percentage: 77.4%\n","Epoch: 8920/10000 | Mean size 10: 16.3 | Longest 10: 023 | Mean steps 10: 113.7 | Wins: 6902 | Win percentage: 77.4%\n","Epoch: 8930/10000 | Mean size 10: 18.8 | Longest 10: 029 | Mean steps 10: 130.0 | Wins: 6912 | Win percentage: 77.4%\n","Epoch: 8940/10000 | Mean size 10: 15.6 | Longest 10: 026 | Mean steps 10: 108.6 | Wins: 6922 | Win percentage: 77.4%\n","Epoch: 8950/10000 | Mean size 10: 18.3 | Longest 10: 028 | Mean steps 10: 135.9 | Wins: 6932 | Win percentage: 77.5%\n","Epoch: 8960/10000 | Mean size 10: 14.6 | Longest 10: 026 | Mean steps 10: 99.1 | Wins: 6942 | Win percentage: 77.5%\n","Epoch: 8970/10000 | Mean size 10: 19.1 | Longest 10: 034 | Mean steps 10: 147.9 | Wins: 6952 | Win percentage: 77.5%\n","Epoch: 8980/10000 | Mean size 10: 21.6 | Longest 10: 027 | Mean steps 10: 170.6 | Wins: 6962 | Win percentage: 77.5%\n","Epoch: 8990/10000 | Mean size 10: 18.7 | Longest 10: 034 | Mean steps 10: 147.1 | Wins: 6972 | Win percentage: 77.6%\n","Epoch: 9000/10000 | Mean size 10: 15.6 | Longest 10: 024 | Mean steps 10: 119.1 | Wins: 6982 | Win percentage: 77.6%\n","Epoch: 9010/10000 | Mean size 10: 20.1 | Longest 10: 030 | Mean steps 10: 148.3 | Wins: 6992 | Win percentage: 77.6%\n","Epoch: 9020/10000 | Mean size 10: 14.6 | Longest 10: 023 | Mean steps 10: 101.4 | Wins: 7002 | Win percentage: 77.6%\n","Epoch: 9030/10000 | Mean size 10: 15.3 | Longest 10: 023 | Mean steps 10: 102.3 | Wins: 7011 | Win percentage: 77.6%\n","Epoch: 9040/10000 | Mean size 10: 17.2 | Longest 10: 031 | Mean steps 10: 125.0 | Wins: 7021 | Win percentage: 77.7%\n","Epoch: 9050/10000 | Mean size 10: 23.0 | Longest 10: 033 | Mean steps 10: 179.5 | Wins: 7031 | Win percentage: 77.7%\n","Epoch: 9060/10000 | Mean size 10: 20.3 | Longest 10: 033 | Mean steps 10: 161.8 | Wins: 7041 | Win percentage: 77.7%\n","Epoch: 9070/10000 | Mean size 10: 21.3 | Longest 10: 028 | Mean steps 10: 167.2 | Wins: 7051 | Win percentage: 77.7%\n","Epoch: 9080/10000 | Mean size 10: 16.9 | Longest 10: 029 | Mean steps 10: 119.4 | Wins: 7061 | Win percentage: 77.8%\n","Epoch: 9090/10000 | Mean size 10: 15.7 | Longest 10: 024 | Mean steps 10: 103.2 | Wins: 7071 | Win percentage: 77.8%\n","Epoch: 9100/10000 | Mean size 10: 19.5 | Longest 10: 030 | Mean steps 10: 133.1 | Wins: 7081 | Win percentage: 77.8%\n","Epoch: 9110/10000 | Mean size 10: 18.9 | Longest 10: 029 | Mean steps 10: 133.9 | Wins: 7091 | Win percentage: 77.8%\n","Epoch: 9120/10000 | Mean size 10: 21.3 | Longest 10: 034 | Mean steps 10: 170.4 | Wins: 7101 | Win percentage: 77.9%\n","Epoch: 9130/10000 | Mean size 10: 16.5 | Longest 10: 028 | Mean steps 10: 114.4 | Wins: 7111 | Win percentage: 77.9%\n","Epoch: 9140/10000 | Mean size 10: 21.5 | Longest 10: 033 | Mean steps 10: 161.7 | Wins: 7121 | Win percentage: 77.9%\n","Epoch: 9150/10000 | Mean size 10: 18.9 | Longest 10: 031 | Mean steps 10: 140.0 | Wins: 7131 | Win percentage: 77.9%\n","Epoch: 9160/10000 | Mean size 10: 14.9 | Longest 10: 022 | Mean steps 10: 100.1 | Wins: 7141 | Win percentage: 78.0%\n","Epoch: 9170/10000 | Mean size 10: 16.1 | Longest 10: 026 | Mean steps 10: 118.0 | Wins: 7151 | Win percentage: 78.0%\n","Epoch: 9180/10000 | Mean size 10: 14.6 | Longest 10: 026 | Mean steps 10: 89.8 | Wins: 7161 | Win percentage: 78.0%\n","Epoch: 9190/10000 | Mean size 10: 15.4 | Longest 10: 027 | Mean steps 10: 99.0 | Wins: 7171 | Win percentage: 78.0%\n","Epoch: 9200/10000 | Mean size 10: 18.0 | Longest 10: 028 | Mean steps 10: 133.1 | Wins: 7181 | Win percentage: 78.1%\n","Epoch: 9210/10000 | Mean size 10: 20.8 | Longest 10: 035 | Mean steps 10: 161.0 | Wins: 7191 | Win percentage: 78.1%\n","Epoch: 9220/10000 | Mean size 10: 16.7 | Longest 10: 024 | Mean steps 10: 120.3 | Wins: 7201 | Win percentage: 78.1%\n","Epoch: 9230/10000 | Mean size 10: 14.5 | Longest 10: 026 | Mean steps 10: 112.2 | Wins: 7211 | Win percentage: 78.1%\n","Epoch: 9240/10000 | Mean size 10: 16.3 | Longest 10: 029 | Mean steps 10: 111.5 | Wins: 7221 | Win percentage: 78.1%\n","Epoch: 9250/10000 | Mean size 10: 17.5 | Longest 10: 028 | Mean steps 10: 120.9 | Wins: 7231 | Win percentage: 78.2%\n","Epoch: 9260/10000 | Mean size 10: 13.8 | Longest 10: 026 | Mean steps 10: 101.1 | Wins: 7241 | Win percentage: 78.2%\n","Epoch: 9270/10000 | Mean size 10: 16.9 | Longest 10: 031 | Mean steps 10: 126.9 | Wins: 7251 | Win percentage: 78.2%\n","Epoch: 9280/10000 | Mean size 10: 16.7 | Longest 10: 033 | Mean steps 10: 117.1 | Wins: 7261 | Win percentage: 78.2%\n","Epoch: 9290/10000 | Mean size 10: 20.1 | Longest 10: 033 | Mean steps 10: 155.4 | Wins: 7271 | Win percentage: 78.3%\n","Epoch: 9300/10000 | Mean size 10: 22.8 | Longest 10: 029 | Mean steps 10: 181.9 | Wins: 7281 | Win percentage: 78.3%\n","Epoch: 9310/10000 | Mean size 10: 18.5 | Longest 10: 033 | Mean steps 10: 139.0 | Wins: 7291 | Win percentage: 78.3%\n","Epoch: 9320/10000 | Mean size 10: 18.0 | Longest 10: 030 | Mean steps 10: 135.1 | Wins: 7301 | Win percentage: 78.3%\n","Epoch: 9330/10000 | Mean size 10: 17.9 | Longest 10: 023 | Mean steps 10: 117.4 | Wins: 7311 | Win percentage: 78.4%\n","Epoch: 9340/10000 | Mean size 10: 18.1 | Longest 10: 036 | Mean steps 10: 119.7 | Wins: 7321 | Win percentage: 78.4%\n","Epoch: 9350/10000 | Mean size 10: 18.0 | Longest 10: 032 | Mean steps 10: 130.6 | Wins: 7331 | Win percentage: 78.4%\n","Epoch: 9360/10000 | Mean size 10: 20.4 | Longest 10: 029 | Mean steps 10: 149.2 | Wins: 7341 | Win percentage: 78.4%\n","Epoch: 9370/10000 | Mean size 10: 16.4 | Longest 10: 024 | Mean steps 10: 105.6 | Wins: 7351 | Win percentage: 78.5%\n","Epoch: 9380/10000 | Mean size 10: 19.6 | Longest 10: 029 | Mean steps 10: 137.1 | Wins: 7361 | Win percentage: 78.5%\n","Epoch: 9390/10000 | Mean size 10: 16.3 | Longest 10: 031 | Mean steps 10: 107.4 | Wins: 7371 | Win percentage: 78.5%\n","Epoch: 9400/10000 | Mean size 10: 18.0 | Longest 10: 030 | Mean steps 10: 132.2 | Wins: 7381 | Win percentage: 78.5%\n","Epoch: 9410/10000 | Mean size 10: 19.2 | Longest 10: 036 | Mean steps 10: 154.7 | Wins: 7391 | Win percentage: 78.5%\n","Epoch: 9420/10000 | Mean size 10: 20.8 | Longest 10: 033 | Mean steps 10: 149.8 | Wins: 7401 | Win percentage: 78.6%\n","Epoch: 9430/10000 | Mean size 10: 20.3 | Longest 10: 032 | Mean steps 10: 155.0 | Wins: 7411 | Win percentage: 78.6%\n","Epoch: 9440/10000 | Mean size 10: 21.6 | Longest 10: 031 | Mean steps 10: 156.5 | Wins: 7421 | Win percentage: 78.6%\n","Epoch: 9450/10000 | Mean size 10: 15.6 | Longest 10: 034 | Mean steps 10: 94.6 | Wins: 7431 | Win percentage: 78.6%\n","Epoch: 9460/10000 | Mean size 10: 16.7 | Longest 10: 027 | Mean steps 10: 127.4 | Wins: 7441 | Win percentage: 78.7%\n","Epoch: 9470/10000 | Mean size 10: 15.5 | Longest 10: 033 | Mean steps 10: 113.8 | Wins: 7451 | Win percentage: 78.7%\n","Epoch: 9480/10000 | Mean size 10: 18.7 | Longest 10: 035 | Mean steps 10: 139.8 | Wins: 7461 | Win percentage: 78.7%\n","Epoch: 9490/10000 | Mean size 10: 17.3 | Longest 10: 034 | Mean steps 10: 126.3 | Wins: 7471 | Win percentage: 78.7%\n","Epoch: 9500/10000 | Mean size 10: 20.7 | Longest 10: 033 | Mean steps 10: 162.9 | Wins: 7481 | Win percentage: 78.7%\n","Epoch: 9510/10000 | Mean size 10: 17.4 | Longest 10: 031 | Mean steps 10: 130.4 | Wins: 7491 | Win percentage: 78.8%\n","Epoch: 9520/10000 | Mean size 10: 15.8 | Longest 10: 038 | Mean steps 10: 115.0 | Wins: 7501 | Win percentage: 78.8%\n","Epoch: 9530/10000 | Mean size 10: 16.6 | Longest 10: 034 | Mean steps 10: 116.7 | Wins: 7511 | Win percentage: 78.8%\n","Epoch: 9540/10000 | Mean size 10: 16.6 | Longest 10: 030 | Mean steps 10: 119.1 | Wins: 7521 | Win percentage: 78.8%\n","Epoch: 9550/10000 | Mean size 10: 16.1 | Longest 10: 026 | Mean steps 10: 110.3 | Wins: 7531 | Win percentage: 78.9%\n","Epoch: 9560/10000 | Mean size 10: 15.1 | Longest 10: 022 | Mean steps 10: 92.0 | Wins: 7541 | Win percentage: 78.9%\n","Epoch: 9570/10000 | Mean size 10: 21.3 | Longest 10: 033 | Mean steps 10: 158.9 | Wins: 7551 | Win percentage: 78.9%\n","Epoch: 9580/10000 | Mean size 10: 19.1 | Longest 10: 033 | Mean steps 10: 148.1 | Wins: 7561 | Win percentage: 78.9%\n","Epoch: 9590/10000 | Mean size 10: 19.4 | Longest 10: 030 | Mean steps 10: 134.7 | Wins: 7571 | Win percentage: 78.9%\n","Epoch: 9600/10000 | Mean size 10: 13.6 | Longest 10: 029 | Mean steps 10: 97.0 | Wins: 7581 | Win percentage: 79.0%\n","Epoch: 9610/10000 | Mean size 10: 18.7 | Longest 10: 032 | Mean steps 10: 141.4 | Wins: 7591 | Win percentage: 79.0%\n","Epoch: 9620/10000 | Mean size 10: 21.9 | Longest 10: 032 | Mean steps 10: 172.0 | Wins: 7601 | Win percentage: 79.0%\n","Epoch: 9630/10000 | Mean size 10: 11.6 | Longest 10: 019 | Mean steps 10: 69.1 | Wins: 7611 | Win percentage: 79.0%\n","Epoch: 9640/10000 | Mean size 10: 19.4 | Longest 10: 030 | Mean steps 10: 146.8 | Wins: 7621 | Win percentage: 79.1%\n","Epoch: 9650/10000 | Mean size 10: 17.4 | Longest 10: 033 | Mean steps 10: 119.9 | Wins: 7631 | Win percentage: 79.1%\n","Epoch: 9660/10000 | Mean size 10: 16.5 | Longest 10: 026 | Mean steps 10: 121.4 | Wins: 7641 | Win percentage: 79.1%\n","Epoch: 9670/10000 | Mean size 10: 20.6 | Longest 10: 028 | Mean steps 10: 158.3 | Wins: 7651 | Win percentage: 79.1%\n","Epoch: 9680/10000 | Mean size 10: 15.6 | Longest 10: 028 | Mean steps 10: 101.3 | Wins: 7661 | Win percentage: 79.1%\n","Epoch: 9690/10000 | Mean size 10: 21.2 | Longest 10: 025 | Mean steps 10: 151.1 | Wins: 7671 | Win percentage: 79.2%\n","Epoch: 9700/10000 | Mean size 10: 24.8 | Longest 10: 036 | Mean steps 10: 184.3 | Wins: 7681 | Win percentage: 79.2%\n","Epoch: 9710/10000 | Mean size 10: 13.2 | Longest 10: 024 | Mean steps 10: 85.6 | Wins: 7691 | Win percentage: 79.2%\n","Epoch: 9720/10000 | Mean size 10: 17.9 | Longest 10: 031 | Mean steps 10: 123.3 | Wins: 7701 | Win percentage: 79.2%\n","Epoch: 9730/10000 | Mean size 10: 18.4 | Longest 10: 027 | Mean steps 10: 131.4 | Wins: 7711 | Win percentage: 79.2%\n","Epoch: 9740/10000 | Mean size 10: 16.1 | Longest 10: 022 | Mean steps 10: 107.3 | Wins: 7721 | Win percentage: 79.3%\n","Epoch: 9750/10000 | Mean size 10: 18.9 | Longest 10: 028 | Mean steps 10: 137.5 | Wins: 7731 | Win percentage: 79.3%\n","Epoch: 9760/10000 | Mean size 10: 17.5 | Longest 10: 028 | Mean steps 10: 129.0 | Wins: 7741 | Win percentage: 79.3%\n","Epoch: 9770/10000 | Mean size 10: 16.8 | Longest 10: 026 | Mean steps 10: 108.4 | Wins: 7751 | Win percentage: 79.3%\n","Epoch: 9780/10000 | Mean size 10: 13.3 | Longest 10: 019 | Mean steps 10: 82.4 | Wins: 7761 | Win percentage: 79.4%\n","Epoch: 9790/10000 | Mean size 10: 14.3 | Longest 10: 021 | Mean steps 10: 88.6 | Wins: 7771 | Win percentage: 79.4%\n","Epoch: 9800/10000 | Mean size 10: 17.2 | Longest 10: 024 | Mean steps 10: 121.5 | Wins: 7781 | Win percentage: 79.4%\n","Epoch: 9810/10000 | Mean size 10: 21.4 | Longest 10: 028 | Mean steps 10: 175.6 | Wins: 7791 | Win percentage: 79.4%\n","Epoch: 9820/10000 | Mean size 10: 18.8 | Longest 10: 032 | Mean steps 10: 136.2 | Wins: 7801 | Win percentage: 79.4%\n","Epoch: 9830/10000 | Mean size 10: 13.1 | Longest 10: 022 | Mean steps 10: 84.5 | Wins: 7811 | Win percentage: 79.5%\n","Epoch: 9840/10000 | Mean size 10: 19.9 | Longest 10: 034 | Mean steps 10: 137.7 | Wins: 7821 | Win percentage: 79.5%\n","Epoch: 9850/10000 | Mean size 10: 15.4 | Longest 10: 028 | Mean steps 10: 117.3 | Wins: 7831 | Win percentage: 79.5%\n","Epoch: 9860/10000 | Mean size 10: 21.6 | Longest 10: 032 | Mean steps 10: 168.8 | Wins: 7841 | Win percentage: 79.5%\n","Epoch: 9870/10000 | Mean size 10: 16.7 | Longest 10: 028 | Mean steps 10: 117.3 | Wins: 7851 | Win percentage: 79.5%\n","Epoch: 9880/10000 | Mean size 10: 17.6 | Longest 10: 030 | Mean steps 10: 129.4 | Wins: 7861 | Win percentage: 79.6%\n","Epoch: 9890/10000 | Mean size 10: 15.8 | Longest 10: 030 | Mean steps 10: 117.9 | Wins: 7871 | Win percentage: 79.6%\n","Epoch: 9900/10000 | Mean size 10: 17.4 | Longest 10: 028 | Mean steps 10: 135.8 | Wins: 7881 | Win percentage: 79.6%\n","Epoch: 9910/10000 | Mean size 10: 19.0 | Longest 10: 031 | Mean steps 10: 150.0 | Wins: 7891 | Win percentage: 79.6%\n","Epoch: 9920/10000 | Mean size 10: 15.1 | Longest 10: 031 | Mean steps 10: 107.9 | Wins: 7901 | Win percentage: 79.6%\n","Epoch: 9930/10000 | Mean size 10: 20.2 | Longest 10: 027 | Mean steps 10: 143.6 | Wins: 7911 | Win percentage: 79.7%\n","Epoch: 9940/10000 | Mean size 10: 17.8 | Longest 10: 029 | Mean steps 10: 132.5 | Wins: 7921 | Win percentage: 79.7%\n","Epoch: 9950/10000 | Mean size 10: 19.8 | Longest 10: 030 | Mean steps 10: 150.2 | Wins: 7931 | Win percentage: 79.7%\n","Epoch: 9960/10000 | Mean size 10: 19.0 | Longest 10: 031 | Mean steps 10: 138.6 | Wins: 7941 | Win percentage: 79.7%\n","Epoch: 9970/10000 | Mean size 10: 14.4 | Longest 10: 022 | Mean steps 10: 103.9 | Wins: 7951 | Win percentage: 79.7%\n","Epoch: 9980/10000 | Mean size 10: 21.2 | Longest 10: 031 | Mean steps 10: 172.2 | Wins: 7961 | Win percentage: 79.8%\n","Epoch: 9990/10000 | Mean size 10: 16.3 | Longest 10: 031 | Mean steps 10: 113.2 | Wins: 7971 | Win percentage: 79.8%\n","Epoch: 10000/10000 | Mean size 10: 16.6 | Longest 10: 027 | Mean steps 10: 125.1 | Wins: 7981 | Win percentage: 79.8%\n"],"name":"stdout"}]},{"metadata":{"id":"RimjdNdPxEmk","colab_type":"code","outputId":"e61cb006-f927-416c-97dd-48054c55120d","executionInfo":{"status":"ok","timestamp":1541689548151,"user_tz":120,"elapsed":6,"user":{"displayName":"Victor Neves","photoUrl":"https://lh5.googleusercontent.com/-zfn1KWpcK60/AAAAAAAAAAI/AAAAAAAAZas/oOSqICtc8i0/s64/photo.jpg","userId":"17687780415293205160"}},"colab":{"resources":{"http://localhost:20227/content/model-epsgreedy-bench-newmemory.zip":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNTAwIChJbnRlcm5hbCBTZXJ2ZXIgRXJyb3IpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj41MDAuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","ok":false,"headers":[["content-length","1461"],["content-type","text/html; charset=utf-8"]],"status":500,"status_text":""}},"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["model.save('keras.h5')\n","\n","!zip -r model-epsgreedy-bench-newmemory.zip keras.h5 \n","from google.colab import files\n","files.download('model-epsgreedy-bench-newmemory.zip')\n","model = load_model('keras.h5', custom_objects={'clipped_error': clipped_error})\n","\n","board_size = 10\n","nb_frames = 4\n","nb_actions = 5\n","\n","target = None\n","\n","agent = Agent(model = model, target = target, memory_size = 1500000,\n","                          nb_frames = nb_frames, board_size = board_size,\n","                          per = False)\n","#%lprun -f agent.train agent.train(game, batch_size = 64, nb_epoch = 10, gamma = 0.95, update_target_freq = 500, policy = \"EpsGreedyQPolicy\")\n","\n","agent.play(game, visual = False, nb_epoch = 10000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["  adding: keras.h5 (deflated 43%)\n","Accuracy: 100.0 %\n","Mean size: 19.7908 | Biggest size: 45 | Smallest size: 4\n","Mean steps: 144.3177 | Biggest step: 801 | Smallest step: 5\n"],"name":"stdout"}]}]}